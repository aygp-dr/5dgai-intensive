1
00:00:00,240 --> 00:00:02,270

welcome everyone to the next iteration

2
00:00:02,270 --> 00:00:02,280
welcome everyone to the next iteration
 

3
00:00:02,280 --> 00:00:04,309
welcome everyone to the next iteration
of the kaggle generative AI intensive

4
00:00:04,309 --> 00:00:04,319
of the kaggle generative AI intensive
 

5
00:00:04,319 --> 00:00:06,749
of the kaggle generative AI intensive
course we're really excited to have over

6
00:00:06,749 --> 00:00:06,759
course we're really excited to have over
 

7
00:00:06,759 --> 00:00:09,190
course we're really excited to have over
a quarter of a million developers dialed

8
00:00:09,190 --> 00:00:09,200
a quarter of a million developers dialed
 

9
00:00:09,200 --> 00:00:11,190
a quarter of a million developers dialed
in today to learn all about the Gemini

10
00:00:11,190 --> 00:00:11,200
in today to learn all about the Gemini
 

11
00:00:11,200 --> 00:00:14,869
in today to learn all about the Gemini
apis AI Studio vertex Ai and all of the

12
00:00:14,869 --> 00:00:14,879
apis AI Studio vertex Ai and all of the
 

13
00:00:14,879 --> 00:00:17,070
apis AI Studio vertex Ai and all of the
wonderful AI features that Google cloud

14
00:00:17,070 --> 00:00:17,080
wonderful AI features that Google cloud
 

15
00:00:17,080 --> 00:00:18,670
wonderful AI features that Google cloud
has been putting out over the last

16
00:00:18,670 --> 00:00:18,680
has been putting out over the last
 

17
00:00:18,680 --> 00:00:21,349
has been putting out over the last
several months um but before we we we

18
00:00:21,349 --> 00:00:21,359
several months um but before we we we
 

19
00:00:21,359 --> 00:00:25,070
several months um but before we we we
begin I really want uh to uh to welcome

20
00:00:25,070 --> 00:00:25,080
begin I really want uh to uh to welcome
 

21
00:00:25,080 --> 00:00:26,830
begin I really want uh to uh to welcome
on to the stage someone very special our

22
00:00:26,830 --> 00:00:26,840
on to the stage someone very special our
 

23
00:00:26,840 --> 00:00:29,310
on to the stage someone very special our
chief scientist at Google um who wanted

24
00:00:29,310 --> 00:00:29,320
chief scientist at Google um who wanted
 

25
00:00:29,320 --> 00:00:31,910
chief scientist at Google um who wanted
to be here to to to give you a short

26
00:00:31,910 --> 00:00:31,920
to be here to to to give you a short
 

27
00:00:31,920 --> 00:00:34,709
to be here to to to give you a short
message um so with that let's welcome

28
00:00:34,709 --> 00:00:34,719
message um so with that let's welcome
 

29
00:00:34,719 --> 00:00:35,990
message um so with that let's welcome
Jeff

30
00:00:35,990 --> 00:00:36,000
Jeff
 

31
00:00:36,000 --> 00:00:39,549
Jeff
Dean hi everyone Jeff de here uh I'm

32
00:00:39,549 --> 00:00:39,559
Dean hi everyone Jeff de here uh I'm
 

33
00:00:39,559 --> 00:00:41,350
Dean hi everyone Jeff de here uh I'm
super excited you've all decided to join

34
00:00:41,350 --> 00:00:41,360
super excited you've all decided to join
 

35
00:00:41,360 --> 00:00:45,750
super excited you've all decided to join
the kaggle Gen uh intensive uh course um

36
00:00:45,750 --> 00:00:45,760
the kaggle Gen uh intensive uh course um
 

37
00:00:45,760 --> 00:00:47,350
the kaggle Gen uh intensive uh course um
you know we ran this course last year

38
00:00:47,350 --> 00:00:47,360
you know we ran this course last year
 

39
00:00:47,360 --> 00:00:50,189
you know we ran this course last year
and we had about 150,000 developers sign

40
00:00:50,189 --> 00:00:50,199
and we had about 150,000 developers sign
 

41
00:00:50,199 --> 00:00:52,110
and we had about 150,000 developers sign
up um we're running it this year looks

42
00:00:52,110 --> 00:00:52,120
up um we're running it this year looks
 

43
00:00:52,120 --> 00:00:53,950
up um we're running it this year looks
like we'll have more than 200,000 which

44
00:00:53,950 --> 00:00:53,960
like we'll have more than 200,000 which
 

45
00:00:53,960 --> 00:00:55,990
like we'll have more than 200,000 which
is pretty amazing um and really we're

46
00:00:55,990 --> 00:00:56,000
is pretty amazing um and really we're
 

47
00:00:56,000 --> 00:00:59,110
is pretty amazing um and really we're
excited to see what people do with uh

48
00:00:59,110 --> 00:00:59,120
excited to see what people do with uh
 

49
00:00:59,120 --> 00:01:00,830
excited to see what people do with uh
these generative model there's so many

50
00:01:00,830 --> 00:01:00,840
these generative model there's so many
 

51
00:01:00,840 --> 00:01:03,189
these generative model there's so many
possibilities in terms of creating you

52
00:01:03,189 --> 00:01:03,199
possibilities in terms of creating you
 

53
00:01:03,199 --> 00:01:05,990
possibilities in terms of creating you
know useful uh things that help people

54
00:01:05,990 --> 00:01:06,000
know useful uh things that help people
 

55
00:01:06,000 --> 00:01:07,710
know useful uh things that help people
get things done you know things that

56
00:01:07,710 --> 00:01:07,720
get things done you know things that
 

57
00:01:07,720 --> 00:01:10,030
get things done you know things that
entertain people things that help people

58
00:01:10,030 --> 00:01:10,040
entertain people things that help people
 

59
00:01:10,040 --> 00:01:11,590
entertain people things that help people
you know take information and transform

60
00:01:11,590 --> 00:01:11,600
you know take information and transform
 

61
00:01:11,600 --> 00:01:14,990
you know take information and transform
it into different kinds of modalities um

62
00:01:14,990 --> 00:01:15,000
it into different kinds of modalities um
 

63
00:01:15,000 --> 00:01:16,870
it into different kinds of modalities um
and the the capabilities of these models

64
00:01:16,870 --> 00:01:16,880
and the the capabilities of these models
 

65
00:01:16,880 --> 00:01:19,670
and the the capabilities of these models
are increasing quickly so that means

66
00:01:19,670 --> 00:01:19,680
are increasing quickly so that means
 

67
00:01:19,680 --> 00:01:21,230
are increasing quickly so that means
that the building blocks that we all

68
00:01:21,230 --> 00:01:21,240
that the building blocks that we all
 

69
00:01:21,240 --> 00:01:24,230
that the building blocks that we all
have as developers are growing in uh you

70
00:01:24,230 --> 00:01:24,240
have as developers are growing in uh you
 

71
00:01:24,240 --> 00:01:26,149
have as developers are growing in uh you
know sophistication and that enable us

72
00:01:26,149 --> 00:01:26,159
know sophistication and that enable us
 

73
00:01:26,159 --> 00:01:27,990
know sophistication and that enable us
to do more and more interesting things

74
00:01:27,990 --> 00:01:28,000
to do more and more interesting things
 

75
00:01:28,000 --> 00:01:29,910
to do more and more interesting things
so we're really really excited about

76
00:01:29,910 --> 00:01:29,920
so we're really really excited about
 

77
00:01:29,920 --> 00:01:31,630
so we're really really excited about
what you all are going to do uh what

78
00:01:31,630 --> 00:01:31,640
what you all are going to do uh what
 

79
00:01:31,640 --> 00:01:33,630
what you all are going to do uh what
you're all going to learn to do and

80
00:01:33,630 --> 00:01:33,640
you're all going to learn to do and
 

81
00:01:33,640 --> 00:01:35,870
you're all going to learn to do and
that's why we're running this course and

82
00:01:35,870 --> 00:01:35,880
that's why we're running this course and
 

83
00:01:35,880 --> 00:01:37,710
that's why we're running this course and
uh we're looking forward to answering

84
00:01:37,710 --> 00:01:37,720
uh we're looking forward to answering
 

85
00:01:37,720 --> 00:01:40,230
uh we're looking forward to answering
questions and giving you you know

86
00:01:40,230 --> 00:01:40,240
questions and giving you you know
 

87
00:01:40,240 --> 00:01:42,149
questions and giving you you know
guidance about how to use these tools

88
00:01:42,149 --> 00:01:42,159
guidance about how to use these tools
 

89
00:01:42,159 --> 00:01:45,870
guidance about how to use these tools
and seeing how you all use them super

90
00:01:45,870 --> 00:01:45,880
and seeing how you all use them super
 

91
00:01:45,880 --> 00:01:47,709
and seeing how you all use them super
exciting thank

92
00:01:47,709 --> 00:01:47,719
exciting thank
 

93
00:01:47,719 --> 00:01:50,510
exciting thank
you excellent and thank you so much Jeff

94
00:01:50,510 --> 00:01:50,520
you excellent and thank you so much Jeff
 

95
00:01:50,520 --> 00:01:54,469
you excellent and thank you so much Jeff
Dean I I think that we all have uh uh

96
00:01:54,469 --> 00:01:54,479
Dean I I think that we all have uh uh
 

97
00:01:54,479 --> 00:01:56,270
Dean I I think that we all have uh uh
we've all used many of the products that

98
00:01:56,270 --> 00:01:56,280
we've all used many of the products that
 

99
00:01:56,280 --> 00:01:58,670
we've all used many of the products that
Jeff has built from the ground up um and

100
00:01:58,670 --> 00:01:58,680
Jeff has built from the ground up um and
 

101
00:01:58,680 --> 00:02:00,910
Jeff has built from the ground up um and
so it's wonderful to have him here today

102
00:02:00,910 --> 00:02:00,920
so it's wonderful to have him here today
 

103
00:02:00,920 --> 00:02:03,830
so it's wonderful to have him here today
to tell us a little bit about um what

104
00:02:03,830 --> 00:02:03,840
to tell us a little bit about um what
 

105
00:02:03,840 --> 00:02:05,950
to tell us a little bit about um what
he's excited to see for our students to

106
00:02:05,950 --> 00:02:05,960
he's excited to see for our students to
 

107
00:02:05,960 --> 00:02:08,510
he's excited to see for our students to
create so taking a look back at the

108
00:02:08,510 --> 00:02:08,520
create so taking a look back at the
 

109
00:02:08,520 --> 00:02:11,270
create so taking a look back at the
slides um just as a general reminder

110
00:02:11,270 --> 00:02:11,280
slides um just as a general reminder
 

111
00:02:11,280 --> 00:02:13,750
slides um just as a general reminder
this five-day intensive course is hosted

112
00:02:13,750 --> 00:02:13,760
this five-day intensive course is hosted
 

113
00:02:13,760 --> 00:02:15,990
this five-day intensive course is hosted
by kaggle it includes everything from

114
00:02:15,990 --> 00:02:16,000
by kaggle it includes everything from
 

115
00:02:16,000 --> 00:02:18,309
by kaggle it includes everything from
daily assignments to Discord discussion

116
00:02:18,309 --> 00:02:18,319
daily assignments to Discord discussion
 

117
00:02:18,319 --> 00:02:21,670
daily assignments to Discord discussion
threads um live stream uh and ask me

118
00:02:21,670 --> 00:02:21,680
threads um live stream uh and ask me
 

119
00:02:21,680 --> 00:02:23,229
threads um live stream uh and ask me
anything session which you're dialed

120
00:02:23,229 --> 00:02:23,239
anything session which you're dialed
 

121
00:02:23,239 --> 00:02:27,070
anything session which you're dialed
into right now um and as a reminder um

122
00:02:27,070 --> 00:02:27,080
into right now um and as a reminder um
 

123
00:02:27,080 --> 00:02:29,350
into right now um and as a reminder um
this 5-day generative AI intensive

124
00:02:29,350 --> 00:02:29,360
this 5-day generative AI intensive
 

125
00:02:29,360 --> 00:02:31,509
this 5-day generative AI intensive
course incorporates foundational models

126
00:02:31,509 --> 00:02:31,519
course incorporates foundational models
 

127
00:02:31,519 --> 00:02:33,869
course incorporates foundational models
embeddings and Vector databases agents

128
00:02:33,869 --> 00:02:33,879
embeddings and Vector databases agents
 

129
00:02:33,879 --> 00:02:36,589
embeddings and Vector databases agents
domain specific models and today we are

130
00:02:36,589 --> 00:02:36,599
domain specific models and today we are
 

131
00:02:36,599 --> 00:02:38,710
domain specific models and today we are
going to be exploring the wonderful

132
00:02:38,710 --> 00:02:38,720
going to be exploring the wonderful
 

133
00:02:38,720 --> 00:02:41,509
going to be exploring the wonderful
world of foundational models and prompt

134
00:02:41,509 --> 00:02:41,519
world of foundational models and prompt
 

135
00:02:41,519 --> 00:02:44,430
world of foundational models and prompt
engineering um just as a reminder you

136
00:02:44,430 --> 00:02:44,440
engineering um just as a reminder you
 

137
00:02:44,440 --> 00:02:45,990
engineering um just as a reminder you
should have been listening to our

138
00:02:45,990 --> 00:02:46,000
should have been listening to our
 

139
00:02:46,000 --> 00:02:47,990
should have been listening to our
summary podcast episode which was

140
00:02:47,990 --> 00:02:48,000
summary podcast episode which was
 

141
00:02:48,000 --> 00:02:50,030
summary podcast episode which was
created by notebook LM we'll get to hear

142
00:02:50,030 --> 00:02:50,040
created by notebook LM we'll get to hear
 

143
00:02:50,040 --> 00:02:52,550
created by notebook LM we'll get to hear
more from The Notebook lmm team later

144
00:02:52,550 --> 00:02:52,560
more from The Notebook lmm team later
 

145
00:02:52,560 --> 00:02:54,470
more from The Notebook lmm team later
this week you should have taken a look

146
00:02:54,470 --> 00:02:54,480
this week you should have taken a look
 

147
00:02:54,480 --> 00:02:56,309
this week you should have taken a look
at some of the white papers that we have

148
00:02:56,309 --> 00:02:56,319
at some of the white papers that we have
 

149
00:02:56,319 --> 00:02:58,830
at some of the white papers that we have
around this first day's content and then

150
00:02:58,830 --> 00:02:58,840
around this first day's content and then
 

151
00:02:58,840 --> 00:03:00,949
around this first day's content and then
also importantly the kaggle code lab

152
00:03:00,949 --> 00:03:00,959
also importantly the kaggle code lab
 

153
00:03:00,959 --> 00:03:02,589
also importantly the kaggle code lab
which an aunt will be walking through

154
00:03:02,589 --> 00:03:02,599
which an aunt will be walking through
 

155
00:03:02,599 --> 00:03:05,670
which an aunt will be walking through
towards the end of today um but before

156
00:03:05,670 --> 00:03:05,680
towards the end of today um but before
 

157
00:03:05,680 --> 00:03:08,630
towards the end of today um but before
we go into this I want to have a huge

158
00:03:08,630 --> 00:03:08,640
we go into this I want to have a huge
 

159
00:03:08,640 --> 00:03:10,789
we go into this I want to have a huge
shout out to our generative AI course

160
00:03:10,789 --> 00:03:10,799
shout out to our generative AI course
 

161
00:03:10,799 --> 00:03:13,070
shout out to our generative AI course
moderators um this is the team that's

162
00:03:13,070 --> 00:03:13,080
moderators um this is the team that's
 

163
00:03:13,080 --> 00:03:15,030
moderators um this is the team that's
been tirelessly answering all of the

164
00:03:15,030 --> 00:03:15,040
been tirelessly answering all of the
 

165
00:03:15,040 --> 00:03:16,589
been tirelessly answering all of the
questions that y'all have been asking on

166
00:03:16,589 --> 00:03:16,599
questions that y'all have been asking on
 

167
00:03:16,599 --> 00:03:18,149
questions that y'all have been asking on
Discord they've been helping spot

168
00:03:18,149 --> 00:03:18,159
Discord they've been helping spot
 

169
00:03:18,159 --> 00:03:19,630
Discord they've been helping spot
questions that could be useful in the

170
00:03:19,630 --> 00:03:19,640
questions that could be useful in the
 

171
00:03:19,640 --> 00:03:22,990
questions that could be useful in the
Q&A session um so thank you so much a

172
00:03:22,990 --> 00:03:23,000
Q&A session um so thank you so much a
 

173
00:03:23,000 --> 00:03:25,670
Q&A session um so thank you so much a
virtual Round of Applause to everybody

174
00:03:25,670 --> 00:03:25,680
virtual Round of Applause to everybody
 

175
00:03:25,680 --> 00:03:28,270
virtual Round of Applause to everybody
who's been um making this course happen

176
00:03:28,270 --> 00:03:28,280
who's been um making this course happen
 

177
00:03:28,280 --> 00:03:30,750
who's been um making this course happen
if you see them digitally uh or in

178
00:03:30,750 --> 00:03:30,760
if you see them digitally uh or in
 

179
00:03:30,760 --> 00:03:34,270
if you see them digitally uh or in
person say thank you um and with that

180
00:03:34,270 --> 00:03:34,280
person say thank you um and with that
 

181
00:03:34,280 --> 00:03:37,750
person say thank you um and with that
let's head into our Q&A so today I am

182
00:03:37,750 --> 00:03:37,760
let's head into our Q&A so today I am
 

183
00:03:37,760 --> 00:03:40,309
let's head into our Q&A so today I am
honored beyond belief to welcome Logan

184
00:03:40,309 --> 00:03:40,319
honored beyond belief to welcome Logan
 

185
00:03:40,319 --> 00:03:43,270
honored beyond belief to welcome Logan
Kilpatrick Warren Barkley Kieran melan

186
00:03:43,270 --> 00:03:43,280
Kilpatrick Warren Barkley Kieran melan
 

187
00:03:43,280 --> 00:03:46,630
Kilpatrick Warren Barkley Kieran melan
um Arena Ziggler and Matt boso to um to

188
00:03:46,630 --> 00:03:46,640
um Arena Ziggler and Matt boso to um to
 

189
00:03:46,640 --> 00:03:49,750
um Arena Ziggler and Matt boso to um to
the stage um they are our expert hosts

190
00:03:49,750 --> 00:03:49,760
the stage um they are our expert hosts
 

191
00:03:49,760 --> 00:03:52,710
the stage um they are our expert hosts
for day one all about prompt engineering

192
00:03:52,710 --> 00:03:52,720
for day one all about prompt engineering
 

193
00:03:52,720 --> 00:03:57,069
for day one all about prompt engineering
and foundational models um and I I with

194
00:03:57,069 --> 00:03:57,079
and foundational models um and I I with
 

195
00:03:57,079 --> 00:03:59,949
and foundational models um and I I with
that I I'm going to to go ahead and get

196
00:03:59,949 --> 00:03:59,959
that I I'm going to to go ahead and get
 

197
00:03:59,959 --> 00:04:04,470
that I I'm going to to go ahead and get
into our first question um so first uh

198
00:04:04,470 --> 00:04:04,480
into our first question um so first uh
 

199
00:04:04,480 --> 00:04:08,270
into our first question um so first uh
first question is for Logan and for Matt

200
00:04:08,270 --> 00:04:08,280
first question is for Logan and for Matt
 

201
00:04:08,280 --> 00:04:11,470
first question is for Logan and for Matt
um I and taking a look at it now tell us

202
00:04:11,470 --> 00:04:11,480
um I and taking a look at it now tell us
 

203
00:04:11,480 --> 00:04:15,110
um I and taking a look at it now tell us
a little bit about AI studio um what

204
00:04:15,110 --> 00:04:15,120
a little bit about AI studio um what
 

205
00:04:15,120 --> 00:04:16,710
a little bit about AI studio um what
capabilities does it unlock for

206
00:04:16,710 --> 00:04:16,720
capabilities does it unlock for
 

207
00:04:16,720 --> 00:04:18,870
capabilities does it unlock for
developers and how does it bridge the

208
00:04:18,870 --> 00:04:18,880
developers and how does it bridge the
 

209
00:04:18,880 --> 00:04:20,870
developers and how does it bridge the
gap between Google deep mind's latest

210
00:04:20,870 --> 00:04:20,880
gap between Google deep mind's latest
 

211
00:04:20,880 --> 00:04:22,950
gap between Google deep mind's latest
research and all of our great Google

212
00:04:22,950 --> 00:04:22,960
research and all of our great Google
 

213
00:04:22,960 --> 00:04:23,990
research and all of our great Google
Cloud

214
00:04:23,990 --> 00:04:24,000
Cloud
 

215
00:04:24,000 --> 00:04:27,430
Cloud
tools want to go first Logan yeah sure

216
00:04:27,430 --> 00:04:27,440
tools want to go first Logan yeah sure
 

217
00:04:27,440 --> 00:04:30,670
tools want to go first Logan yeah sure
I'm uh I'm happy to Pig uh this is this

218
00:04:30,670 --> 00:04:30,680
I'm uh I'm happy to Pig uh this is this
 

219
00:04:30,680 --> 00:04:32,430
I'm uh I'm happy to Pig uh this is this
is a ton of fun to tell folks about

220
00:04:32,430 --> 00:04:32,440
is a ton of fun to tell folks about
 

221
00:04:32,440 --> 00:04:33,749
is a ton of fun to tell folks about
about AI studio and everything that

222
00:04:33,749 --> 00:04:33,759
about AI studio and everything that
 

223
00:04:33,759 --> 00:04:35,990
about AI studio and everything that
we're doing the sort of high level

224
00:04:35,990 --> 00:04:36,000
we're doing the sort of high level
 

225
00:04:36,000 --> 00:04:38,150
we're doing the sort of high level
mental model of this is aiio is intended

226
00:04:38,150 --> 00:04:38,160
mental model of this is aiio is intended
 

227
00:04:38,160 --> 00:04:40,790
mental model of this is aiio is intended
to be the F the fast path to access the

228
00:04:40,790 --> 00:04:40,800
to be the F the fast path to access the
 

229
00:04:40,800 --> 00:04:42,150
to be the F the fast path to access the
latest Gemini models the latest

230
00:04:42,150 --> 00:04:42,160
latest Gemini models the latest
 

231
00:04:42,160 --> 00:04:44,070
latest Gemini models the latest
generative AI models coming out of

232
00:04:44,070 --> 00:04:44,080
generative AI models coming out of
 

233
00:04:44,080 --> 00:04:45,670
generative AI models coming out of
Google and and specifically coming out

234
00:04:45,670 --> 00:04:45,680
Google and and specifically coming out
 

235
00:04:45,680 --> 00:04:47,950
Google and and specifically coming out
of Deep Mind um so if you're a developer

236
00:04:47,950 --> 00:04:47,960
of Deep Mind um so if you're a developer
 

237
00:04:47,960 --> 00:04:50,189
of Deep Mind um so if you're a developer
and you want to see what's possible with

238
00:04:50,189 --> 00:04:50,199
and you want to see what's possible with
 

239
00:04:50,199 --> 00:04:51,790
and you want to see what's possible with
with the Gemini models and page I know

240
00:04:51,790 --> 00:04:51,800
with the Gemini models and page I know
 

241
00:04:51,800 --> 00:04:53,870
with the Gemini models and page I know
you and I are do this all the time you

242
00:04:53,870 --> 00:04:53,880
you and I are do this all the time you
 

243
00:04:53,880 --> 00:04:55,270
you and I are do this all the time you
have some idea of what you want to build

244
00:04:55,270 --> 00:04:55,280
have some idea of what you want to build
 

245
00:04:55,280 --> 00:04:57,270
have some idea of what you want to build
with ai ai studio is a great place to

246
00:04:57,270 --> 00:04:57,280
with ai ai studio is a great place to
 

247
00:04:57,280 --> 00:04:59,350
with ai ai studio is a great place to
just show up test can the model actually

248
00:04:59,350 --> 00:04:59,360
just show up test can the model actually
 

249
00:04:59,360 --> 00:05:00,870
just show up test can the model actually
do this thing does it have this

250
00:05:00,870 --> 00:05:00,880
do this thing does it have this
 

251
00:05:00,880 --> 00:05:03,230
do this thing does it have this
capability baked in um and then with a

252
00:05:03,230 --> 00:05:03,240
capability baked in um and then with a
 

253
00:05:03,240 --> 00:05:05,110
capability baked in um and then with a
single click you can go and grab a bunch

254
00:05:05,110 --> 00:05:05,120
single click you can go and grab a bunch
 

255
00:05:05,120 --> 00:05:06,790
single click you can go and grab a bunch
of code and and actually start building

256
00:05:06,790 --> 00:05:06,800
of code and and actually start building
 

257
00:05:06,800 --> 00:05:09,510
of code and and actually start building
that idea um in python or JavaScript or

258
00:05:09,510 --> 00:05:09,520
that idea um in python or JavaScript or
 

259
00:05:09,520 --> 00:05:11,510
that idea um in python or JavaScript or
whatever your language of choic is um

260
00:05:11,510 --> 00:05:11,520
whatever your language of choic is um
 

261
00:05:11,520 --> 00:05:13,310
whatever your language of choic is um
you can also get an API key and and

262
00:05:13,310 --> 00:05:13,320
you can also get an API key and and
 

263
00:05:13,320 --> 00:05:14,670
you can also get an API key and and
start building with the API really

264
00:05:14,670 --> 00:05:14,680
start building with the API really
 

265
00:05:14,680 --> 00:05:17,950
start building with the API really
quickly so really sort of the the path

266
00:05:17,950 --> 00:05:17,960
quickly so really sort of the the path
 

267
00:05:17,960 --> 00:05:20,950
quickly so really sort of the the path
to accelerate um idea to actually

268
00:05:20,950 --> 00:05:20,960
to accelerate um idea to actually
 

269
00:05:20,960 --> 00:05:22,909
to accelerate um idea to actually
building something with Gemini

270
00:05:22,909 --> 00:05:22,919
building something with Gemini
 

271
00:05:22,919 --> 00:05:25,350
building something with Gemini
absolutely and I know that uh just

272
00:05:25,350 --> 00:05:25,360
absolutely and I know that uh just
 

273
00:05:25,360 --> 00:05:27,550
absolutely and I know that uh just
recently we've been focused on unifying

274
00:05:27,550 --> 00:05:27,560
recently we've been focused on unifying
 

275
00:05:27,560 --> 00:05:30,629
recently we've been focused on unifying
the sdks such that you know when you hit

276
00:05:30,629 --> 00:05:30,639
the sdks such that you know when you hit
 

277
00:05:30,639 --> 00:05:32,230
the sdks such that you know when you hit
that get code button it's a really

278
00:05:32,230 --> 00:05:32,240
that get code button it's a really
 

279
00:05:32,240 --> 00:05:35,070
that get code button it's a really
seamless path to get from AI Studio code

280
00:05:35,070 --> 00:05:35,080
seamless path to get from AI Studio code
 

281
00:05:35,080 --> 00:05:38,029
seamless path to get from AI Studio code
to code that works with vertex AI um so

282
00:05:38,029 --> 00:05:38,039
to code that works with vertex AI um so
 

283
00:05:38,039 --> 00:05:40,629
to code that works with vertex AI um so
it's been really really cool to see um

284
00:05:40,629 --> 00:05:40,639
it's been really really cool to see um
 

285
00:05:40,639 --> 00:05:43,150
it's been really really cool to see um
Matt is there anything you want to add

286
00:05:43,150 --> 00:05:43,160
Matt is there anything you want to add
 

287
00:05:43,160 --> 00:05:46,230
Matt is there anything you want to add
no that's what we going say people often

288
00:05:46,230 --> 00:05:46,240
no that's what we going say people often
 

289
00:05:46,240 --> 00:05:49,150
no that's what we going say people often
ask why we have two different products

290
00:05:49,150 --> 00:05:49,160
ask why we have two different products
 

291
00:05:49,160 --> 00:05:51,150
ask why we have two different products
vertex and the I studio and Warren is

292
00:05:51,150 --> 00:05:51,160
vertex and the I studio and Warren is
 

293
00:05:51,160 --> 00:05:54,070
vertex and the I studio and Warren is
going to talk about vertex soon but uh

294
00:05:54,070 --> 00:05:54,080
going to talk about vertex soon but uh
 

295
00:05:54,080 --> 00:05:57,029
going to talk about vertex soon but uh
we want to have both a unified developer

296
00:05:57,029 --> 00:05:57,039
we want to have both a unified developer
 

297
00:05:57,039 --> 00:05:58,710
we want to have both a unified developer
experience like you all say that you

298
00:05:58,710 --> 00:05:58,720
experience like you all say that you
 

299
00:05:58,720 --> 00:06:00,430
experience like you all say that you
shouldn't have to Cho choose one or the

300
00:06:00,430 --> 00:06:00,440
shouldn't have to Cho choose one or the
 

301
00:06:00,440 --> 00:06:02,950
shouldn't have to Cho choose one or the
other before you start coding like you

302
00:06:02,950 --> 00:06:02,960
other before you start coding like you
 

303
00:06:02,960 --> 00:06:05,309
other before you start coding like you
can just you know pick an SDK and just

304
00:06:05,309 --> 00:06:05,319
can just you know pick an SDK and just
 

305
00:06:05,319 --> 00:06:07,629
can just you know pick an SDK and just
work with it uh but there is value

306
00:06:07,629 --> 00:06:07,639
work with it uh but there is value
 

307
00:06:07,639 --> 00:06:09,390
work with it uh but there is value
having two different products for two

308
00:06:09,390 --> 00:06:09,400
having two different products for two
 

309
00:06:09,400 --> 00:06:11,909
having two different products for two
different types of use case I studio is

310
00:06:11,909 --> 00:06:11,919
different types of use case I studio is
 

311
00:06:11,919 --> 00:06:14,909
different types of use case I studio is
very very simple it's not an mlops

312
00:06:14,909 --> 00:06:14,919
very very simple it's not an mlops
 

313
00:06:14,919 --> 00:06:17,390
very very simple it's not an mlops
platform it doesn't have a model garden

314
00:06:17,390 --> 00:06:17,400
platform it doesn't have a model garden
 

315
00:06:17,400 --> 00:06:19,469
platform it doesn't have a model garden
with many models right it's just

316
00:06:19,469 --> 00:06:19,479
with many models right it's just
 

317
00:06:19,479 --> 00:06:22,510
with many models right it's just
Google's models uh is the simplest sort

318
00:06:22,510 --> 00:06:22,520
Google's models uh is the simplest sort
 

319
00:06:22,520 --> 00:06:25,029
Google's models uh is the simplest sort
of getting started experience where

320
00:06:25,029 --> 00:06:25,039
of getting started experience where
 

321
00:06:25,039 --> 00:06:26,990
of getting started experience where
vertex is an Enterprise product and it

322
00:06:26,990 --> 00:06:27,000
vertex is an Enterprise product and it
 

323
00:06:27,000 --> 00:06:29,150
vertex is an Enterprise product and it
has a bunch of capabilities that AI

324
00:06:29,150 --> 00:06:29,160
has a bunch of capabilities that AI
 

325
00:06:29,160 --> 00:06:31,990
has a bunch of capabilities that AI
Studio does not have so there is very

326
00:06:31,990 --> 00:06:32,000
Studio does not have so there is very
 

327
00:06:32,000 --> 00:06:33,550
Studio does not have so there is very
having these two different things as

328
00:06:33,550 --> 00:06:33,560
having these two different things as
 

329
00:06:33,560 --> 00:06:35,189
having these two different things as
long as we have a unified developer

330
00:06:35,189 --> 00:06:35,199
long as we have a unified developer
 

331
00:06:35,199 --> 00:06:36,390
long as we have a unified developer
experience so you don't have to write

332
00:06:36,390 --> 00:06:36,400
experience so you don't have to write
 

333
00:06:36,400 --> 00:06:38,589
experience so you don't have to write
your code twice and and we are doing

334
00:06:38,589 --> 00:06:38,599
your code twice and and we are doing
 

335
00:06:38,599 --> 00:06:41,150
your code twice and and we are doing
that excellent and people can try it out

336
00:06:41,150 --> 00:06:41,160
that excellent and people can try it out
 

337
00:06:41,160 --> 00:06:45,270
that excellent and people can try it out
today uh at AI dodev I believe uh that

338
00:06:45,270 --> 00:06:45,280
today uh at AI dodev I believe uh that
 

339
00:06:45,280 --> 00:06:48,230
today uh at AI dodev I believe uh that
which is the coolest URL that uh that I

340
00:06:48,230 --> 00:06:48,240
which is the coolest URL that uh that I
 

341
00:06:48,240 --> 00:06:51,510
which is the coolest URL that uh that I
have seen in recent so very very excited

342
00:06:51,510 --> 00:06:51,520
have seen in recent so very very excited
 

343
00:06:51,520 --> 00:06:54,510
have seen in recent so very very excited
thank you for the great answer um second

344
00:06:54,510 --> 00:06:54,520
thank you for the great answer um second
 

345
00:06:54,520 --> 00:06:57,390
thank you for the great answer um second
question for Kieran um how has prompt

346
00:06:57,390 --> 00:06:57,400
question for Kieran um how has prompt
 

347
00:06:57,400 --> 00:06:59,670
question for Kieran um how has prompt
engineering evolved since the early days

348
00:06:59,670 --> 00:06:59,680
engineering evolved since the early days
 

349
00:06:59,680 --> 00:07:01,270
engineering evolved since the early days
of large language models so as an

350
00:07:01,270 --> 00:07:01,280
of large language models so as an
 

351
00:07:01,280 --> 00:07:04,110
of large language models so as an
example Palm 2 gpt3 and the like how do

352
00:07:04,110 --> 00:07:04,120
example Palm 2 gpt3 and the like how do
 

353
00:07:04,120 --> 00:07:05,909
example Palm 2 gpt3 and the like how do
you think it will evolve over time

354
00:07:05,909 --> 00:07:05,919
you think it will evolve over time
 

355
00:07:05,919 --> 00:07:07,790
you think it will evolve over time
especially given the rise and popularity

356
00:07:07,790 --> 00:07:07,800
especially given the rise and popularity
 

357
00:07:07,800 --> 00:07:09,550
especially given the rise and popularity
for multimodal models and you've been

358
00:07:09,550 --> 00:07:09,560
for multimodal models and you've been
 

359
00:07:09,560 --> 00:07:11,990
for multimodal models and you've been
doing this for quite some time I believe

360
00:07:11,990 --> 00:07:12,000
doing this for quite some time I believe
 

361
00:07:12,000 --> 00:07:14,110
doing this for quite some time I believe
so this this is a fascinating question I

362
00:07:14,110 --> 00:07:14,120
so this this is a fascinating question I
 

363
00:07:14,120 --> 00:07:15,749
so this this is a fascinating question I
think it's fascinating to look back at

364
00:07:15,749 --> 00:07:15,759
think it's fascinating to look back at
 

365
00:07:15,759 --> 00:07:17,350
think it's fascinating to look back at
the evolution not just of prompt

366
00:07:17,350 --> 00:07:17,360
the evolution not just of prompt
 

367
00:07:17,360 --> 00:07:19,510
the evolution not just of prompt
engineering but s of language models

368
00:07:19,510 --> 00:07:19,520
engineering but s of language models
 

369
00:07:19,520 --> 00:07:21,909
engineering but s of language models
themselves um so if you look at how

370
00:07:21,909 --> 00:07:21,919
themselves um so if you look at how
 

371
00:07:21,919 --> 00:07:23,270
themselves um so if you look at how
language models began they weren't

372
00:07:23,270 --> 00:07:23,280
language models began they weren't
 

373
00:07:23,280 --> 00:07:25,189
language models began they weren't
specifically designed to do the tasks

374
00:07:25,189 --> 00:07:25,199
specifically designed to do the tasks
 

375
00:07:25,199 --> 00:07:26,670
specifically designed to do the tasks
that we're using them for now they were

376
00:07:26,670 --> 00:07:26,680
that we're using them for now they were
 

377
00:07:26,680 --> 00:07:28,309
that we're using them for now they were
designed as probabilistic models that

378
00:07:28,309 --> 00:07:28,319
designed as probabilistic models that
 

379
00:07:28,319 --> 00:07:31,070
designed as probabilistic models that
are able to predict the next um the next

380
00:07:31,070 --> 00:07:31,080
are able to predict the next um the next
 

381
00:07:31,080 --> 00:07:34,189
are able to predict the next um the next
token or word in a sentence and um in

382
00:07:34,189 --> 00:07:34,199
token or word in a sentence and um in
 

383
00:07:34,199 --> 00:07:35,909
token or word in a sentence and um in
the early days we got these models as

384
00:07:35,909 --> 00:07:35,919
the early days we got these models as
 

385
00:07:35,919 --> 00:07:38,950
the early days we got these models as
tools and um really there was a lot of

386
00:07:38,950 --> 00:07:38,960
tools and um really there was a lot of
 

387
00:07:38,960 --> 00:07:41,070
tools and um really there was a lot of
trial and error to find techniques that

388
00:07:41,070 --> 00:07:41,080
trial and error to find techniques that
 

389
00:07:41,080 --> 00:07:43,990
trial and error to find techniques that
worked well with them and um Ena them to

390
00:07:43,990 --> 00:07:44,000
worked well with them and um Ena them to
 

391
00:07:44,000 --> 00:07:46,589
worked well with them and um Ena them to
you do useful things um so this led to

392
00:07:46,589 --> 00:07:46,599
you do useful things um so this led to
 

393
00:07:46,599 --> 00:07:48,510
you do useful things um so this led to
things like Chain of Thought and F shop

394
00:07:48,510 --> 00:07:48,520
things like Chain of Thought and F shop
 

395
00:07:48,520 --> 00:07:50,670
things like Chain of Thought and F shop
prompting adding personas things like

396
00:07:50,670 --> 00:07:50,680
prompting adding personas things like
 

397
00:07:50,680 --> 00:07:52,909
prompting adding personas things like
that they they were just techniques that

398
00:07:52,909 --> 00:07:52,919
that they they were just techniques that
 

399
00:07:52,919 --> 00:07:54,790
that they they were just techniques that
um were essentially discovered rather

400
00:07:54,790 --> 00:07:54,800
um were essentially discovered rather
 

401
00:07:54,800 --> 00:07:57,950
um were essentially discovered rather
than designed um but in parallel we put

402
00:07:57,950 --> 00:07:57,960
than designed um but in parallel we put
 

403
00:07:57,960 --> 00:08:00,749
than designed um but in parallel we put
a huge amount of work and effort into

404
00:08:00,749 --> 00:08:00,759
a huge amount of work and effort into
 

405
00:08:00,759 --> 00:08:03,749
a huge amount of work and effort into
instruction tuning the models um uh so

406
00:08:03,749 --> 00:08:03,759
instruction tuning the models um uh so
 

407
00:08:03,759 --> 00:08:05,430
instruction tuning the models um uh so
this is really us teaching the models

408
00:08:05,430 --> 00:08:05,440
this is really us teaching the models
 

409
00:08:05,440 --> 00:08:07,909
this is really us teaching the models
how to behave in certain scenarios um

410
00:08:07,909 --> 00:08:07,919
how to behave in certain scenarios um
 

411
00:08:07,919 --> 00:08:10,309
how to behave in certain scenarios um
this has progressed really rapidly um uh

412
00:08:10,309 --> 00:08:10,319
this has progressed really rapidly um uh
 

413
00:08:10,319 --> 00:08:12,029
this has progressed really rapidly um uh
and it's moving to a future where like

414
00:08:12,029 --> 00:08:12,039
and it's moving to a future where like
 

415
00:08:12,039 --> 00:08:13,710
and it's moving to a future where like
all the techniques are really designed

416
00:08:13,710 --> 00:08:13,720
all the techniques are really designed
 

417
00:08:13,720 --> 00:08:16,029
all the techniques are really designed
techniques rather um and they're

418
00:08:16,029 --> 00:08:16,039
techniques rather um and they're
 

419
00:08:16,039 --> 00:08:19,390
techniques rather um and they're
intentional um so as a team within

420
00:08:19,390 --> 00:08:19,400
intentional um so as a team within
 

421
00:08:19,400 --> 00:08:22,070
intentional um so as a team within
Google it's it's our job to make llms as

422
00:08:22,070 --> 00:08:22,080
Google it's it's our job to make llms as
 

423
00:08:22,080 --> 00:08:25,029
Google it's it's our job to make llms as
intuitive and easy to use as possible um

424
00:08:25,029 --> 00:08:25,039
intuitive and easy to use as possible um
 

425
00:08:25,039 --> 00:08:26,790
intuitive and easy to use as possible um
and when I think about sort of prompt

426
00:08:26,790 --> 00:08:26,800
and when I think about sort of prompt
 

427
00:08:26,800 --> 00:08:29,230
and when I think about sort of prompt
engineering I think of two sort of sets

428
00:08:29,230 --> 00:08:29,240
engineering I think of two sort of sets
 

429
00:08:29,240 --> 00:08:32,070
engineering I think of two sort of sets
of Persona of user bases um you can

430
00:08:32,070 --> 00:08:32,080
of Persona of user bases um you can
 

431
00:08:32,080 --> 00:08:34,949
of Persona of user bases um you can
think of end users um so those are folks

432
00:08:34,949 --> 00:08:34,959
think of end users um so those are folks
 

433
00:08:34,959 --> 00:08:36,709
think of end users um so those are folks
maybe like using the Gemini app trying

434
00:08:36,709 --> 00:08:36,719
maybe like using the Gemini app trying
 

435
00:08:36,719 --> 00:08:39,990
maybe like using the Gemini app trying
to use Gemini to answer questions um on

436
00:08:39,990 --> 00:08:40,000
to use Gemini to answer questions um on
 

437
00:08:40,000 --> 00:08:41,750
to use Gemini to answer questions um on
a daily basis you know they may not be

438
00:08:41,750 --> 00:08:41,760
a daily basis you know they may not be
 

439
00:08:41,760 --> 00:08:44,630
a daily basis you know they may not be
experts in prompting um and like really

440
00:08:44,630 --> 00:08:44,640
experts in prompting um and like really
 

441
00:08:44,640 --> 00:08:46,350
experts in prompting um and like really
what we're trying to do there is get to

442
00:08:46,350 --> 00:08:46,360
what we're trying to do there is get to
 

443
00:08:46,360 --> 00:08:48,829
what we're trying to do there is get to
a state where like no prompt engineering

444
00:08:48,829 --> 00:08:48,839
a state where like no prompt engineering
 

445
00:08:48,839 --> 00:08:51,110
a state where like no prompt engineering
is required at all if um you should just

446
00:08:51,110 --> 00:08:51,120
is required at all if um you should just
 

447
00:08:51,120 --> 00:08:52,829
is required at all if um you should just
be able to answer a user's reasonable

448
00:08:52,829 --> 00:08:52,839
be able to answer a user's reasonable
 

449
00:08:52,839 --> 00:08:55,310
be able to answer a user's reasonable
request and um if there's something

450
00:08:55,310 --> 00:08:55,320
request and um if there's something
 

451
00:08:55,320 --> 00:08:57,550
request and um if there's something
that's not clear then the app should um

452
00:08:57,550 --> 00:08:57,560
that's not clear then the app should um
 

453
00:08:57,560 --> 00:08:59,430
that's not clear then the app should um
just be asking for clarification or

454
00:08:59,430 --> 00:08:59,440
just be asking for clarification or
 

455
00:08:59,440 --> 00:09:01,069
just be asking for clarification or
offing multiple suggestions things like

456
00:09:01,069 --> 00:09:01,079
offing multiple suggestions things like
 

457
00:09:01,079 --> 00:09:03,949
offing multiple suggestions things like
that um but then you can also think

458
00:09:03,949 --> 00:09:03,959
that um but then you can also think
 

459
00:09:03,959 --> 00:09:05,550
that um but then you can also think
about people trying to build

460
00:09:05,550 --> 00:09:05,560
about people trying to build
 

461
00:09:05,560 --> 00:09:08,190
about people trying to build
applications with other thems and um

462
00:09:08,190 --> 00:09:08,200
applications with other thems and um
 

463
00:09:08,200 --> 00:09:10,710
applications with other thems and um
really what we're trying to do here is

464
00:09:10,710 --> 00:09:10,720
really what we're trying to do here is
 

465
00:09:10,720 --> 00:09:13,630
really what we're trying to do here is
um make that process as simple and easy

466
00:09:13,630 --> 00:09:13,640
um make that process as simple and easy
 

467
00:09:13,640 --> 00:09:16,550
um make that process as simple and easy
as possible like if Gemini um doesn't

468
00:09:16,550 --> 00:09:16,560
as possible like if Gemini um doesn't
 

469
00:09:16,560 --> 00:09:17,710
as possible like if Gemini um doesn't
understand something in your prompt then

470
00:09:17,710 --> 00:09:17,720
understand something in your prompt then
 

471
00:09:17,720 --> 00:09:19,509
understand something in your prompt then
it should be asking you about that um

472
00:09:19,509 --> 00:09:19,519
it should be asking you about that um
 

473
00:09:19,519 --> 00:09:21,150
it should be asking you about that um
when you're developing your application

474
00:09:21,150 --> 00:09:21,160
when you're developing your application
 

475
00:09:21,160 --> 00:09:23,509
when you're developing your application
rather than when you're deploying it um

476
00:09:23,509 --> 00:09:23,519
rather than when you're deploying it um
 

477
00:09:23,519 --> 00:09:25,150
rather than when you're deploying it um
and really what what we're doing in

478
00:09:25,150 --> 00:09:25,160
and really what what we're doing in
 

479
00:09:25,160 --> 00:09:26,350
and really what what we're doing in
parallel is kind of coming up with

480
00:09:26,350 --> 00:09:26,360
parallel is kind of coming up with
 

481
00:09:26,360 --> 00:09:27,910
parallel is kind of coming up with
design patterns you know much the same

482
00:09:27,910 --> 00:09:27,920
design patterns you know much the same
 

483
00:09:27,920 --> 00:09:30,030
design patterns you know much the same
way that programming language is evolved

484
00:09:30,030 --> 00:09:30,040
way that programming language is evolved
 

485
00:09:30,040 --> 00:09:33,269
way that programming language is evolved
um and you got good design patterns for

486
00:09:33,269 --> 00:09:33,279
um and you got good design patterns for
 

487
00:09:33,279 --> 00:09:37,190
um and you got good design patterns for
um like building applications um uh in

488
00:09:37,190 --> 00:09:37,200
um like building applications um uh in
 

489
00:09:37,200 --> 00:09:38,630
um like building applications um uh in
like standard programming languages I

490
00:09:38,630 --> 00:09:38,640
like standard programming languages I
 

491
00:09:38,640 --> 00:09:40,670
like standard programming languages I
see the same same thing happening in LM

492
00:09:40,670 --> 00:09:40,680
see the same same thing happening in LM
 

493
00:09:40,680 --> 00:09:43,030
see the same same thing happening in LM
so you can imagine sort of guide books

494
00:09:43,030 --> 00:09:43,040
so you can imagine sort of guide books
 

495
00:09:43,040 --> 00:09:44,990
so you can imagine sort of guide books
and things like that allowing you to

496
00:09:44,990 --> 00:09:45,000
and things like that allowing you to
 

497
00:09:45,000 --> 00:09:48,269
and things like that allowing you to
sort of almost take off the shelf a um

498
00:09:48,269 --> 00:09:48,279
sort of almost take off the shelf a um
 

499
00:09:48,279 --> 00:09:50,110
sort of almost take off the shelf a um
uh like an example bit of code and then

500
00:09:50,110 --> 00:09:50,120
uh like an example bit of code and then
 

501
00:09:50,120 --> 00:09:52,150
uh like an example bit of code and then
customize that um to your specific

502
00:09:52,150 --> 00:09:52,160
customize that um to your specific
 

503
00:09:52,160 --> 00:09:54,630
customize that um to your specific
application um when we talk about

504
00:09:54,630 --> 00:09:54,640
application um when we talk about
 

505
00:09:54,640 --> 00:09:56,790
application um when we talk about
multimodal um I don't think anything

506
00:09:56,790 --> 00:09:56,800
multimodal um I don't think anything
 

507
00:09:56,800 --> 00:09:58,110
multimodal um I don't think anything
should really be changing here in terms

508
00:09:58,110 --> 00:09:58,120
should really be changing here in terms
 

509
00:09:58,120 --> 00:10:00,150
should really be changing here in terms
of our prompting techniques um you know

510
00:10:00,150 --> 00:10:00,160
of our prompting techniques um you know
 

511
00:10:00,160 --> 00:10:01,190
of our prompting techniques um you know
one thing is that we're a little bit

512
00:10:01,190 --> 00:10:01,200
one thing is that we're a little bit
 

513
00:10:01,200 --> 00:10:03,509
one thing is that we're a little bit
earlier in our journey um for multimodal

514
00:10:03,509 --> 00:10:03,519
earlier in our journey um for multimodal
 

515
00:10:03,519 --> 00:10:07,190
earlier in our journey um for multimodal
prompts than um uh we are for text so

516
00:10:07,190 --> 00:10:07,200
prompts than um uh we are for text so
 

517
00:10:07,200 --> 00:10:09,110
prompts than um uh we are for text so
like maybe we're still understanding

518
00:10:09,110 --> 00:10:09,120
like maybe we're still understanding
 

519
00:10:09,120 --> 00:10:11,590
like maybe we're still understanding
where the gaps are um but I think it's

520
00:10:11,590 --> 00:10:11,600
where the gaps are um but I think it's
 

521
00:10:11,600 --> 00:10:13,750
where the gaps are um but I think it's
actually really um really cool to try

522
00:10:13,750 --> 00:10:13,760
actually really um really cool to try
 

523
00:10:13,760 --> 00:10:15,750
actually really um really cool to try
prompting um like the Gemini app by

524
00:10:15,750 --> 00:10:15,760
prompting um like the Gemini app by
 

525
00:10:15,760 --> 00:10:18,150
prompting um like the Gemini app by
voice um because what you see there or

526
00:10:18,150 --> 00:10:18,160
voice um because what you see there or
 

527
00:10:18,160 --> 00:10:20,230
voice um because what you see there or
certainly what I see is um a really

528
00:10:20,230 --> 00:10:20,240
certainly what I see is um a really
 

529
00:10:20,240 --> 00:10:23,350
certainly what I see is um a really
impressive performance um despite the

530
00:10:23,350 --> 00:10:23,360
impressive performance um despite the
 

531
00:10:23,360 --> 00:10:25,230
impressive performance um despite the
prompts really being less well written

532
00:10:25,230 --> 00:10:25,240
prompts really being less well written
 

533
00:10:25,240 --> 00:10:27,230
prompts really being less well written
than they are in text um like if I'm

534
00:10:27,230 --> 00:10:27,240
than they are in text um like if I'm
 

535
00:10:27,240 --> 00:10:28,670
than they are in text um like if I'm
talking to the Gemini app it'll be full

536
00:10:28,670 --> 00:10:28,680
talking to the Gemini app it'll be full
 

537
00:10:28,680 --> 00:10:29,870
talking to the Gemini app it'll be full
of ums and

538
00:10:29,870 --> 00:10:29,880
of ums and
 

539
00:10:29,880 --> 00:10:31,310
of ums and
and I might go back and correct my

540
00:10:31,310 --> 00:10:31,320
and I might go back and correct my
 

541
00:10:31,320 --> 00:10:32,750
and I might go back and correct my
sentence you know I wouldn't do that if

542
00:10:32,750 --> 00:10:32,760
sentence you know I wouldn't do that if
 

543
00:10:32,760 --> 00:10:34,990
sentence you know I wouldn't do that if
I was text prompting and yet what you

544
00:10:34,990 --> 00:10:35,000
I was text prompting and yet what you
 

545
00:10:35,000 --> 00:10:39,110
I was text prompting and yet what you
really see is um like a very coherent um

546
00:10:39,110 --> 00:10:39,120
really see is um like a very coherent um
 

547
00:10:39,120 --> 00:10:43,910
really see is um like a very coherent um
fluent process um excellent I I love the

548
00:10:43,910 --> 00:10:43,920
fluent process um excellent I I love the
 

549
00:10:43,920 --> 00:10:47,310
fluent process um excellent I I love the
I love the the recommendation or the the

550
00:10:47,310 --> 00:10:47,320
I love the the recommendation or the the
 

551
00:10:47,320 --> 00:10:49,230
I love the the recommendation or the the
the sort of forecast that in the future

552
00:10:49,230 --> 00:10:49,240
the sort of forecast that in the future
 

553
00:10:49,240 --> 00:10:51,310
the sort of forecast that in the future
models will be asking for clarifying

554
00:10:51,310 --> 00:10:51,320
models will be asking for clarifying
 

555
00:10:51,320 --> 00:10:53,670
models will be asking for clarifying
questions or or they'll be pulling in

556
00:10:53,670 --> 00:10:53,680
questions or or they'll be pulling in
 

557
00:10:53,680 --> 00:10:56,910
questions or or they'll be pulling in
additional context to help support users

558
00:10:56,910 --> 00:10:56,920
additional context to help support users
 

559
00:10:56,920 --> 00:10:59,350
additional context to help support users
requests um and make sure that they have

560
00:10:59,350 --> 00:10:59,360
requests um and make sure that they have
 

561
00:10:59,360 --> 00:11:01,750
requests um and make sure that they have
the best possible outputs so it sounds

562
00:11:01,750 --> 00:11:01,760
the best possible outputs so it sounds
 

563
00:11:01,760 --> 00:11:03,509
the best possible outputs so it sounds
it sounds like a Brave New World very

564
00:11:03,509 --> 00:11:03,519
it sounds like a Brave New World very
 

565
00:11:03,519 --> 00:11:07,230
it sounds like a Brave New World very
excited for it in the future excellent

566
00:11:07,230 --> 00:11:07,240
excited for it in the future excellent
 

567
00:11:07,240 --> 00:11:10,030
excited for it in the future excellent
um next question uh is all about vertex

568
00:11:10,030 --> 00:11:10,040
um next question uh is all about vertex
 

569
00:11:10,040 --> 00:11:13,350
um next question uh is all about vertex
AI um so vertex AI is rapidly evolving

570
00:11:13,350 --> 00:11:13,360
AI um so vertex AI is rapidly evolving
 

571
00:11:13,360 --> 00:11:15,629
AI um so vertex AI is rapidly evolving
with new features and capabilities

572
00:11:15,629 --> 00:11:15,639
with new features and capabilities
 

573
00:11:15,639 --> 00:11:17,190
with new features and capabilities
looking ahead what are some of the key

574
00:11:17,190 --> 00:11:17,200
looking ahead what are some of the key
 

575
00:11:17,200 --> 00:11:19,470
looking ahead what are some of the key
emerging Trends in Enterprise AI that

576
00:11:19,470 --> 00:11:19,480
emerging Trends in Enterprise AI that
 

577
00:11:19,480 --> 00:11:21,230
emerging Trends in Enterprise AI that
you're most excited about and how is

578
00:11:21,230 --> 00:11:21,240
you're most excited about and how is
 

579
00:11:21,240 --> 00:11:23,509
you're most excited about and how is
vertex AI positioned to help businesses

580
00:11:23,509 --> 00:11:23,519
vertex AI positioned to help businesses
 

581
00:11:23,519 --> 00:11:26,470
vertex AI positioned to help businesses
leverage those for real world impact um

582
00:11:26,470 --> 00:11:26,480
leverage those for real world impact um
 

583
00:11:26,480 --> 00:11:28,629
leverage those for real world impact um
Warren and Arena uh do you want to do

584
00:11:28,629 --> 00:11:28,639
Warren and Arena uh do you want to do
 

585
00:11:28,639 --> 00:11:30,949
Warren and Arena uh do you want to do
you want to answer

586
00:11:30,949 --> 00:11:30,959
you want to answer
 

587
00:11:30,959 --> 00:11:33,470
you want to answer
yeah sure um I'll go first Serena feel

588
00:11:33,470 --> 00:11:33,480
yeah sure um I'll go first Serena feel
 

589
00:11:33,480 --> 00:11:36,430
yeah sure um I'll go first Serena feel
free to fill in um I think that when I

590
00:11:36,430 --> 00:11:36,440
free to fill in um I think that when I
 

591
00:11:36,440 --> 00:11:38,350
free to fill in um I think that when I
look at like where we were and where

592
00:11:38,350 --> 00:11:38,360
look at like where we were and where
 

593
00:11:38,360 --> 00:11:41,150
look at like where we were and where
we're going now um talking to customers

594
00:11:41,150 --> 00:11:41,160
we're going now um talking to customers
 

595
00:11:41,160 --> 00:11:43,069
we're going now um talking to customers
we're in this kind of large place of

596
00:11:43,069 --> 00:11:43,079
we're in this kind of large place of
 

597
00:11:43,079 --> 00:11:44,790
we're in this kind of large place of
business you know process automation

598
00:11:44,790 --> 00:11:44,800
business you know process automation
 

599
00:11:44,800 --> 00:11:46,389
business you know process automation
where it was how do I understand

600
00:11:46,389 --> 00:11:46,399
where it was how do I understand
 

601
00:11:46,399 --> 00:11:48,150
where it was how do I understand
unstructured data how do I pull it out

602
00:11:48,150 --> 00:11:48,160
unstructured data how do I pull it out
 

603
00:11:48,160 --> 00:11:49,910
unstructured data how do I pull it out
and do things with it you know what

604
00:11:49,910 --> 00:11:49,920
and do things with it you know what
 

605
00:11:49,920 --> 00:11:51,829
and do things with it you know what
we're seeing now as playing with the

606
00:11:51,829 --> 00:11:51,839
we're seeing now as playing with the
 

607
00:11:51,839 --> 00:11:53,990
we're seeing now as playing with the
gentic Frameworks and some latest models

608
00:11:53,990 --> 00:11:54,000
gentic Frameworks and some latest models
 

609
00:11:54,000 --> 00:11:56,389
gentic Frameworks and some latest models
last week is moving to this place of

610
00:11:56,389 --> 00:11:56,399
last week is moving to this place of
 

611
00:11:56,399 --> 00:11:59,550
last week is moving to this place of
being able to uh understand and reason

612
00:11:59,550 --> 00:11:59,560
being able to uh understand and reason
 

613
00:11:59,560 --> 00:12:02,190
being able to uh understand and reason
to kind of do deep research type things

614
00:12:02,190 --> 00:12:02,200
to kind of do deep research type things
 

615
00:12:02,200 --> 00:12:05,069
to kind of do deep research type things
so Mar Way Beyond I just want to take

616
00:12:05,069 --> 00:12:05,079
so Mar Way Beyond I just want to take
 

617
00:12:05,079 --> 00:12:06,670
so Mar Way Beyond I just want to take
this unstructured data and understand

618
00:12:06,670 --> 00:12:06,680
this unstructured data and understand
 

619
00:12:06,680 --> 00:12:09,069
this unstructured data and understand
what it is to hey can I do comparative

620
00:12:09,069 --> 00:12:09,079
what it is to hey can I do comparative
 

621
00:12:09,079 --> 00:12:11,069
what it is to hey can I do comparative
analysis of things and so when I was

622
00:12:11,069 --> 00:12:11,079
analysis of things and so when I was
 

623
00:12:11,079 --> 00:12:12,829
analysis of things and so when I was
playing with the agent the model

624
00:12:12,829 --> 00:12:12,839
playing with the agent the model
 

625
00:12:12,839 --> 00:12:14,750
playing with the agent the model
actually was smart enough to know that

626
00:12:14,750 --> 00:12:14,760
actually was smart enough to know that
 

627
00:12:14,760 --> 00:12:16,710
actually was smart enough to know that
it didn't have the latest information

628
00:12:16,710 --> 00:12:16,720
it didn't have the latest information
 

629
00:12:16,720 --> 00:12:18,949
it didn't have the latest information
and invoked a tool to go out actually on

630
00:12:18,949 --> 00:12:18,959
and invoked a tool to go out actually on
 

631
00:12:18,959 --> 00:12:21,110
and invoked a tool to go out actually on
the web and find the latest information

632
00:12:21,110 --> 00:12:21,120
the web and find the latest information
 

633
00:12:21,120 --> 00:12:23,030
the web and find the latest information
and bring it into the analysis I was

634
00:12:23,030 --> 00:12:23,040
and bring it into the analysis I was
 

635
00:12:23,040 --> 00:12:26,870
and bring it into the analysis I was
doing and so it becomes much uh easier

636
00:12:26,870 --> 00:12:26,880
doing and so it becomes much uh easier
 

637
00:12:26,880 --> 00:12:28,670
doing and so it becomes much uh easier
for you to do kind of some of this deep

638
00:12:28,670 --> 00:12:28,680
for you to do kind of some of this deep
 

639
00:12:28,680 --> 00:12:31,189
for you to do kind of some of this deep
uh analys is and understanding of the

640
00:12:31,189 --> 00:12:31,199
uh analys is and understanding of the
 

641
00:12:31,199 --> 00:12:33,230
uh analys is and understanding of the
data in the world and so I think that

642
00:12:33,230 --> 00:12:33,240
data in the world and so I think that
 

643
00:12:33,240 --> 00:12:35,269
data in the world and so I think that
that's I see a ton of that in

644
00:12:35,269 --> 00:12:35,279
that's I see a ton of that in
 

645
00:12:35,279 --> 00:12:37,189
that's I see a ton of that in
Enterprises going on where they're

646
00:12:37,189 --> 00:12:37,199
Enterprises going on where they're
 

647
00:12:37,199 --> 00:12:39,550
Enterprises going on where they're
really getting Beyond just being able to

648
00:12:39,550 --> 00:12:39,560
really getting Beyond just being able to
 

649
00:12:39,560 --> 00:12:41,269
really getting Beyond just being able to
search and understand that they've got a

650
00:12:41,269 --> 00:12:41,279
search and understand that they've got a
 

651
00:12:41,279 --> 00:12:43,069
search and understand that they've got a
bunch of unstructured data and what to

652
00:12:43,069 --> 00:12:43,079
bunch of unstructured data and what to
 

653
00:12:43,079 --> 00:12:47,110
bunch of unstructured data and what to
do with it Arena you want to fill

654
00:12:47,110 --> 00:12:47,120
do with it Arena you want to fill
 

655
00:12:47,120 --> 00:12:50,389
do with it Arena you want to fill
in I think that sounds good

656
00:12:50,389 --> 00:12:50,399
in I think that sounds good
 

657
00:12:50,399 --> 00:12:54,110
in I think that sounds good
great excellent and I I know that there

658
00:12:54,110 --> 00:12:54,120
great excellent and I I know that there
 

659
00:12:54,120 --> 00:12:56,750
great excellent and I I know that there
uh the the question around evals and how

660
00:12:56,750 --> 00:12:56,760
uh the the question around evals and how
 

661
00:12:56,760 --> 00:12:59,310
uh the the question around evals and how
to adequately evaluate these these

662
00:12:59,310 --> 00:12:59,320
to adequately evaluate these these
 

663
00:12:59,320 --> 00:13:01,750
to adequately evaluate these these
agentic workflows is is something that

664
00:13:01,750 --> 00:13:01,760
agentic workflows is is something that
 

665
00:13:01,760 --> 00:13:03,629
agentic workflows is is something that
uh that many companies are are thinking

666
00:13:03,629 --> 00:13:03,639
uh that many companies are are thinking
 

667
00:13:03,639 --> 00:13:06,949
uh that many companies are are thinking
about and and starting to Pioneer um so

668
00:13:06,949 --> 00:13:06,959
about and and starting to Pioneer um so
 

669
00:13:06,959 --> 00:13:09,910
about and and starting to Pioneer um so
I excited to hear more about uh Arena's

670
00:13:09,910 --> 00:13:09,920
I excited to hear more about uh Arena's
 

671
00:13:09,920 --> 00:13:14,949
I excited to hear more about uh Arena's
uh uh work in evals later on as well um

672
00:13:14,949 --> 00:13:14,959
uh uh work in evals later on as well um
 

673
00:13:14,959 --> 00:13:17,790
uh uh work in evals later on as well um
cool so next question is for everyone on

674
00:13:17,790 --> 00:13:17,800
cool so next question is for everyone on
 

675
00:13:17,800 --> 00:13:20,790
cool so next question is for everyone on
the call um uh looking ahead one to

676
00:13:20,790 --> 00:13:20,800
the call um uh looking ahead one to
 

677
00:13:20,800 --> 00:13:23,350
the call um uh looking ahead one to
three years what specific task or

678
00:13:23,350 --> 00:13:23,360
three years what specific task or
 

679
00:13:23,360 --> 00:13:25,230
three years what specific task or
capability do you believe foundational

680
00:13:25,230 --> 00:13:25,240
capability do you believe foundational
 

681
00:13:25,240 --> 00:13:26,870
capability do you believe foundational
models will unlock that seems

682
00:13:26,870 --> 00:13:26,880
models will unlock that seems
 

683
00:13:26,880 --> 00:13:29,550
models will unlock that seems
challenging or impossible today um

684
00:13:29,550 --> 00:13:29,560
challenging or impossible today um
 

685
00:13:29,560 --> 00:13:32,110
challenging or impossible today um
conversely what inherent limitations do

686
00:13:32,110 --> 00:13:32,120
conversely what inherent limitations do
 

687
00:13:32,120 --> 00:13:33,910
conversely what inherent limitations do
you think will persist despite all of

688
00:13:33,910 --> 00:13:33,920
you think will persist despite all of
 

689
00:13:33,920 --> 00:13:36,389
you think will persist despite all of
the progress that we've made um over the

690
00:13:36,389 --> 00:13:36,399
the progress that we've made um over the
 

691
00:13:36,399 --> 00:13:37,949
the progress that we've made um over the
last couple of years and especially in

692
00:13:37,949 --> 00:13:37,959
last couple of years and especially in
 

693
00:13:37,959 --> 00:13:41,069
last couple of years and especially in
the last several months so so just uh

694
00:13:41,069 --> 00:13:41,079
the last several months so so just uh
 

695
00:13:41,079 --> 00:13:42,470
the last several months so so just uh
given that we have a lot of folks on the

696
00:13:42,470 --> 00:13:42,480
given that we have a lot of folks on the
 

697
00:13:42,480 --> 00:13:45,590
given that we have a lot of folks on the
call let's keep it brief so around uh

698
00:13:45,590 --> 00:13:45,600
call let's keep it brief so around uh
 

699
00:13:45,600 --> 00:13:48,470
call let's keep it brief so around uh
just two to three sentences um and to

700
00:13:48,470 --> 00:13:48,480
just two to three sentences um and to
 

701
00:13:48,480 --> 00:13:52,189
just two to three sentences um and to
start U Matt do you want to go first

702
00:13:52,189 --> 00:13:52,199
start U Matt do you want to go first
 

703
00:13:52,199 --> 00:13:55,310
start U Matt do you want to go first
sure three years in AI is 100 years in

704
00:13:55,310 --> 00:13:55,320
sure three years in AI is 100 years in
 

705
00:13:55,320 --> 00:13:58,749
sure three years in AI is 100 years in
in human life so like that's question uh

706
00:13:58,749 --> 00:13:58,759
in human life so like that's question uh
 

707
00:13:58,759 --> 00:14:01,110
in human life so like that's question uh
I think think the way we build software

708
00:14:01,110 --> 00:14:01,120
I think think the way we build software
 

709
00:14:01,120 --> 00:14:03,629
I think think the way we build software
and the way we use software will

710
00:14:03,629 --> 00:14:03,639
and the way we use software will
 

711
00:14:03,639 --> 00:14:05,590
and the way we use software will
radically change like change in ways

712
00:14:05,590 --> 00:14:05,600
radically change like change in ways
 

713
00:14:05,600 --> 00:14:07,550
radically change like change in ways
that people are still not realizing I

714
00:14:07,550 --> 00:14:07,560
that people are still not realizing I
 

715
00:14:07,560 --> 00:14:10,430
that people are still not realizing I
would summarize as that

716
00:14:10,430 --> 00:14:10,440
would summarize as that
 

717
00:14:10,440 --> 00:14:13,189
would summarize as that
yeah and Logan what about you what do

718
00:14:13,189 --> 00:14:13,199
yeah and Logan what about you what do
 

719
00:14:13,199 --> 00:14:14,949
yeah and Logan what about you what do
you think will be different in the next

720
00:14:14,949 --> 00:14:14,959
you think will be different in the next
 

721
00:14:14,959 --> 00:14:16,509
you think will be different in the next
one to three

722
00:14:16,509 --> 00:14:16,519
one to three
 

723
00:14:16,519 --> 00:14:19,110
one to three
years yeah I think the whole code

724
00:14:19,110 --> 00:14:19,120
years yeah I think the whole code
 

725
00:14:19,120 --> 00:14:21,110
years yeah I think the whole code
generation software engineering stuff is

726
00:14:21,110 --> 00:14:21,120
generation software engineering stuff is
 

727
00:14:21,120 --> 00:14:22,509
generation software engineering stuff is
going to continue to take off but I

728
00:14:22,509 --> 00:14:22,519
going to continue to take off but I
 

729
00:14:22,519 --> 00:14:24,990
going to continue to take off but I
think the thing that is most interesting

730
00:14:24,990 --> 00:14:25,000
think the thing that is most interesting
 

731
00:14:25,000 --> 00:14:27,269
think the thing that is most interesting
to me is I think on the context of

732
00:14:27,269 --> 00:14:27,279
to me is I think on the context of
 

733
00:14:27,279 --> 00:14:29,910
to me is I think on the context of
what's not going to happen I do think

734
00:14:29,910 --> 00:14:29,920
what's not going to happen I do think
 

735
00:14:29,920 --> 00:14:32,870
what's not going to happen I do think
the models are still going to be bad if

736
00:14:32,870 --> 00:14:32,880
the models are still going to be bad if
 

737
00:14:32,880 --> 00:14:34,790
the models are still going to be bad if
you don't give them enough context like

738
00:14:34,790 --> 00:14:34,800
you don't give them enough context like
 

739
00:14:34,800 --> 00:14:36,310
you don't give them enough context like
I feel like this is like not going to be

740
00:14:36,310 --> 00:14:36,320
I feel like this is like not going to be
 

741
00:14:36,320 --> 00:14:38,590
I feel like this is like not going to be
a solved problem if you just like ask a

742
00:14:38,590 --> 00:14:38,600
a solved problem if you just like ask a
 

743
00:14:38,600 --> 00:14:40,069
a solved problem if you just like ask a
really basic question and expect the

744
00:14:40,069 --> 00:14:40,079
really basic question and expect the
 

745
00:14:40,079 --> 00:14:41,870
really basic question and expect the
model to be able to do some miraculous

746
00:14:41,870 --> 00:14:41,880
model to be able to do some miraculous
 

747
00:14:41,880 --> 00:14:43,990
model to be able to do some miraculous
thing for you um I think you're you're

748
00:14:43,990 --> 00:14:44,000
thing for you um I think you're you're
 

749
00:14:44,000 --> 00:14:45,350
thing for you um I think you're you're
still going to be disappointed in a few

750
00:14:45,350 --> 00:14:45,360
still going to be disappointed in a few
 

751
00:14:45,360 --> 00:14:46,509
still going to be disappointed in a few
years maybe the models will be smart

752
00:14:46,509 --> 00:14:46,519
years maybe the models will be smart
 

753
00:14:46,519 --> 00:14:48,509
years maybe the models will be smart
enough to know that they have to ask a

754
00:14:48,509 --> 00:14:48,519
enough to know that they have to ask a
 

755
00:14:48,519 --> 00:14:49,550
enough to know that they have to ask a
bunch of follow-up questions or

756
00:14:49,550 --> 00:14:49,560
bunch of follow-up questions or
 

757
00:14:49,560 --> 00:14:51,030
bunch of follow-up questions or
something like that in in the next one

758
00:14:51,030 --> 00:14:51,040
something like that in in the next one
 

759
00:14:51,040 --> 00:14:54,269
something like that in in the next one
to three years but uh that that piece is

760
00:14:54,269 --> 00:14:54,279
to three years but uh that that piece is
 

761
00:14:54,279 --> 00:14:56,069
to three years but uh that that piece is
is still going to

762
00:14:56,069 --> 00:14:56,079
is still going to
 

763
00:14:56,079 --> 00:14:59,030
is still going to
persist absolutely the it's just like

764
00:14:59,030 --> 00:14:59,040
persist absolutely the it's just like
 

765
00:14:59,040 --> 00:15:01,670
persist absolutely the it's just like
today if you ask a person to to solve a

766
00:15:01,670 --> 00:15:01,680
today if you ask a person to to solve a
 

767
00:15:01,680 --> 00:15:03,870
today if you ask a person to to solve a
task or to or to do something for you

768
00:15:03,870 --> 00:15:03,880
task or to or to do something for you
 

769
00:15:03,880 --> 00:15:06,030
task or to or to do something for you
you still need to give them enough uh

770
00:15:06,030 --> 00:15:06,040
you still need to give them enough uh
 

771
00:15:06,040 --> 00:15:07,749
you still need to give them enough uh
enough background information to be

772
00:15:07,749 --> 00:15:07,759
enough background information to be
 

773
00:15:07,759 --> 00:15:09,310
enough background information to be
successful and to point them in the

774
00:15:09,310 --> 00:15:09,320
successful and to point them in the
 

775
00:15:09,320 --> 00:15:10,389
successful and to point them in the
right

776
00:15:10,389 --> 00:15:10,399
right
 

777
00:15:10,399 --> 00:15:14,310
right
direction Arena do you want to go

778
00:15:14,310 --> 00:15:14,320
direction Arena do you want to go
 

779
00:15:14,320 --> 00:15:16,870
direction Arena do you want to go
next I think mine is kind of similar

780
00:15:16,870 --> 00:15:16,880
next I think mine is kind of similar
 

781
00:15:16,880 --> 00:15:19,110
next I think mine is kind of similar
from from the eval lens I think we will

782
00:15:19,110 --> 00:15:19,120
from from the eval lens I think we will
 

783
00:15:19,120 --> 00:15:21,230
from from the eval lens I think we will
see less issues with how to automate

784
00:15:21,230 --> 00:15:21,240
see less issues with how to automate
 

785
00:15:21,240 --> 00:15:23,949
see less issues with how to automate
eval flows but we will still be at the

786
00:15:23,949 --> 00:15:23,959
eval flows but we will still be at the
 

787
00:15:23,959 --> 00:15:27,269
eval flows but we will still be at the
point where if you don't give the model

788
00:15:27,269 --> 00:15:27,279
point where if you don't give the model
 

789
00:15:27,279 --> 00:15:29,269
point where if you don't give the model
your criteria and what you care of about

790
00:15:29,269 --> 00:15:29,279
your criteria and what you care of about
 

791
00:15:29,279 --> 00:15:31,150
your criteria and what you care of about
and what you define is good there is

792
00:15:31,150 --> 00:15:31,160
and what you define is good there is
 

793
00:15:31,160 --> 00:15:33,269
and what you define is good there is
just no way to to know that and that can

794
00:15:33,269 --> 00:15:33,279
just no way to to know that and that can
 

795
00:15:33,279 --> 00:15:35,350
just no way to to know that and that can
be provided in context or somewhere else

796
00:15:35,350 --> 00:15:35,360
be provided in context or somewhere else
 

797
00:15:35,360 --> 00:15:38,509
be provided in context or somewhere else
but you will still need to provide that

798
00:15:38,509 --> 00:15:38,519
but you will still need to provide that
 

799
00:15:38,519 --> 00:15:41,910
but you will still need to provide that
somehow absolutely and

800
00:15:41,910 --> 00:15:41,920
somehow absolutely and
 

801
00:15:41,920 --> 00:15:43,749
somehow absolutely and
Karen I might bring it back to the

802
00:15:43,749 --> 00:15:43,759
Karen I might bring it back to the
 

803
00:15:43,759 --> 00:15:45,629
Karen I might bring it back to the
previous question and say that in a few

804
00:15:45,629 --> 00:15:45,639
previous question and say that in a few
 

805
00:15:45,639 --> 00:15:47,069
previous question and say that in a few
years I hope that prompt engineering

806
00:15:47,069 --> 00:15:47,079
years I hope that prompt engineering
 

807
00:15:47,079 --> 00:15:48,949
years I hope that prompt engineering
will be a thing of the past and I and i'

808
00:15:48,949 --> 00:15:48,959
will be a thing of the past and I and i'
 

809
00:15:48,959 --> 00:15:51,550
will be a thing of the past and I and i'
expect it to be um uh But like everyone

810
00:15:51,550 --> 00:15:51,560
expect it to be um uh But like everyone
 

811
00:15:51,560 --> 00:15:53,389
expect it to be um uh But like everyone
else says the models will never be able

812
00:15:53,389 --> 00:15:53,399
else says the models will never be able
 

813
00:15:53,399 --> 00:15:56,829
else says the models will never be able
to read your mind and um uh people have

814
00:15:56,829 --> 00:15:56,839
to read your mind and um uh people have
 

815
00:15:56,839 --> 00:15:59,030
to read your mind and um uh people have
to have to get more into the mindset

816
00:15:59,030 --> 00:15:59,040
to have to get more into the mindset
 

817
00:15:59,040 --> 00:16:02,550
to have to get more into the mindset
being super clear and specific on their

818
00:16:02,550 --> 00:16:02,560
being super clear and specific on their
 

819
00:16:02,560 --> 00:16:04,550
being super clear and specific on their
intention yeah and

820
00:16:04,550 --> 00:16:04,560
intention yeah and
 

821
00:16:04,560 --> 00:16:07,110
intention yeah and
Warren I think even in this exists today

822
00:16:07,110 --> 00:16:07,120
Warren I think even in this exists today
 

823
00:16:07,120 --> 00:16:09,590
Warren I think even in this exists today
but I think it gets worse is over time

824
00:16:09,590 --> 00:16:09,600
but I think it gets worse is over time
 

825
00:16:09,600 --> 00:16:12,030
but I think it gets worse is over time
which is uh Enterprises especially are

826
00:16:12,030 --> 00:16:12,040
which is uh Enterprises especially are
 

827
00:16:12,040 --> 00:16:14,470
which is uh Enterprises especially are
not built around the ability to move

828
00:16:14,470 --> 00:16:14,480
not built around the ability to move
 

829
00:16:14,480 --> 00:16:16,550
not built around the ability to move
super quickly and so anything that

830
00:16:16,550 --> 00:16:16,560
super quickly and so anything that
 

831
00:16:16,560 --> 00:16:18,230
super quickly and so anything that
happened six months ago that was two

832
00:16:18,230 --> 00:16:18,240
happened six months ago that was two
 

833
00:16:18,240 --> 00:16:20,350
happened six months ago that was two
generations ago and so the ability for

834
00:16:20,350 --> 00:16:20,360
generations ago and so the ability for
 

835
00:16:20,360 --> 00:16:22,550
generations ago and so the ability for
you to keep up and make sure that you

836
00:16:22,550 --> 00:16:22,560
you to keep up and make sure that you
 

837
00:16:22,560 --> 00:16:24,430
you to keep up and make sure that you
design your you know the way that you

838
00:16:24,430 --> 00:16:24,440
design your you know the way that you
 

839
00:16:24,440 --> 00:16:26,910
design your you know the way that you
roll out software the way that you put

840
00:16:26,910 --> 00:16:26,920
roll out software the way that you put
 

841
00:16:26,920 --> 00:16:28,629
roll out software the way that you put
it in production that you can actually

842
00:16:28,629 --> 00:16:28,639
it in production that you can actually
 

843
00:16:28,639 --> 00:16:30,430
it in production that you can actually
do that in an agile way that allows you

844
00:16:30,430 --> 00:16:30,440
do that in an agile way that allows you
 

845
00:16:30,440 --> 00:16:32,670
do that in an agile way that allows you
to take the latest and greatest I think

846
00:16:32,670 --> 00:16:32,680
to take the latest and greatest I think
 

847
00:16:32,680 --> 00:16:35,870
to take the latest and greatest I think
that that's uh one of the big uh issues

848
00:16:35,870 --> 00:16:35,880
that that's uh one of the big uh issues
 

849
00:16:35,880 --> 00:16:37,910
that that's uh one of the big uh issues
that folks are are facing now and will

850
00:16:37,910 --> 00:16:37,920
that folks are are facing now and will
 

851
00:16:37,920 --> 00:16:40,069
that folks are are facing now and will
continue to face as we move forward

852
00:16:40,069 --> 00:16:40,079
continue to face as we move forward
 

853
00:16:40,079 --> 00:16:43,509
continue to face as we move forward
excellent and H and thank you so much to

854
00:16:43,509 --> 00:16:43,519
excellent and H and thank you so much to
 

855
00:16:43,519 --> 00:16:45,990
excellent and H and thank you so much to
to everyone who's dialed in this week to

856
00:16:45,990 --> 00:16:46,000
to everyone who's dialed in this week to
 

857
00:16:46,000 --> 00:16:48,550
to everyone who's dialed in this week to
to learn more about what's being built

858
00:16:48,550 --> 00:16:48,560
to learn more about what's being built
 

859
00:16:48,560 --> 00:16:50,670
to learn more about what's being built
and to learn how to incorporate it into

860
00:16:50,670 --> 00:16:50,680
and to learn how to incorporate it into
 

861
00:16:50,680 --> 00:16:52,470
and to learn how to incorporate it into
your businesses into your work or into

862
00:16:52,470 --> 00:16:52,480
your businesses into your work or into
 

863
00:16:52,480 --> 00:16:54,269
your businesses into your work or into
your personal projects that is one of

864
00:16:54,269 --> 00:16:54,279
your personal projects that is one of
 

865
00:16:54,279 --> 00:16:56,590
your personal projects that is one of
the first steps that you can do um to

866
00:16:56,590 --> 00:16:56,600
the first steps that you can do um to
 

867
00:16:56,600 --> 00:16:58,269
the first steps that you can do um to
make sure that you're staying up to date

868
00:16:58,269 --> 00:16:58,279
make sure that you're staying up to date
 

869
00:16:58,279 --> 00:16:59,990
make sure that you're staying up to date
and that as all of these great new

870
00:16:59,990 --> 00:17:00,000
and that as all of these great new
 

871
00:17:00,000 --> 00:17:01,990
and that as all of these great new
features get released you are on top of

872
00:17:01,990 --> 00:17:02,000
features get released you are on top of
 

873
00:17:02,000 --> 00:17:04,909
features get released you are on top of
all of them so so um good work to

874
00:17:04,909 --> 00:17:04,919
all of them so so um good work to
 

875
00:17:04,919 --> 00:17:06,949
all of them so so um good work to
everybody dialed in today and learning

876
00:17:06,949 --> 00:17:06,959
everybody dialed in today and learning
 

877
00:17:06,959 --> 00:17:09,270
everybody dialed in today and learning
more through the kaggle generative AI

878
00:17:09,270 --> 00:17:09,280
more through the kaggle generative AI
 

879
00:17:09,280 --> 00:17:12,390
more through the kaggle generative AI
intensive course um the next question

880
00:17:12,390 --> 00:17:12,400
intensive course um the next question
 

881
00:17:12,400 --> 00:17:14,309
intensive course um the next question
that we have is our first Community

882
00:17:14,309 --> 00:17:14,319
that we have is our first Community
 

883
00:17:14,319 --> 00:17:17,669
that we have is our first Community
question um from donna. oie uh thank you

884
00:17:17,669 --> 00:17:17,679
question um from donna. oie uh thank you
 

885
00:17:17,679 --> 00:17:20,590
question um from donna. oie uh thank you
for adding this on Discord how can AI

886
00:17:20,590 --> 00:17:20,600
for adding this on Discord how can AI
 

887
00:17:20,600 --> 00:17:22,949
for adding this on Discord how can AI
system design and prompt engineering be

888
00:17:22,949 --> 00:17:22,959
system design and prompt engineering be
 

889
00:17:22,959 --> 00:17:25,590
system design and prompt engineering be
optimized to improve Energy Efficiency

890
00:17:25,590 --> 00:17:25,600
optimized to improve Energy Efficiency
 

891
00:17:25,600 --> 00:17:27,710
optimized to improve Energy Efficiency
computational performance so things like

892
00:17:27,710 --> 00:17:27,720
computational performance so things like
 

893
00:17:27,720 --> 00:17:29,950
computational performance so things like
speed and accuracy and reduce

894
00:17:29,950 --> 00:17:29,960
speed and accuracy and reduce
 

895
00:17:29,960 --> 00:17:32,070
speed and accuracy and reduce
environmental impact while maintaining

896
00:17:32,070 --> 00:17:32,080
environmental impact while maintaining
 

897
00:17:32,080 --> 00:17:35,110
environmental impact while maintaining
output model quality um so Logan would

898
00:17:35,110 --> 00:17:35,120
output model quality um so Logan would
 

899
00:17:35,120 --> 00:17:37,470
output model quality um so Logan would
you like to to take a stab at this

900
00:17:37,470 --> 00:17:37,480
you like to to take a stab at this
 

901
00:17:37,480 --> 00:17:39,470
you like to to take a stab at this
one yeah this is a this is an

902
00:17:39,470 --> 00:17:39,480
one yeah this is a this is an
 

903
00:17:39,480 --> 00:17:40,830
one yeah this is a this is an
interesting question I think there's a

904
00:17:40,830 --> 00:17:40,840
interesting question I think there's a
 

905
00:17:40,840 --> 00:17:42,230
interesting question I think there's a
bunch of different dimensions I think

906
00:17:42,230 --> 00:17:42,240
bunch of different dimensions I think
 

907
00:17:42,240 --> 00:17:44,070
bunch of different dimensions I think
one of them is

908
00:17:44,070 --> 00:17:44,080
one of them is
 

909
00:17:44,080 --> 00:17:46,710
one of them is
um at some layer like different API

910
00:17:46,710 --> 00:17:46,720
um at some layer like different API
 

911
00:17:46,720 --> 00:17:48,350
um at some layer like different API
Services have ways to help with this

912
00:17:48,350 --> 00:17:48,360
Services have ways to help with this
 

913
00:17:48,360 --> 00:17:50,110
Services have ways to help with this
like you could do prompt caching as an

914
00:17:50,110 --> 00:17:50,120
like you could do prompt caching as an
 

915
00:17:50,120 --> 00:17:53,150
like you could do prompt caching as an
example um I think there's also like you

916
00:17:53,150 --> 00:17:53,160
example um I think there's also like you
 

917
00:17:53,160 --> 00:17:55,590
example um I think there's also like you
know another angle of this is like batch

918
00:17:55,590 --> 00:17:55,600
know another angle of this is like batch
 

919
00:17:55,600 --> 00:17:57,669
know another angle of this is like batch
apis like you can take sort of you know

920
00:17:57,669 --> 00:17:57,679
apis like you can take sort of you know
 

921
00:17:57,679 --> 00:18:00,710
apis like you can take sort of you know
the fixed cost of the fixed requirements

922
00:18:00,710 --> 00:18:00,720
the fixed cost of the fixed requirements
 

923
00:18:00,720 --> 00:18:02,830
the fixed cost of the fixed requirements
of of having Services just like running

924
00:18:02,830 --> 00:18:02,840
of of having Services just like running
 

925
00:18:02,840 --> 00:18:04,789
of of having Services just like running
all the time and find times when there's

926
00:18:04,789 --> 00:18:04,799
all the time and find times when there's
 

927
00:18:04,799 --> 00:18:06,549
all the time and find times when there's
like idle compute just sitting around

928
00:18:06,549 --> 00:18:06,559
like idle compute just sitting around
 

929
00:18:06,559 --> 00:18:08,669
like idle compute just sitting around
there um and be able to sort of flatten

930
00:18:08,669 --> 00:18:08,679
there um and be able to sort of flatten
 

931
00:18:08,679 --> 00:18:11,549
there um and be able to sort of flatten
out and make sure that um you know the

932
00:18:11,549 --> 00:18:11,559
out and make sure that um you know the
 

933
00:18:11,559 --> 00:18:13,029
out and make sure that um you know the
compu is being used from the Energy

934
00:18:13,029 --> 00:18:13,039
compu is being used from the Energy
 

935
00:18:13,039 --> 00:18:14,390
compu is being used from the Energy
Efficiency standpoint you could imagine

936
00:18:14,390 --> 00:18:14,400
Efficiency standpoint you could imagine
 

937
00:18:14,400 --> 00:18:17,029
Efficiency standpoint you could imagine
a world where like you say in the future

938
00:18:17,029 --> 00:18:17,039
a world where like you say in the future
 

939
00:18:17,039 --> 00:18:18,750
a world where like you say in the future
this doesn't exist yet but you could say

940
00:18:18,750 --> 00:18:18,760
this doesn't exist yet but you could say
 

941
00:18:18,760 --> 00:18:21,110
this doesn't exist yet but you could say
I'm a batch API customer and I care a

942
00:18:21,110 --> 00:18:21,120
I'm a batch API customer and I care a
 

943
00:18:21,120 --> 00:18:22,990
I'm a batch API customer and I care a
lot about the environmental impact so

944
00:18:22,990 --> 00:18:23,000
lot about the environmental impact so
 

945
00:18:23,000 --> 00:18:25,350
lot about the environmental impact so
actually try to run my batches times

946
00:18:25,350 --> 00:18:25,360
actually try to run my batches times
 

947
00:18:25,360 --> 00:18:26,789
actually try to run my batches times
when you know data centers are more

948
00:18:26,789 --> 00:18:26,799
when you know data centers are more
 

949
00:18:26,799 --> 00:18:28,470
when you know data centers are more
likely to be using renewable energy or

950
00:18:28,470 --> 00:18:28,480
likely to be using renewable energy or
 

951
00:18:28,480 --> 00:18:30,149
likely to be using renewable energy or
something like that so I think there's a

952
00:18:30,149 --> 00:18:30,159
something like that so I think there's a
 

953
00:18:30,159 --> 00:18:33,590
something like that so I think there's a
lot of degrees of freedom um and and

954
00:18:33,590 --> 00:18:33,600
lot of degrees of freedom um and and
 

955
00:18:33,600 --> 00:18:35,149
lot of degrees of freedom um and and
dimensions in which in which you could

956
00:18:35,149 --> 00:18:35,159
dimensions in which in which you could
 

957
00:18:35,159 --> 00:18:37,990
dimensions in which in which you could
do this absolutely and I know that you

958
00:18:37,990 --> 00:18:38,000
do this absolutely and I know that you
 

959
00:18:38,000 --> 00:18:40,110
do this absolutely and I know that you
know Google's been investing a lot in in

960
00:18:40,110 --> 00:18:40,120
know Google's been investing a lot in in
 

961
00:18:40,120 --> 00:18:42,350
know Google's been investing a lot in in
smaller models especially smaller open

962
00:18:42,350 --> 00:18:42,360
smaller models especially smaller open
 

963
00:18:42,360 --> 00:18:44,909
smaller models especially smaller open
models and that can help reduce the the

964
00:18:44,909 --> 00:18:44,919
models and that can help reduce the the
 

965
00:18:44,919 --> 00:18:47,510
models and that can help reduce the the
environmental impact footprint as well

966
00:18:47,510 --> 00:18:47,520
environmental impact footprint as well
 

967
00:18:47,520 --> 00:18:49,430
environmental impact footprint as well
um if you're using something that's on

968
00:18:49,430 --> 00:18:49,440
um if you're using something that's on
 

969
00:18:49,440 --> 00:18:52,230
um if you're using something that's on
device as opposed to to pinging a server

970
00:18:52,230 --> 00:18:52,240
device as opposed to to pinging a server
 

971
00:18:52,240 --> 00:18:54,830
device as opposed to to pinging a server
with a larger model so it's it's been

972
00:18:54,830 --> 00:18:54,840
with a larger model so it's it's been
 

973
00:18:54,840 --> 00:18:56,830
with a larger model so it's it's been
it's been really inspiring to see the

974
00:18:56,830 --> 00:18:56,840
it's been really inspiring to see the
 

975
00:18:56,840 --> 00:18:59,110
it's been really inspiring to see the
creative ways that the community is kind

976
00:18:59,110 --> 00:18:59,120
creative ways that the community is kind
 

977
00:18:59,120 --> 00:19:01,149
creative ways that the community is kind
of rallying around and and making these

978
00:19:01,149 --> 00:19:01,159
of rallying around and and making these
 

979
00:19:01,159 --> 00:19:04,149
of rallying around and and making these
workloads more efficient in real

980
00:19:04,149 --> 00:19:04,159
workloads more efficient in real
 

981
00:19:04,159 --> 00:19:07,470
workloads more efficient in real
time next question um this is from

982
00:19:07,470 --> 00:19:07,480
time next question um this is from
 

983
00:19:07,480 --> 00:19:10,789
time next question um this is from
Channing Ogden um again from Discord how

984
00:19:10,789 --> 00:19:10,799
Channing Ogden um again from Discord how
 

985
00:19:10,799 --> 00:19:12,789
Channing Ogden um again from Discord how
can judge models be selected and

986
00:19:12,789 --> 00:19:12,799
can judge models be selected and
 

987
00:19:12,799 --> 00:19:14,750
can judge models be selected and
customized efficiently to minimize

988
00:19:14,750 --> 00:19:14,760
customized efficiently to minimize
 

989
00:19:14,760 --> 00:19:17,350
customized efficiently to minimize
compounded bias and accuracy issues

990
00:19:17,350 --> 00:19:17,360
compounded bias and accuracy issues
 

991
00:19:17,360 --> 00:19:19,149
compounded bias and accuracy issues
building confidence without relying

992
00:19:19,149 --> 00:19:19,159
building confidence without relying
 

993
00:19:19,159 --> 00:19:22,510
building confidence without relying
solely on repeated evals um Arena do you

994
00:19:22,510 --> 00:19:22,520
solely on repeated evals um Arena do you
 

995
00:19:22,520 --> 00:19:25,310
solely on repeated evals um Arena do you
want to take this one yes that's a great

996
00:19:25,310 --> 00:19:25,320
want to take this one yes that's a great
 

997
00:19:25,320 --> 00:19:26,789
want to take this one yes that's a great
question and something I'm thinking

998
00:19:26,789 --> 00:19:26,799
question and something I'm thinking
 

999
00:19:26,799 --> 00:19:29,070
question and something I'm thinking
about a lot how do you actually build

1000
00:19:29,070 --> 00:19:29,080
about a lot how do you actually build
 

1001
00:19:29,080 --> 00:19:31,149
about a lot how do you actually build
trust in all the judge models and auto

1002
00:19:31,149 --> 00:19:31,159
trust in all the judge models and auto
 

1003
00:19:31,159 --> 00:19:33,549
trust in all the judge models and auto
writers that you use so I I kind of

1004
00:19:33,549 --> 00:19:33,559
writers that you use so I I kind of
 

1005
00:19:33,559 --> 00:19:35,870
writers that you use so I I kind of
think about three areas here first you

1006
00:19:35,870 --> 00:19:35,880
think about three areas here first you
 

1007
00:19:35,880 --> 00:19:37,669
think about three areas here first you
just need to have a good foundation

1008
00:19:37,669 --> 00:19:37,679
just need to have a good foundation
 

1009
00:19:37,679 --> 00:19:39,430
just need to have a good foundation
right so do I use a state-of-the-art

1010
00:19:39,430 --> 00:19:39,440
right so do I use a state-of-the-art
 

1011
00:19:39,440 --> 00:19:42,070
right so do I use a state-of-the-art
model as a judge and do I have some core

1012
00:19:42,070 --> 00:19:42,080
model as a judge and do I have some core
 

1013
00:19:42,080 --> 00:19:44,750
model as a judge and do I have some core
techniques in place to mitigate bias so

1014
00:19:44,750 --> 00:19:44,760
techniques in place to mitigate bias so
 

1015
00:19:44,760 --> 00:19:46,190
techniques in place to mitigate bias so
just a couple examples you already

1016
00:19:46,190 --> 00:19:46,200
just a couple examples you already
 

1017
00:19:46,200 --> 00:19:47,950
just a couple examples you already
mentioned multi sampling right so not

1018
00:19:47,950 --> 00:19:47,960
mentioned multi sampling right so not
 

1019
00:19:47,960 --> 00:19:49,870
mentioned multi sampling right so not
relying on the first answer and there's

1020
00:19:49,870 --> 00:19:49,880
relying on the first answer and there's
 

1021
00:19:49,880 --> 00:19:51,990
relying on the first answer and there's
other things like flipping because you

1022
00:19:51,990 --> 00:19:52,000
other things like flipping because you
 

1023
00:19:52,000 --> 00:19:54,310
other things like flipping because you
need to flip orders as these models show

1024
00:19:54,310 --> 00:19:54,320
need to flip orders as these models show
 

1025
00:19:54,320 --> 00:19:56,190
need to flip orders as these models show
a position bias they prefer the first or

1026
00:19:56,190 --> 00:19:56,200
a position bias they prefer the first or
 

1027
00:19:56,200 --> 00:19:58,870
a position bias they prefer the first or
the second in a p w comparison so this

1028
00:19:58,870 --> 00:19:58,880
the second in a p w comparison so this
 

1029
00:19:58,880 --> 00:20:02,149
the second in a p w comparison so this
would be kind of the first step and then

1030
00:20:02,149 --> 00:20:02,159
would be kind of the first step and then
 

1031
00:20:02,159 --> 00:20:04,430
would be kind of the first step and then
I would say evaluate your judge it's yet

1032
00:20:04,430 --> 00:20:04,440
I would say evaluate your judge it's yet
 

1033
00:20:04,440 --> 00:20:06,270
I would say evaluate your judge it's yet
another llm application so you want to

1034
00:20:06,270 --> 00:20:06,280
another llm application so you want to
 

1035
00:20:06,280 --> 00:20:07,669
another llm application so you want to
test it and this doesn't have to be a

1036
00:20:07,669 --> 00:20:07,679
test it and this doesn't have to be a
 

1037
00:20:07,679 --> 00:20:10,710
test it and this doesn't have to be a
lot of data but some basic understanding

1038
00:20:10,710 --> 00:20:10,720
lot of data but some basic understanding
 

1039
00:20:10,720 --> 00:20:13,190
lot of data but some basic understanding
if the alignment if there is alignment

1040
00:20:13,190 --> 00:20:13,200
if the alignment if there is alignment
 

1041
00:20:13,200 --> 00:20:14,990
if the alignment if there is alignment
between your human experts or your own

1042
00:20:14,990 --> 00:20:15,000
between your human experts or your own
 

1043
00:20:15,000 --> 00:20:18,270
between your human experts or your own
judgment and what the judge says on

1044
00:20:18,270 --> 00:20:18,280
judgment and what the judge says on
 

1045
00:20:18,280 --> 00:20:20,549
judgment and what the judge says on
again your very specific data and if you

1046
00:20:20,549 --> 00:20:20,559
again your very specific data and if you
 

1047
00:20:20,559 --> 00:20:22,390
again your very specific data and if you
don't see that alignment and this is the

1048
00:20:22,390 --> 00:20:22,400
don't see that alignment and this is the
 

1049
00:20:22,400 --> 00:20:25,029
don't see that alignment and this is the
third thing start with prompt

1050
00:20:25,029 --> 00:20:25,039
third thing start with prompt
 

1051
00:20:25,039 --> 00:20:26,830
third thing start with prompt
engineering right we already spoke about

1052
00:20:26,830 --> 00:20:26,840
engineering right we already spoke about
 

1053
00:20:26,840 --> 00:20:29,669
engineering right we already spoke about
this a lot today but are the criteria

1054
00:20:29,669 --> 00:20:29,679
this a lot today but are the criteria
 

1055
00:20:29,679 --> 00:20:31,230
this a lot today but are the criteria
that you give the judge clear and

1056
00:20:31,230 --> 00:20:31,240
that you give the judge clear and
 

1057
00:20:31,240 --> 00:20:34,110
that you give the judge clear and
specific I often see hidden criteria so

1058
00:20:34,110 --> 00:20:34,120
specific I often see hidden criteria so
 

1059
00:20:34,120 --> 00:20:36,789
specific I often see hidden criteria so
users implicitly consider something but

1060
00:20:36,789 --> 00:20:36,799
users implicitly consider something but
 

1061
00:20:36,799 --> 00:20:38,669
users implicitly consider something but
they don't really explicitly state it in

1062
00:20:38,669 --> 00:20:38,679
they don't really explicitly state it in
 

1063
00:20:38,679 --> 00:20:40,990
they don't really explicitly state it in
the autter rator promt and as Kieran

1064
00:20:40,990 --> 00:20:41,000
the autter rator promt and as Kieran
 

1065
00:20:41,000 --> 00:20:43,310
the autter rator promt and as Kieran
said maybe in the future they can ask

1066
00:20:43,310 --> 00:20:43,320
said maybe in the future they can ask
 

1067
00:20:43,320 --> 00:20:45,230
said maybe in the future they can ask
clarifying questions but they don't do

1068
00:20:45,230 --> 00:20:45,240
clarifying questions but they don't do
 

1069
00:20:45,240 --> 00:20:47,950
clarifying questions but they don't do
that today so this is something you have

1070
00:20:47,950 --> 00:20:47,960
that today so this is something you have
 

1071
00:20:47,960 --> 00:20:49,710
that today so this is something you have
to think about and then after you've

1072
00:20:49,710 --> 00:20:49,720
to think about and then after you've
 

1073
00:20:49,720 --> 00:20:52,070
to think about and then after you've
done this there's and and you see that

1074
00:20:52,070 --> 00:20:52,080
done this there's and and you see that
 

1075
00:20:52,080 --> 00:20:53,909
done this there's and and you see that
refining The Prompt doesn't really close

1076
00:20:53,909 --> 00:20:53,919
refining The Prompt doesn't really close
 

1077
00:20:53,919 --> 00:20:55,549
refining The Prompt doesn't really close
the alignment gap between you and the

1078
00:20:55,549 --> 00:20:55,559
the alignment gap between you and the
 

1079
00:20:55,559 --> 00:20:57,470
the alignment gap between you and the
judge then you can explore more Advanced

1080
00:20:57,470 --> 00:20:57,480
judge then you can explore more Advanced
 

1081
00:20:57,480 --> 00:20:59,710
judge then you can explore more Advanced
Techniques so maybe maybe the data set

1082
00:20:59,710 --> 00:20:59,720
Techniques so maybe maybe the data set
 

1083
00:20:59,720 --> 00:21:01,909
Techniques so maybe maybe the data set
level criteria that you're working with

1084
00:21:01,909 --> 00:21:01,919
level criteria that you're working with
 

1085
00:21:01,919 --> 00:21:03,510
level criteria that you're working with
they just don't fit your data set it's

1086
00:21:03,510 --> 00:21:03,520
they just don't fit your data set it's
 

1087
00:21:03,520 --> 00:21:05,630
they just don't fit your data set it's
too diverse so maybe you need something

1088
00:21:05,630 --> 00:21:05,640
too diverse so maybe you need something
 

1089
00:21:05,640 --> 00:21:07,390
too diverse so maybe you need something
like per item rubrics instead of

1090
00:21:07,390 --> 00:21:07,400
like per item rubrics instead of
 

1091
00:21:07,400 --> 00:21:09,909
like per item rubrics instead of
universal criteria potentially you might

1092
00:21:09,909 --> 00:21:09,919
universal criteria potentially you might
 

1093
00:21:09,919 --> 00:21:11,549
universal criteria potentially you might
consider fine-tuning your judge because

1094
00:21:11,549 --> 00:21:11,559
consider fine-tuning your judge because
 

1095
00:21:11,559 --> 00:21:13,990
consider fine-tuning your judge because
your data set is so so specific but

1096
00:21:13,990 --> 00:21:14,000
your data set is so so specific but
 

1097
00:21:14,000 --> 00:21:15,430
your data set is so so specific but
again I would start with the prompt

1098
00:21:15,430 --> 00:21:15,440
again I would start with the prompt
 

1099
00:21:15,440 --> 00:21:17,110
again I would start with the prompt
engineering if you see some misalignment

1100
00:21:17,110 --> 00:21:17,120
engineering if you see some misalignment
 

1101
00:21:17,120 --> 00:21:19,029
engineering if you see some misalignment
between you and the judge model

1102
00:21:19,029 --> 00:21:19,039
between you and the judge model
 

1103
00:21:19,039 --> 00:21:20,630
between you and the judge model
excellent I feel like we all just got a

1104
00:21:20,630 --> 00:21:20,640
excellent I feel like we all just got a
 

1105
00:21:20,640 --> 00:21:23,909
excellent I feel like we all just got a
crash course in uh in evaluating and

1106
00:21:23,909 --> 00:21:23,919
crash course in uh in evaluating and
 

1107
00:21:23,919 --> 00:21:25,950
crash course in uh in evaluating and
sort of perfecting judge models this is

1108
00:21:25,950 --> 00:21:25,960
sort of perfecting judge models this is
 

1109
00:21:25,960 --> 00:21:28,230
sort of perfecting judge models this is
wonderful um and I hope we I hope we get

1110
00:21:28,230 --> 00:21:28,240
wonderful um and I hope we I hope we get
 

1111
00:21:28,240 --> 00:21:29,710
wonderful um and I hope we I hope we get
to cover for a little bit more of it

1112
00:21:29,710 --> 00:21:29,720
to cover for a little bit more of it
 

1113
00:21:29,720 --> 00:21:34,630
to cover for a little bit more of it
later on in the week cool um next

1114
00:21:34,630 --> 00:21:34,640
later on in the week cool um next
 

1115
00:21:34,640 --> 00:21:38,830
later on in the week cool um next
question um from sarar 42 uh is writing

1116
00:21:38,830 --> 00:21:38,840
question um from sarar 42 uh is writing
 

1117
00:21:38,840 --> 00:21:40,990
question um from sarar 42 uh is writing
the prompt appropriately prompt

1118
00:21:40,990 --> 00:21:41,000
the prompt appropriately prompt
 

1119
00:21:41,000 --> 00:21:42,950
the prompt appropriately prompt
engineering or setting up the number of

1120
00:21:42,950 --> 00:21:42,960
engineering or setting up the number of
 

1121
00:21:42,960 --> 00:21:45,110
engineering or setting up the number of
tokens temperature top p is prompt

1122
00:21:45,110 --> 00:21:45,120
tokens temperature top p is prompt
 

1123
00:21:45,120 --> 00:21:48,830
tokens temperature top p is prompt
engineering or both um and uh an not do

1124
00:21:48,830 --> 00:21:48,840
engineering or both um and uh an not do
 

1125
00:21:48,840 --> 00:21:50,190
engineering or both um and uh an not do
you want to do you want to take a stab

1126
00:21:50,190 --> 00:21:50,200
you want to do you want to take a stab
 

1127
00:21:50,200 --> 00:21:52,630
you want to do you want to take a stab
at answering this question um I also

1128
00:21:52,630 --> 00:21:52,640
at answering this question um I also
 

1129
00:21:52,640 --> 00:21:55,510
at answering this question um I also
love to hear everyone's uh everyone's

1130
00:21:55,510 --> 00:21:55,520
love to hear everyone's uh everyone's
 

1131
00:21:55,520 --> 00:21:57,310
love to hear everyone's uh everyone's
initial thoughts around prompt

1132
00:21:57,310 --> 00:21:57,320
initial thoughts around prompt
 

1133
00:21:57,320 --> 00:21:59,590
initial thoughts around prompt
engineering what is it why is it useful

1134
00:21:59,590 --> 00:21:59,600
engineering what is it why is it useful
 

1135
00:21:59,600 --> 00:22:02,149
engineering what is it why is it useful
and what exactly um what exactly does it

1136
00:22:02,149 --> 00:22:02,159
and what exactly um what exactly does it
 

1137
00:22:02,159 --> 00:22:05,070
and what exactly um what exactly does it
entail so maybe I just give a quick uh

1138
00:22:05,070 --> 00:22:05,080
entail so maybe I just give a quick uh
 

1139
00:22:05,080 --> 00:22:06,870
entail so maybe I just give a quick uh
overview and then uh the rest can take

1140
00:22:06,870 --> 00:22:06,880
overview and then uh the rest can take
 

1141
00:22:06,880 --> 00:22:09,470
overview and then uh the rest can take
over so I see prompt The Prompt part of

1142
00:22:09,470 --> 00:22:09,480
over so I see prompt The Prompt part of
 

1143
00:22:09,480 --> 00:22:11,230
over so I see prompt The Prompt part of
the prompt engineering as the input like

1144
00:22:11,230 --> 00:22:11,240
the prompt engineering as the input like
 

1145
00:22:11,240 --> 00:22:12,830
the prompt engineering as the input like
in traditional ml models where there

1146
00:22:12,830 --> 00:22:12,840
in traditional ml models where there
 

1147
00:22:12,840 --> 00:22:15,269
in traditional ml models where there
were features so prompt is the input to

1148
00:22:15,269 --> 00:22:15,279
were features so prompt is the input to
 

1149
00:22:15,279 --> 00:22:18,310
were features so prompt is the input to
the model while the the other parameters

1150
00:22:18,310 --> 00:22:18,320
the model while the the other parameters
 

1151
00:22:18,320 --> 00:22:20,149
the model while the the other parameters
the generation or decoding parameters

1152
00:22:20,149 --> 00:22:20,159
the generation or decoding parameters
 

1153
00:22:20,159 --> 00:22:22,390
the generation or decoding parameters
like top PE temperature these are

1154
00:22:22,390 --> 00:22:22,400
like top PE temperature these are
 

1155
00:22:22,400 --> 00:22:24,510
like top PE temperature these are
operating at the output level where we

1156
00:22:24,510 --> 00:22:24,520
operating at the output level where we
 

1157
00:22:24,520 --> 00:22:27,350
operating at the output level where we
kind of uh with the fixed input what

1158
00:22:27,350 --> 00:22:27,360
kind of uh with the fixed input what
 

1159
00:22:27,360 --> 00:22:29,350
kind of uh with the fixed input what
else we can modify the the output to

1160
00:22:29,350 --> 00:22:29,360
else we can modify the the output to
 

1161
00:22:29,360 --> 00:22:30,950
else we can modify the the output to
kind of ensure that the tokens that are

1162
00:22:30,950 --> 00:22:30,960
kind of ensure that the tokens that are
 

1163
00:22:30,960 --> 00:22:34,510
kind of ensure that the tokens that are
selected and sampled are um done so in a

1164
00:22:34,510 --> 00:22:34,520
selected and sampled are um done so in a
 

1165
00:22:34,520 --> 00:22:37,390
selected and sampled are um done so in a
way that optimizes the response for the

1166
00:22:37,390 --> 00:22:37,400
way that optimizes the response for the
 

1167
00:22:37,400 --> 00:22:40,390
way that optimizes the response for the
task anybody else uh want to take a step

1168
00:22:40,390 --> 00:22:40,400
task anybody else uh want to take a step
 

1169
00:22:40,400 --> 00:22:44,710
task anybody else uh want to take a step
at this maybe um uh Matt or

1170
00:22:44,710 --> 00:22:44,720
at this maybe um uh Matt or
 

1171
00:22:44,720 --> 00:22:46,269
at this maybe um uh Matt or
airon

1172
00:22:46,269 --> 00:22:46,279
airon
 

1173
00:22:46,279 --> 00:22:49,430
airon
sure I'll ask K to cover his ears as I

1174
00:22:49,430 --> 00:22:49,440
sure I'll ask K to cover his ears as I
 

1175
00:22:49,440 --> 00:22:51,510
sure I'll ask K to cover his ears as I
say this but like my opinion is propt

1176
00:22:51,510 --> 00:22:51,520
say this but like my opinion is propt
 

1177
00:22:51,520 --> 00:22:53,870
say this but like my opinion is propt
engineer is a little bit of an art right

1178
00:22:53,870 --> 00:22:53,880
engineer is a little bit of an art right
 

1179
00:22:53,880 --> 00:22:55,549
engineer is a little bit of an art right
it's a little bit of try and error it's

1180
00:22:55,549 --> 00:22:55,559
it's a little bit of try and error it's
 

1181
00:22:55,559 --> 00:22:57,990
it's a little bit of try and error it's
a little bit of let me test this with

1182
00:22:57,990 --> 00:22:58,000
a little bit of let me test this with
 

1183
00:22:58,000 --> 00:23:00,350
a little bit of let me test this with
multiple models let me test this with

1184
00:23:00,350 --> 00:23:00,360
multiple models let me test this with
 

1185
00:23:00,360 --> 00:23:02,830
multiple models let me test this with
multiple temperature settings does a

1186
00:23:02,830 --> 00:23:02,840
multiple temperature settings does a
 

1187
00:23:02,840 --> 00:23:05,149
multiple temperature settings does a
smaller model can do this job or do I

1188
00:23:05,149 --> 00:23:05,159
smaller model can do this job or do I
 

1189
00:23:05,159 --> 00:23:07,350
smaller model can do this job or do I
need a larger model like a lot of these

1190
00:23:07,350 --> 00:23:07,360
need a larger model like a lot of these
 

1191
00:23:07,360 --> 00:23:09,110
need a larger model like a lot of these
things I put in the bucket of prompt

1192
00:23:09,110 --> 00:23:09,120
things I put in the bucket of prompt
 

1193
00:23:09,120 --> 00:23:11,310
things I put in the bucket of prompt
engineering which is experimentation

1194
00:23:11,310 --> 00:23:11,320
engineering which is experimentation
 

1195
00:23:11,320 --> 00:23:14,269
engineering which is experimentation
over and over again until you find the

1196
00:23:14,269 --> 00:23:14,279
over and over again until you find the
 

1197
00:23:14,279 --> 00:23:17,269
over and over again until you find the
optimal uh you know way of working for

1198
00:23:17,269 --> 00:23:17,279
optimal uh you know way of working for
 

1199
00:23:17,279 --> 00:23:20,029
optimal uh you know way of working for
your

1200
00:23:20,029 --> 00:23:20,039

 

1201
00:23:20,039 --> 00:23:22,510

scenario and yeah I I'd agree I'd agree

1202
00:23:22,510 --> 00:23:22,520
scenario and yeah I I'd agree I'd agree
 

1203
00:23:22,520 --> 00:23:24,789
scenario and yeah I I'd agree I'd agree
with the art points at the minute um

1204
00:23:24,789 --> 00:23:24,799
with the art points at the minute um
 

1205
00:23:24,799 --> 00:23:26,750
with the art points at the minute um
like I see a future where there's far

1206
00:23:26,750 --> 00:23:26,760
like I see a future where there's far
 

1207
00:23:26,760 --> 00:23:28,950
like I see a future where there's far
less art involved and

1208
00:23:28,950 --> 00:23:28,960
less art involved and
 

1209
00:23:28,960 --> 00:23:30,789
less art involved and
you know if we think about this sort of

1210
00:23:30,789 --> 00:23:30,799
you know if we think about this sort of
 

1211
00:23:30,799 --> 00:23:32,110
you know if we think about this sort of
specific question you know whether you

1212
00:23:32,110 --> 00:23:32,120
specific question you know whether you
 

1213
00:23:32,120 --> 00:23:34,269
specific question you know whether you
call it prompt engineering or llm

1214
00:23:34,269 --> 00:23:34,279
call it prompt engineering or llm
 

1215
00:23:34,279 --> 00:23:35,830
call it prompt engineering or llm
engineering or whatever these are all

1216
00:23:35,830 --> 00:23:35,840
engineering or whatever these are all
 

1217
00:23:35,840 --> 00:23:37,669
engineering or whatever these are all
things as a developer that you have to

1218
00:23:37,669 --> 00:23:37,679
things as a developer that you have to
 

1219
00:23:37,679 --> 00:23:40,470
things as a developer that you have to
think about um and there are various

1220
00:23:40,470 --> 00:23:40,480
think about um and there are various
 

1221
00:23:40,480 --> 00:23:43,350
think about um and there are various
knobs that you can control um and you

1222
00:23:43,350 --> 00:23:43,360
knobs that you can control um and you
 

1223
00:23:43,360 --> 00:23:45,630
knobs that you can control um and you
may not you need to play with them a bit

1224
00:23:45,630 --> 00:23:45,640
may not you need to play with them a bit
 

1225
00:23:45,640 --> 00:23:47,350
may not you need to play with them a bit
to build up an appreciation of the

1226
00:23:47,350 --> 00:23:47,360
to build up an appreciation of the
 

1227
00:23:47,360 --> 00:23:49,630
to build up an appreciation of the
effects that they're going to have um

1228
00:23:49,630 --> 00:23:49,640
effects that they're going to have um
 

1229
00:23:49,640 --> 00:23:51,430
effects that they're going to have um
and there's something that I would

1230
00:23:51,430 --> 00:23:51,440
and there's something that I would
 

1231
00:23:51,440 --> 00:23:54,710
and there's something that I would
expect the um like need to go down for

1232
00:23:54,710 --> 00:23:54,720
expect the um like need to go down for
 

1233
00:23:54,720 --> 00:23:56,390
expect the um like need to go down for
in the future as well like temperature

1234
00:23:56,390 --> 00:23:56,400
in the future as well like temperature
 

1235
00:23:56,400 --> 00:23:58,870
in the future as well like temperature
is really just a way of balancing your

1236
00:23:58,870 --> 00:23:58,880
is really just a way of balancing your
 

1237
00:23:58,880 --> 00:24:01,990
is really just a way of balancing your
creativity um and uh like increasing

1238
00:24:01,990 --> 00:24:02,000
creativity um and uh like increasing
 

1239
00:24:02,000 --> 00:24:03,909
creativity um and uh like increasing
your creativity to make sure you don't

1240
00:24:03,909 --> 00:24:03,919
your creativity to make sure you don't
 

1241
00:24:03,919 --> 00:24:07,549
your creativity to make sure you don't
get the same response every time um and

1242
00:24:07,549 --> 00:24:07,559
get the same response every time um and
 

1243
00:24:07,559 --> 00:24:10,269
get the same response every time um and
uh that's uh that's I guess a knob that

1244
00:24:10,269 --> 00:24:10,279
uh that's uh that's I guess a knob that
 

1245
00:24:10,279 --> 00:24:14,669
uh that's uh that's I guess a knob that
we expect to sort of to stay um but um

1246
00:24:14,669 --> 00:24:14,679
we expect to sort of to stay um but um
 

1247
00:24:14,679 --> 00:24:16,110
we expect to sort of to stay um but um
be clear on what these parameters are

1248
00:24:16,110 --> 00:24:16,120
be clear on what these parameters are
 

1249
00:24:16,120 --> 00:24:17,549
be clear on what these parameters are
are doing and expect things to look a

1250
00:24:17,549 --> 00:24:17,559
are doing and expect things to look a
 

1251
00:24:17,559 --> 00:24:19,269
are doing and expect things to look a
bit different in the future just one

1252
00:24:19,269 --> 00:24:19,279
bit different in the future just one
 

1253
00:24:19,279 --> 00:24:21,029
bit different in the future just one
thing to add on to that for those of you

1254
00:24:21,029 --> 00:24:21,039
thing to add on to that for those of you
 

1255
00:24:21,039 --> 00:24:22,710
thing to add on to that for those of you
who haven't read The Prompt engineering

1256
00:24:22,710 --> 00:24:22,720
who haven't read The Prompt engineering
 

1257
00:24:22,720 --> 00:24:24,350
who haven't read The Prompt engineering
white paper yet we have a section on

1258
00:24:24,350 --> 00:24:24,360
white paper yet we have a section on
 

1259
00:24:24,360 --> 00:24:26,350
white paper yet we have a section on
automated prompt engineering which kind

1260
00:24:26,350 --> 00:24:26,360
automated prompt engineering which kind
 

1261
00:24:26,360 --> 00:24:28,149
automated prompt engineering which kind
of crafts a prompt uses together with

1262
00:24:28,149 --> 00:24:28,159
of crafts a prompt uses together with
 

1263
00:24:28,159 --> 00:24:30,750
of crafts a prompt uses together with
the evaluation topic it evaluates and

1264
00:24:30,750 --> 00:24:30,760
the evaluation topic it evaluates and
 

1265
00:24:30,760 --> 00:24:33,389
the evaluation topic it evaluates and
crafts it for you so have a look at that

1266
00:24:33,389 --> 00:24:33,399
crafts it for you so have a look at that
 

1267
00:24:33,399 --> 00:24:38,149
crafts it for you so have a look at that
it's very useful um for your projects

1268
00:24:38,149 --> 00:24:38,159
it's very useful um for your projects
 

1269
00:24:38,159 --> 00:24:41,029
it's very useful um for your projects
absolutely next question um

1270
00:24:41,029 --> 00:24:41,039
absolutely next question um
 

1271
00:24:41,039 --> 00:24:43,190
absolutely next question um
Hallucination is a major challenge for

1272
00:24:43,190 --> 00:24:43,200
Hallucination is a major challenge for
 

1273
00:24:43,200 --> 00:24:45,230
Hallucination is a major challenge for
large language models techniques like

1274
00:24:45,230 --> 00:24:45,240
large language models techniques like
 

1275
00:24:45,240 --> 00:24:47,750
large language models techniques like
Rag and prompt engineering can help um

1276
00:24:47,750 --> 00:24:47,760
Rag and prompt engineering can help um
 

1277
00:24:47,760 --> 00:24:49,870
Rag and prompt engineering can help um
but what are the most effective methods

1278
00:24:49,870 --> 00:24:49,880
but what are the most effective methods
 

1279
00:24:49,880 --> 00:24:53,269
but what are the most effective methods
that Google uses in Gemini 2 to minimize

1280
00:24:53,269 --> 00:24:53,279
that Google uses in Gemini 2 to minimize
 

1281
00:24:53,279 --> 00:24:55,389
that Google uses in Gemini 2 to minimize
uh incorrect or misleading outputs are

1282
00:24:55,389 --> 00:24:55,399
uh incorrect or misleading outputs are
 

1283
00:24:55,399 --> 00:24:57,149
uh incorrect or misleading outputs are
there trade-offs between reducing

1284
00:24:57,149 --> 00:24:57,159
there trade-offs between reducing
 

1285
00:24:57,159 --> 00:25:00,310
there trade-offs between reducing
hallucinations and model creativity um

1286
00:25:00,310 --> 00:25:00,320
hallucinations and model creativity um
 

1287
00:25:00,320 --> 00:25:02,230
hallucinations and model creativity um
Kieran uh do you want to do you want to

1288
00:25:02,230 --> 00:25:02,240
Kieran uh do you want to do you want to
 

1289
00:25:02,240 --> 00:25:04,149
Kieran uh do you want to do you want to
take a stab with this one sure

1290
00:25:04,149 --> 00:25:04,159
take a stab with this one sure
 

1291
00:25:04,159 --> 00:25:07,190
take a stab with this one sure
absolutely so reducing hallucinations um

1292
00:25:07,190 --> 00:25:07,200
absolutely so reducing hallucinations um
 

1293
00:25:07,200 --> 00:25:08,430
absolutely so reducing hallucinations um
or you know this is an area that we call

1294
00:25:08,430 --> 00:25:08,440
or you know this is an area that we call
 

1295
00:25:08,440 --> 00:25:10,389
or you know this is an area that we call
factuality as well it's one of our key

1296
00:25:10,389 --> 00:25:10,399
factuality as well it's one of our key
 

1297
00:25:10,399 --> 00:25:12,110
factuality as well it's one of our key
areas of focus it's one of the things

1298
00:25:12,110 --> 00:25:12,120
areas of focus it's one of the things
 

1299
00:25:12,120 --> 00:25:14,830
areas of focus it's one of the things
that um was very apparent in LMS in the

1300
00:25:14,830 --> 00:25:14,840
that um was very apparent in LMS in the
 

1301
00:25:14,840 --> 00:25:17,710
that um was very apparent in LMS in the
early days and um it's critical to make

1302
00:25:17,710 --> 00:25:17,720
early days and um it's critical to make
 

1303
00:25:17,720 --> 00:25:19,590
early days and um it's critical to make
them like most useful going forwards and

1304
00:25:19,590 --> 00:25:19,600
them like most useful going forwards and
 

1305
00:25:19,600 --> 00:25:20,870
them like most useful going forwards and
I think there has been a lot of progress

1306
00:25:20,870 --> 00:25:20,880
I think there has been a lot of progress
 

1307
00:25:20,880 --> 00:25:23,830
I think there has been a lot of progress
over the last couple of years um uh when

1308
00:25:23,830 --> 00:25:23,840
over the last couple of years um uh when
 

1309
00:25:23,840 --> 00:25:26,350
over the last couple of years um uh when
I think about H Nations I think of it in

1310
00:25:26,350 --> 00:25:26,360
I think about H Nations I think of it in
 

1311
00:25:26,360 --> 00:25:28,310
I think about H Nations I think of it in
sort of terms of sort of two different

1312
00:25:28,310 --> 00:25:28,320
sort of terms of sort of two different
 

1313
00:25:28,320 --> 00:25:31,710
sort of terms of sort of two different
framings um one is whether um Gemini is

1314
00:25:31,710 --> 00:25:31,720
framings um one is whether um Gemini is
 

1315
00:25:31,720 --> 00:25:34,149
framings um one is whether um Gemini is
trying to answer a question or Pro

1316
00:25:34,149 --> 00:25:34,159
trying to answer a question or Pro
 

1317
00:25:34,159 --> 00:25:36,070
trying to answer a question or Pro
provide an answer based on a bit of

1318
00:25:36,070 --> 00:25:36,080
provide an answer based on a bit of
 

1319
00:25:36,080 --> 00:25:37,870
provide an answer based on a bit of
context that has been input um so you

1320
00:25:37,870 --> 00:25:37,880
context that has been input um so you
 

1321
00:25:37,880 --> 00:25:39,789
context that has been input um so you
know that can be rag but it can also be

1322
00:25:39,789 --> 00:25:39,799
know that can be rag but it can also be
 

1323
00:25:39,799 --> 00:25:41,630
know that can be rag but it can also be
like answering questions over documents

1324
00:25:41,630 --> 00:25:41,640
like answering questions over documents
 

1325
00:25:41,640 --> 00:25:45,149
like answering questions over documents
that you provide um and also if you look

1326
00:25:45,149 --> 00:25:45,159
that you provide um and also if you look
 

1327
00:25:45,159 --> 00:25:47,510
that you provide um and also if you look
at the um the generative experience in

1328
00:25:47,510 --> 00:25:47,520
at the um the generative experience in
 

1329
00:25:47,520 --> 00:25:49,310
at the um the generative experience in
Google search that's what happens um

1330
00:25:49,310 --> 00:25:49,320
Google search that's what happens um
 

1331
00:25:49,320 --> 00:25:50,909
Google search that's what happens um
like gini will go away and do a search

1332
00:25:50,909 --> 00:25:50,919
like gini will go away and do a search
 

1333
00:25:50,919 --> 00:25:53,389
like gini will go away and do a search
and then uh of summarize those results

1334
00:25:53,389 --> 00:25:53,399
and then uh of summarize those results
 

1335
00:25:53,399 --> 00:25:55,909
and then uh of summarize those results
for you so um it's able to produce its

1336
00:25:55,909 --> 00:25:55,919
for you so um it's able to produce its
 

1337
00:25:55,919 --> 00:25:57,750
for you so um it's able to produce its
answer based entirely on knowledge that

1338
00:25:57,750 --> 00:25:57,760
answer based entirely on knowledge that
 

1339
00:25:57,760 --> 00:25:59,950
answer based entirely on knowledge that
is being Prov ided to it um and then

1340
00:25:59,950 --> 00:25:59,960
is being Prov ided to it um and then
 

1341
00:25:59,960 --> 00:26:02,190
is being Prov ided to it um and then
there's a second one which is where um

1342
00:26:02,190 --> 00:26:02,200
there's a second one which is where um
 

1343
00:26:02,200 --> 00:26:05,029
there's a second one which is where um
Gemini is um like answering essentially

1344
00:26:05,029 --> 00:26:05,039
Gemini is um like answering essentially
 

1345
00:26:05,039 --> 00:26:07,630
Gemini is um like answering essentially
from um from its training data or you

1346
00:26:07,630 --> 00:26:07,640
from um from its training data or you
 

1347
00:26:07,640 --> 00:26:09,590
from um from its training data or you
know if you think of human analogy it's

1348
00:26:09,590 --> 00:26:09,600
know if you think of human analogy it's
 

1349
00:26:09,600 --> 00:26:10,830
know if you think of human analogy it's
kind of like answering from your

1350
00:26:10,830 --> 00:26:10,840
kind of like answering from your
 

1351
00:26:10,840 --> 00:26:15,190
kind of like answering from your
education or um your memory um the first

1352
00:26:15,190 --> 00:26:15,200
education or um your memory um the first
 

1353
00:26:15,200 --> 00:26:17,950
education or um your memory um the first
is always going to be um like inherently

1354
00:26:17,950 --> 00:26:17,960
is always going to be um like inherently
 

1355
00:26:17,960 --> 00:26:20,669
is always going to be um like inherently
a bit easier for it to do and um you

1356
00:26:20,669 --> 00:26:20,679
a bit easier for it to do and um you
 

1357
00:26:20,679 --> 00:26:22,510
a bit easier for it to do and um you
know probably a safer bet you know I've

1358
00:26:22,510 --> 00:26:22,520
know probably a safer bet you know I've
 

1359
00:26:22,520 --> 00:26:24,630
know probably a safer bet you know I've
I it's a lot easier for me to verify an

1360
00:26:24,630 --> 00:26:24,640
I it's a lot easier for me to verify an
 

1361
00:26:24,640 --> 00:26:26,149
I it's a lot easier for me to verify an
answer if I'm looking at some Source

1362
00:26:26,149 --> 00:26:26,159
answer if I'm looking at some Source
 

1363
00:26:26,159 --> 00:26:27,870
answer if I'm looking at some Source
materials in front of me than if I'm

1364
00:26:27,870 --> 00:26:27,880
materials in front of me than if I'm
 

1365
00:26:27,880 --> 00:26:30,830
materials in front of me than if I'm
having to sort of troll through um some

1366
00:26:30,830 --> 00:26:30,840
having to sort of troll through um some
 

1367
00:26:30,840 --> 00:26:33,110
having to sort of troll through um some
distant knowledge of facts In My Memory

1368
00:26:33,110 --> 00:26:33,120
distant knowledge of facts In My Memory
 

1369
00:26:33,120 --> 00:26:36,510
distant knowledge of facts In My Memory
um so um really if you're trying to

1370
00:26:36,510 --> 00:26:36,520
um so um really if you're trying to
 

1371
00:26:36,520 --> 00:26:39,230
um so um really if you're trying to
reduce hallucinations finding ways to

1372
00:26:39,230 --> 00:26:39,240
reduce hallucinations finding ways to
 

1373
00:26:39,240 --> 00:26:42,070
reduce hallucinations finding ways to
ground to the input um is like a very

1374
00:26:42,070 --> 00:26:42,080
ground to the input um is like a very
 

1375
00:26:42,080 --> 00:26:43,789
ground to the input um is like a very
good strategy and like vertex offer

1376
00:26:43,789 --> 00:26:43,799
good strategy and like vertex offer
 

1377
00:26:43,799 --> 00:26:47,350
good strategy and like vertex offer
search grounding as um an option on the

1378
00:26:47,350 --> 00:26:47,360
search grounding as um an option on the
 

1379
00:26:47,360 --> 00:26:49,230
search grounding as um an option on the
API um so what you're really doing there

1380
00:26:49,230 --> 00:26:49,240
API um so what you're really doing there
 

1381
00:26:49,240 --> 00:26:51,350
API um so what you're really doing there
is giving verifiable um references to

1382
00:26:51,350 --> 00:26:51,360
is giving verifiable um references to
 

1383
00:26:51,360 --> 00:26:54,830
is giving verifiable um references to
justify the answers is coming out um um

1384
00:26:54,830 --> 00:26:54,840
justify the answers is coming out um um
 

1385
00:26:54,840 --> 00:26:57,750
justify the answers is coming out um um
beyond that um you know you can be uh

1386
00:26:57,750 --> 00:26:57,760
beyond that um you know you can be uh
 

1387
00:26:57,760 --> 00:26:59,750
beyond that um you know you can be uh
explicit in your prompt about what your

1388
00:26:59,750 --> 00:26:59,760
explicit in your prompt about what your
 

1389
00:26:59,760 --> 00:27:02,389
explicit in your prompt about what your
requirements here are that may help um

1390
00:27:02,389 --> 00:27:02,399
requirements here are that may help um
 

1391
00:27:02,399 --> 00:27:04,029
requirements here are that may help um
and you can also add on explicit

1392
00:27:04,029 --> 00:27:04,039
and you can also add on explicit
 

1393
00:27:04,039 --> 00:27:06,029
and you can also add on explicit
verification steps at the end if you

1394
00:27:06,029 --> 00:27:06,039
verification steps at the end if you
 

1395
00:27:06,039 --> 00:27:09,470
verification steps at the end if you
want to be really uh really um like uh

1396
00:27:09,470 --> 00:27:09,480
want to be really uh really um like uh
 

1397
00:27:09,480 --> 00:27:12,950
want to be really uh really um like uh
mindful about your factuality um uh llms

1398
00:27:12,950 --> 00:27:12,960
mindful about your factuality um uh llms
 

1399
00:27:12,960 --> 00:27:15,029
mindful about your factuality um uh llms
are also very good at verifying answers

1400
00:27:15,029 --> 00:27:15,039
are also very good at verifying answers
 

1401
00:27:15,039 --> 00:27:16,590
are also very good at verifying answers
and checking whether an answer coming

1402
00:27:16,590 --> 00:27:16,600
and checking whether an answer coming
 

1403
00:27:16,600 --> 00:27:19,789
and checking whether an answer coming
back is uh meets a certain criteria so

1404
00:27:19,789 --> 00:27:19,799
back is uh meets a certain criteria so
 

1405
00:27:19,799 --> 00:27:21,789
back is uh meets a certain criteria so
um you can try that strategy um even

1406
00:27:21,789 --> 00:27:21,799
um you can try that strategy um even
 

1407
00:27:21,799 --> 00:27:24,389
um you can try that strategy um even
maybe combined with self-correction um

1408
00:27:24,389 --> 00:27:24,399
maybe combined with self-correction um
 

1409
00:27:24,399 --> 00:27:26,269
maybe combined with self-correction um
to uh like go away and correct any

1410
00:27:26,269 --> 00:27:26,279
to uh like go away and correct any
 

1411
00:27:26,279 --> 00:27:29,630
to uh like go away and correct any
errors that you see um and in terms of

1412
00:27:29,630 --> 00:27:29,640
errors that you see um and in terms of
 

1413
00:27:29,640 --> 00:27:31,950
errors that you see um and in terms of
the balance between uh like factuality

1414
00:27:31,950 --> 00:27:31,960
the balance between uh like factuality
 

1415
00:27:31,960 --> 00:27:35,430
the balance between uh like factuality
and creativity um you know there is a uh

1416
00:27:35,430 --> 00:27:35,440
and creativity um you know there is a uh
 

1417
00:27:35,440 --> 00:27:37,549
and creativity um you know there is a uh
a trade-off there um you know it's

1418
00:27:37,549 --> 00:27:37,559
a trade-off there um you know it's
 

1419
00:27:37,559 --> 00:27:40,310
a trade-off there um you know it's
difficult to write a very creative poem

1420
00:27:40,310 --> 00:27:40,320
difficult to write a very creative poem
 

1421
00:27:40,320 --> 00:27:43,789
difficult to write a very creative poem
that is grounded in sort of real fact um

1422
00:27:43,789 --> 00:27:43,799
that is grounded in sort of real fact um
 

1423
00:27:43,799 --> 00:27:46,389
that is grounded in sort of real fact um
um but uh you know something to be aware

1424
00:27:46,389 --> 00:27:46,399
um but uh you know something to be aware
 

1425
00:27:46,399 --> 00:27:47,710
um but uh you know something to be aware
of coming back to the previous point

1426
00:27:47,710 --> 00:27:47,720
of coming back to the previous point
 

1427
00:27:47,720 --> 00:27:49,710
of coming back to the previous point
about essentially the temperature knob

1428
00:27:49,710 --> 00:27:49,720
about essentially the temperature knob
 

1429
00:27:49,720 --> 00:27:52,310
about essentially the temperature knob
um when you increase the temperature um

1430
00:27:52,310 --> 00:27:52,320
um when you increase the temperature um
 

1431
00:27:52,320 --> 00:27:55,149
um when you increase the temperature um
what you're doing is you're making it um

1432
00:27:55,149 --> 00:27:55,159
what you're doing is you're making it um
 

1433
00:27:55,159 --> 00:27:56,710
what you're doing is you're making it um
uh you're making it more likely for the

1434
00:27:56,710 --> 00:27:56,720
uh you're making it more likely for the
 

1435
00:27:56,720 --> 00:27:59,350
uh you're making it more likely for the
model to pick a less probable token uh

1436
00:27:59,350 --> 00:27:59,360
model to pick a less probable token uh
 

1437
00:27:59,360 --> 00:28:00,990
model to pick a less probable token uh
because these are probabilistic models

1438
00:28:00,990 --> 00:28:01,000
because these are probabilistic models
 

1439
00:28:01,000 --> 00:28:04,110
because these are probabilistic models
um so when you're exploring your

1440
00:28:04,110 --> 00:28:04,120
um so when you're exploring your
 

1441
00:28:04,120 --> 00:28:06,789
um so when you're exploring your
factuality you can play it safe and uh

1442
00:28:06,789 --> 00:28:06,799
factuality you can play it safe and uh
 

1443
00:28:06,799 --> 00:28:08,830
factuality you can play it safe and uh
turn the temperature right down to zero

1444
00:28:08,830 --> 00:28:08,840
turn the temperature right down to zero
 

1445
00:28:08,840 --> 00:28:11,149
turn the temperature right down to zero
and you can also look empirically at the

1446
00:28:11,149 --> 00:28:11,159
and you can also look empirically at the
 

1447
00:28:11,159 --> 00:28:12,590
and you can also look empirically at the
results that you're getting back as you

1448
00:28:12,590 --> 00:28:12,600
results that you're getting back as you
 

1449
00:28:12,600 --> 00:28:14,789
results that you're getting back as you
increase that to see what is the right

1450
00:28:14,789 --> 00:28:14,799
increase that to see what is the right
 

1451
00:28:14,799 --> 00:28:16,590
increase that to see what is the right
balance for you in your use

1452
00:28:16,590 --> 00:28:16,600
balance for you in your use
 

1453
00:28:16,600 --> 00:28:19,669
balance for you in your use
case absolutely just like uh Warren was

1454
00:28:19,669 --> 00:28:19,679
case absolutely just like uh Warren was
 

1455
00:28:19,679 --> 00:28:21,590
case absolutely just like uh Warren was
mentioning a little bit earlier I love

1456
00:28:21,590 --> 00:28:21,600
mentioning a little bit earlier I love
 

1457
00:28:21,600 --> 00:28:23,430
mentioning a little bit earlier I love
all of the knobs and dials that you can

1458
00:28:23,430 --> 00:28:23,440
all of the knobs and dials that you can
 

1459
00:28:23,440 --> 00:28:25,750
all of the knobs and dials that you can
use to to ground and to reduce the

1460
00:28:25,750 --> 00:28:25,760
use to to ground and to reduce the
 

1461
00:28:25,760 --> 00:28:27,669
use to to ground and to reduce the
likelihood of hallucinations for models

1462
00:28:27,669 --> 00:28:27,679
likelihood of hallucinations for models
 

1463
00:28:27,679 --> 00:28:30,110
likelihood of hallucinations for models
things like search grounding or or code

1464
00:28:30,110 --> 00:28:30,120
things like search grounding or or code
 

1465
00:28:30,120 --> 00:28:33,149
things like search grounding or or code
execution um these are all wonderful um

1466
00:28:33,149 --> 00:28:33,159
execution um these are all wonderful um
 

1467
00:28:33,159 --> 00:28:35,549
execution um these are all wonderful um
wonderful methods for for getting the

1468
00:28:35,549 --> 00:28:35,559
wonderful methods for for getting the
 

1469
00:28:35,559 --> 00:28:37,430
wonderful methods for for getting the
right kinds of outputs from from the

1470
00:28:37,430 --> 00:28:37,440
right kinds of outputs from from the
 

1471
00:28:37,440 --> 00:28:39,950
right kinds of outputs from from the
models that we have available

1472
00:28:39,950 --> 00:28:39,960
models that we have available
 

1473
00:28:39,960 --> 00:28:43,350
models that we have available
today and with that uh thank you so much

1474
00:28:43,350 --> 00:28:43,360
today and with that uh thank you so much
 

1475
00:28:43,360 --> 00:28:45,990
today and with that uh thank you so much
to all of our expert guests uh for

1476
00:28:45,990 --> 00:28:46,000
to all of our expert guests uh for
 

1477
00:28:46,000 --> 00:28:47,870
to all of our expert guests uh for
coming today for for getting to share a

1478
00:28:47,870 --> 00:28:47,880
coming today for for getting to share a
 

1479
00:28:47,880 --> 00:28:49,029
coming today for for getting to share a
little bit more about what you're

1480
00:28:49,029 --> 00:28:49,039
little bit more about what you're
 

1481
00:28:49,039 --> 00:28:50,710
little bit more about what you're
working on and to answer all of our

1482
00:28:50,710 --> 00:28:50,720
working on and to answer all of our
 

1483
00:28:50,720 --> 00:28:52,870
working on and to answer all of our
great Community questions we really

1484
00:28:52,870 --> 00:28:52,880
great Community questions we really
 

1485
00:28:52,880 --> 00:28:56,029
great Community questions we really
appreciate you and your time uh and am

1486
00:28:56,029 --> 00:28:56,039
appreciate you and your time uh and am
 

1487
00:28:56,039 --> 00:28:58,350
appreciate you and your time uh and am
looking forward to seeing what uh what

1488
00:28:58,350 --> 00:28:58,360
looking forward to seeing what uh what
 

1489
00:28:58,360 --> 00:29:00,750
looking forward to seeing what uh what
y'all um build and release over the

1490
00:29:00,750 --> 00:29:00,760
y'all um build and release over the
 

1491
00:29:00,760 --> 00:29:02,389
y'all um build and release over the
course of the next several months this

1492
00:29:02,389 --> 00:29:02,399
course of the next several months this
 

1493
00:29:02,399 --> 00:29:06,230
course of the next several months this
has been excellent thank

1494
00:29:06,230 --> 00:29:06,240
has been excellent thank
 

1495
00:29:06,240 --> 00:29:10,470
has been excellent thank
you wonderful and now uh uh one of the

1496
00:29:10,470 --> 00:29:10,480
you wonderful and now uh uh one of the
 

1497
00:29:10,480 --> 00:29:14,110
you wonderful and now uh uh one of the
most exciting parts of the of the the

1498
00:29:14,110 --> 00:29:14,120
most exciting parts of the of the the
 

1499
00:29:14,120 --> 00:29:15,950
most exciting parts of the of the the
sort of kaggle generative AI intensive

1500
00:29:15,950 --> 00:29:15,960
sort of kaggle generative AI intensive
 

1501
00:29:15,960 --> 00:29:17,909
sort of kaggle generative AI intensive
course at least to me I'm delighted to

1502
00:29:17,909 --> 00:29:17,919
course at least to me I'm delighted to
 

1503
00:29:17,919 --> 00:29:20,830
course at least to me I'm delighted to
welcome on to the stage um anant who is

1504
00:29:20,830 --> 00:29:20,840
welcome on to the stage um anant who is
 

1505
00:29:20,840 --> 00:29:23,430
welcome on to the stage um anant who is
going to be going over our code labs and

1506
00:29:23,430 --> 00:29:23,440
going to be going over our code labs and
 

1507
00:29:23,440 --> 00:29:26,630
going to be going over our code labs and
demos um as well as giving a a brief

1508
00:29:26,630 --> 00:29:26,640
demos um as well as giving a a brief
 

1509
00:29:26,640 --> 00:29:28,590
demos um as well as giving a a brief
overview of of some of the cont content

1510
00:29:28,590 --> 00:29:28,600
overview of of some of the cont content
 

1511
00:29:28,600 --> 00:29:29,750
overview of of some of the cont content
that you all should have been learning

1512
00:29:29,750 --> 00:29:29,760
that you all should have been learning
 

1513
00:29:29,760 --> 00:29:31,590
that you all should have been learning
about over the course of the last 24

1514
00:29:31,590 --> 00:29:31,600
about over the course of the last 24
 

1515
00:29:31,600 --> 00:29:35,230
about over the course of the last 24
hours um take it away an a thanks paig

1516
00:29:35,230 --> 00:29:35,240
hours um take it away an a thanks paig
 

1517
00:29:35,240 --> 00:29:39,110
hours um take it away an a thanks paig
so hi everyone um so uh we aware that uh

1518
00:29:39,110 --> 00:29:39,120
so hi everyone um so uh we aware that uh
 

1519
00:29:39,120 --> 00:29:40,549
so hi everyone um so uh we aware that uh
some of you are directly joining for the

1520
00:29:40,549 --> 00:29:40,559
some of you are directly joining for the
 

1521
00:29:40,559 --> 00:29:42,590
some of you are directly joining for the
live stream and uh I'm going to give you

1522
00:29:42,590 --> 00:29:42,600
live stream and uh I'm going to give you
 

1523
00:29:42,600 --> 00:29:44,430
live stream and uh I'm going to give you
a quick overview of the white papers and

1524
00:29:44,430 --> 00:29:44,440
a quick overview of the white papers and
 

1525
00:29:44,440 --> 00:29:46,149
a quick overview of the white papers and
then move on to the code labs to give

1526
00:29:46,149 --> 00:29:46,159
then move on to the code labs to give
 

1527
00:29:46,159 --> 00:29:48,070
then move on to the code labs to give
you a flavor of what you learned or will

1528
00:29:48,070 --> 00:29:48,080
you a flavor of what you learned or will
 

1529
00:29:48,080 --> 00:29:50,070
you a flavor of what you learned or will
learn if you have haven't had a chance

1530
00:29:50,070 --> 00:29:50,080
learn if you have haven't had a chance
 

1531
00:29:50,080 --> 00:29:53,789
learn if you have haven't had a chance
to dive into it so uh uh let's start off

1532
00:29:53,789 --> 00:29:53,799
to dive into it so uh uh let's start off
 

1533
00:29:53,799 --> 00:29:56,549
to dive into it so uh uh let's start off
with the white paper overview so the we

1534
00:29:56,549 --> 00:29:56,559
with the white paper overview so the we
 

1535
00:29:56,559 --> 00:29:58,430
with the white paper overview so the we
had two white papers the found one on

1536
00:29:58,430 --> 00:29:58,440
had two white papers the found one on
 

1537
00:29:58,440 --> 00:30:00,230
had two white papers the found one on
foundational models and the one on

1538
00:30:00,230 --> 00:30:00,240
foundational models and the one on
 

1539
00:30:00,240 --> 00:30:02,470
foundational models and the one on
prompt engineering and first in the

1540
00:30:02,470 --> 00:30:02,480
prompt engineering and first in the
 

1541
00:30:02,480 --> 00:30:04,750
prompt engineering and first in the
foundational model we looked at the uh

1542
00:30:04,750 --> 00:30:04,760
foundational model we looked at the uh
 

1543
00:30:04,760 --> 00:30:07,230
foundational model we looked at the uh
basically how these AI models which

1544
00:30:07,230 --> 00:30:07,240
basically how these AI models which
 

1545
00:30:07,240 --> 00:30:09,269
basically how these AI models which
often use a Transformer architecture

1546
00:30:09,269 --> 00:30:09,279
often use a Transformer architecture
 

1547
00:30:09,279 --> 00:30:11,230
often use a Transformer architecture
leverage the attention mechanism to

1548
00:30:11,230 --> 00:30:11,240
leverage the attention mechanism to
 

1549
00:30:11,240 --> 00:30:14,549
leverage the attention mechanism to
train on like large Corpus and leverage

1550
00:30:14,549 --> 00:30:14,559
train on like large Corpus and leverage
 

1551
00:30:14,559 --> 00:30:16,830
train on like large Corpus and leverage
large context we also looked at

1552
00:30:16,830 --> 00:30:16,840
large context we also looked at
 

1553
00:30:16,840 --> 00:30:18,909
large context we also looked at
architectural variations like mixture of

1554
00:30:18,909 --> 00:30:18,919
architectural variations like mixture of
 

1555
00:30:18,919 --> 00:30:20,909
architectural variations like mixture of
experts how they can improve efficiency

1556
00:30:20,909 --> 00:30:20,919
experts how they can improve efficiency
 

1557
00:30:20,919 --> 00:30:23,430
experts how they can improve efficiency
and quality we also saw the rapid

1558
00:30:23,430 --> 00:30:23,440
and quality we also saw the rapid
 

1559
00:30:23,440 --> 00:30:25,190
and quality we also saw the rapid
Evolution through different models all

1560
00:30:25,190 --> 00:30:25,200
Evolution through different models all
 

1561
00:30:25,200 --> 00:30:28,230
Evolution through different models all
the way from birt Palm to Gemini along

1562
00:30:28,230 --> 00:30:28,240
the way from birt Palm to Gemini along
 

1563
00:30:28,240 --> 00:30:30,750
the way from birt Palm to Gemini along
with several open um models as well

1564
00:30:30,750 --> 00:30:30,760
with several open um models as well
 

1565
00:30:30,760 --> 00:30:33,750
with several open um models as well
driven by scaling data and model size

1566
00:30:33,750 --> 00:30:33,760
driven by scaling data and model size
 

1567
00:30:33,760 --> 00:30:36,149
driven by scaling data and model size
next we looked at how these models are

1568
00:30:36,149 --> 00:30:36,159
next we looked at how these models are
 

1569
00:30:36,159 --> 00:30:38,909
next we looked at how these models are
trained and adapted so they start off

1570
00:30:38,909 --> 00:30:38,919
trained and adapted so they start off
 

1571
00:30:38,919 --> 00:30:41,350
trained and adapted so they start off
with pre-training on v data sets for

1572
00:30:41,350 --> 00:30:41,360
with pre-training on v data sets for
 

1573
00:30:41,360 --> 00:30:43,630
with pre-training on v data sets for
basically just building a general

1574
00:30:43,630 --> 00:30:43,640
basically just building a general
 

1575
00:30:43,640 --> 00:30:46,470
basically just building a general
understanding of um the modality they're

1576
00:30:46,470 --> 00:30:46,480
understanding of um the modality they're
 

1577
00:30:46,480 --> 00:30:49,269
understanding of um the modality they're
training on then often to ensure that

1578
00:30:49,269 --> 00:30:49,279
training on then often to ensure that
 

1579
00:30:49,279 --> 00:30:51,269
training on then often to ensure that
they we looked at basically to ensure

1580
00:30:51,269 --> 00:30:51,279
they we looked at basically to ensure
 

1581
00:30:51,279 --> 00:30:53,230
they we looked at basically to ensure
that they follow the instructions that

1582
00:30:53,230 --> 00:30:53,240
that they follow the instructions that
 

1583
00:30:53,240 --> 00:30:55,830
that they follow the instructions that
you provide we use super they go through

1584
00:30:55,830 --> 00:30:55,840
you provide we use super they go through
 

1585
00:30:55,840 --> 00:30:59,190
you provide we use super they go through
a supervised fine tuning state AG um uh

1586
00:30:59,190 --> 00:30:59,200
a supervised fine tuning state AG um uh
 

1587
00:30:59,200 --> 00:31:01,350
a supervised fine tuning state AG um uh
to improve instruction following and to

1588
00:31:01,350 --> 00:31:01,360
to improve instruction following and to
 

1589
00:31:01,360 --> 00:31:04,950
to improve instruction following and to
make them more task specific and lastly

1590
00:31:04,950 --> 00:31:04,960
make them more task specific and lastly
 

1591
00:31:04,960 --> 00:31:06,590
make them more task specific and lastly
um we looked at how reinforcement

1592
00:31:06,590 --> 00:31:06,600
um we looked at how reinforcement
 

1593
00:31:06,600 --> 00:31:08,870
um we looked at how reinforcement
learning from Human feedback also called

1594
00:31:08,870 --> 00:31:08,880
learning from Human feedback also called
 

1595
00:31:08,880 --> 00:31:11,870
learning from Human feedback also called
rlf aligns the outputs with human

1596
00:31:11,870 --> 00:31:11,880
rlf aligns the outputs with human
 

1597
00:31:11,880 --> 00:31:14,590
rlf aligns the outputs with human
preferences for basically things like uh

1598
00:31:14,590 --> 00:31:14,600
preferences for basically things like uh
 

1599
00:31:14,600 --> 00:31:17,549
preferences for basically things like uh
helpfulness and safety then we looked at

1600
00:31:17,549 --> 00:31:17,559
helpfulness and safety then we looked at
 

1601
00:31:17,559 --> 00:31:20,110
helpfulness and safety then we looked at
okay the training uh tuning these models

1602
00:31:20,110 --> 00:31:20,120
okay the training uh tuning these models
 

1603
00:31:20,120 --> 00:31:22,789
okay the training uh tuning these models
especially to tasks um Downstream toss

1604
00:31:22,789 --> 00:31:22,799
especially to tasks um Downstream toss
 

1605
00:31:22,799 --> 00:31:25,269
especially to tasks um Downstream toss
quite expensive so we can do we use

1606
00:31:25,269 --> 00:31:25,279
quite expensive so we can do we use
 

1607
00:31:25,279 --> 00:31:27,710
quite expensive so we can do we use
various parameter efficient methods

1608
00:31:27,710 --> 00:31:27,720
various parameter efficient methods
 

1609
00:31:27,720 --> 00:31:30,110
various parameter efficient methods
which which uh make uh tuning these uh

1610
00:31:30,110 --> 00:31:30,120
which which uh make uh tuning these uh
 

1611
00:31:30,120 --> 00:31:32,750
which which uh make uh tuning these uh
models very efficient we all in the

1612
00:31:32,750 --> 00:31:32,760
models very efficient we all in the
 

1613
00:31:32,760 --> 00:31:35,149
models very efficient we all in the
second paper on prompt engineering right

1614
00:31:35,149 --> 00:31:35,159
second paper on prompt engineering right
 

1615
00:31:35,159 --> 00:31:36,710
second paper on prompt engineering right
um this is a core part of what we

1616
00:31:36,710 --> 00:31:36,720
um this is a core part of what we
 

1617
00:31:36,720 --> 00:31:38,950
um this is a core part of what we
discussed in the earlier on in the Q&A

1618
00:31:38,950 --> 00:31:38,960
discussed in the earlier on in the Q&A
 

1619
00:31:38,960 --> 00:31:40,669
discussed in the earlier on in the Q&A
session we looked at the various

1620
00:31:40,669 --> 00:31:40,679
session we looked at the various
 

1621
00:31:40,679 --> 00:31:42,830
session we looked at the various
prompting techniques all the way from

1622
00:31:42,830 --> 00:31:42,840
prompting techniques all the way from
 

1623
00:31:42,840 --> 00:31:45,029
prompting techniques all the way from
generation or sampling parameters such

1624
00:31:45,029 --> 00:31:45,039
generation or sampling parameters such
 

1625
00:31:45,039 --> 00:31:47,990
generation or sampling parameters such
as temperature which controls the

1626
00:31:47,990 --> 00:31:48,000
as temperature which controls the
 

1627
00:31:48,000 --> 00:31:50,070
as temperature which controls the
randomness uh and diversity of the

1628
00:31:50,070 --> 00:31:50,080
randomness uh and diversity of the
 

1629
00:31:50,080 --> 00:31:53,310
randomness uh and diversity of the
output top p and top K and how it's

1630
00:31:53,310 --> 00:31:53,320
output top p and top K and how it's
 

1631
00:31:53,320 --> 00:31:55,909
output top p and top K and how it's
crucial to balance predictability and

1632
00:31:55,909 --> 00:31:55,919
crucial to balance predictability and
 

1633
00:31:55,919 --> 00:31:58,470
crucial to balance predictability and
creativ creativity depending on the task

1634
00:31:58,470 --> 00:31:58,480
creativ creativity depending on the task
 

1635
00:31:58,480 --> 00:32:01,590
creativ creativity depending on the task
you are using them for we also looked at

1636
00:32:01,590 --> 00:32:01,600
you are using them for we also looked at
 

1637
00:32:01,600 --> 00:32:03,629
you are using them for we also looked at
various prompting techniques all the way

1638
00:32:03,629 --> 00:32:03,639
various prompting techniques all the way
 

1639
00:32:03,639 --> 00:32:06,590
various prompting techniques all the way
from uh basic zero shot techniques where

1640
00:32:06,590 --> 00:32:06,600
from uh basic zero shot techniques where
 

1641
00:32:06,600 --> 00:32:08,870
from uh basic zero shot techniques where
you just ask the llm to providing

1642
00:32:08,870 --> 00:32:08,880
you just ask the llm to providing
 

1643
00:32:08,880 --> 00:32:11,990
you just ask the llm to providing
examples to the llm uh uh to make it

1644
00:32:11,990 --> 00:32:12,000
examples to the llm uh uh to make it
 

1645
00:32:12,000 --> 00:32:13,669
examples to the llm uh uh to make it
more effective for the tasks that you're

1646
00:32:13,669 --> 00:32:13,679
more effective for the tasks that you're
 

1647
00:32:13,679 --> 00:32:15,750
more effective for the tasks that you're
using it for we also looked at

1648
00:32:15,750 --> 00:32:15,760
using it for we also looked at
 

1649
00:32:15,760 --> 00:32:17,990
using it for we also looked at
structuring prompts uh or structuring

1650
00:32:17,990 --> 00:32:18,000
structuring prompts uh or structuring
 

1651
00:32:18,000 --> 00:32:20,590
structuring prompts uh or structuring
prompts uh with system instructions task

1652
00:32:20,590 --> 00:32:20,600
prompts uh with system instructions task
 

1653
00:32:20,600 --> 00:32:23,110
prompts uh with system instructions task
relevant context or assigning a role

1654
00:32:23,110 --> 00:32:23,120
relevant context or assigning a role
 

1655
00:32:23,120 --> 00:32:25,590
relevant context or assigning a role
also can help guide the model and we

1656
00:32:25,590 --> 00:32:25,600
also can help guide the model and we
 

1657
00:32:25,600 --> 00:32:28,509
also can help guide the model and we
also then um later on uh look at things

1658
00:32:28,509 --> 00:32:28,519
also then um later on uh look at things
 

1659
00:32:28,519 --> 00:32:30,350
also then um later on uh look at things
like for more complex problems which

1660
00:32:30,350 --> 00:32:30,360
like for more complex problems which
 

1661
00:32:30,360 --> 00:32:32,269
like for more complex problems which
involve reasoning Chain of Thought

1662
00:32:32,269 --> 00:32:32,279
involve reasoning Chain of Thought
 

1663
00:32:32,279 --> 00:32:35,509
involve reasoning Chain of Thought
models uh step by prompting prompting

1664
00:32:35,509 --> 00:32:35,519
models uh step by prompting prompting
 

1665
00:32:35,519 --> 00:32:37,430
models uh step by prompting prompting
and uh other techniques which can help

1666
00:32:37,430 --> 00:32:37,440
and uh other techniques which can help
 

1667
00:32:37,440 --> 00:32:40,070
and uh other techniques which can help
with uh enhance complex problem solving

1668
00:32:40,070 --> 00:32:40,080
with uh enhance complex problem solving
 

1669
00:32:40,080 --> 00:32:43,190
with uh enhance complex problem solving
for models finally uh in the

1670
00:32:43,190 --> 00:32:43,200
for models finally uh in the
 

1671
00:32:43,200 --> 00:32:46,029
for models finally uh in the
foundational model chapter we looked at

1672
00:32:46,029 --> 00:32:46,039
foundational model chapter we looked at
 

1673
00:32:46,039 --> 00:32:48,870
foundational model chapter we looked at
um uh how to optimize the models for

1674
00:32:48,870 --> 00:32:48,880
um uh how to optimize the models for
 

1675
00:32:48,880 --> 00:32:50,909
um uh how to optimize the models for
Speed and efficiency as discussed

1676
00:32:50,909 --> 00:32:50,919
Speed and efficiency as discussed
 

1677
00:32:50,919 --> 00:32:52,830
Speed and efficiency as discussed
earlier on in one of the Q&A questions

1678
00:32:52,830 --> 00:32:52,840
earlier on in one of the Q&A questions
 

1679
00:32:52,840 --> 00:32:54,629
earlier on in one of the Q&A questions
there various techniques that we can use

1680
00:32:54,629 --> 00:32:54,639
there various techniques that we can use
 

1681
00:32:54,639 --> 00:32:56,230
there various techniques that we can use
for that from

1682
00:32:56,230 --> 00:32:56,240
for that from
 

1683
00:32:56,240 --> 00:32:58,430
for that from
quantization uh distill

1684
00:32:58,430 --> 00:32:58,440
quantization uh distill
 

1685
00:32:58,440 --> 00:33:00,470
quantization uh distill
as well as various other techniques such

1686
00:33:00,470 --> 00:33:00,480
as well as various other techniques such
 

1687
00:33:00,480 --> 00:33:02,830
as well as various other techniques such
as speculative decoding which basically

1688
00:33:02,830 --> 00:33:02,840
as speculative decoding which basically
 

1689
00:33:02,840 --> 00:33:05,789
as speculative decoding which basically
makes models faster and cheaper I would

1690
00:33:05,789 --> 00:33:05,799
makes models faster and cheaper I would
 

1691
00:33:05,799 --> 00:33:08,029
makes models faster and cheaper I would
recommend uh looking into the podcast of

1692
00:33:08,029 --> 00:33:08,039
recommend uh looking into the podcast of
 

1693
00:33:08,039 --> 00:33:10,590
recommend uh looking into the podcast of
white paper to know more about them and

1694
00:33:10,590 --> 00:33:10,600
white paper to know more about them and
 

1695
00:33:10,600 --> 00:33:13,230
white paper to know more about them and
then finally we concluded the whole um

1696
00:33:13,230 --> 00:33:13,240
then finally we concluded the whole um
 

1697
00:33:13,240 --> 00:33:15,230
then finally we concluded the whole um
paper with looking at evaluation and

1698
00:33:15,230 --> 00:33:15,240
paper with looking at evaluation and
 

1699
00:33:15,240 --> 00:33:17,070
paper with looking at evaluation and
best practices for evaluating your

1700
00:33:17,070 --> 00:33:17,080
best practices for evaluating your
 

1701
00:33:17,080 --> 00:33:19,870
best practices for evaluating your
models using other llms as well as well

1702
00:33:19,870 --> 00:33:19,880
models using other llms as well as well
 

1703
00:33:19,880 --> 00:33:24,070
models using other llms as well as well
as uh simpler techniques so yes um now

1704
00:33:24,070 --> 00:33:24,080
as uh simpler techniques so yes um now
 

1705
00:33:24,080 --> 00:33:25,909
as uh simpler techniques so yes um now
that we have uh kind of gotten an

1706
00:33:25,909 --> 00:33:25,919
that we have uh kind of gotten an
 

1707
00:33:25,919 --> 00:33:28,629
that we have uh kind of gotten an
overview of what we that in the reading

1708
00:33:28,629 --> 00:33:28,639
overview of what we that in the reading
 

1709
00:33:28,639 --> 00:33:31,870
overview of what we that in the reading
assignments let us dive straight into

1710
00:33:31,870 --> 00:33:31,880
assignments let us dive straight into
 

1711
00:33:31,880 --> 00:33:33,549
assignments let us dive straight into
I'm going to be sharing my screen for

1712
00:33:33,549 --> 00:33:33,559
I'm going to be sharing my screen for
 

1713
00:33:33,559 --> 00:33:40,830
I'm going to be sharing my screen for
the code

1714
00:33:40,830 --> 00:33:40,840

 

1715
00:33:40,840 --> 00:33:44,470

lab so hope everybody can see my screen

1716
00:33:44,470 --> 00:33:44,480
lab so hope everybody can see my screen
 

1717
00:33:44,480 --> 00:33:47,710
lab so hope everybody can see my screen
so the first code lab um that you all

1718
00:33:47,710 --> 00:33:47,720
so the first code lab um that you all
 

1719
00:33:47,720 --> 00:33:51,070
so the first code lab um that you all
have received basically dives into um

1720
00:33:51,070 --> 00:33:51,080
have received basically dives into um
 

1721
00:33:51,080 --> 00:33:52,990
have received basically dives into um
prompt engineering and how we can

1722
00:33:52,990 --> 00:33:53,000
prompt engineering and how we can
 

1723
00:33:53,000 --> 00:33:57,269
prompt engineering and how we can
utilize these um um models to for your

1724
00:33:57,269 --> 00:33:57,279
utilize these um um models to for your
 

1725
00:33:57,279 --> 00:34:00,230
utilize these um um models to for your
uh applic ations so the first part of it

1726
00:34:00,230 --> 00:34:00,240
uh applic ations so the first part of it
 

1727
00:34:00,240 --> 00:34:03,430
uh applic ations so the first part of it
is basically just using Gemini 2.0 flash

1728
00:34:03,430 --> 00:34:03,440
is basically just using Gemini 2.0 flash
 

1729
00:34:03,440 --> 00:34:06,470
is basically just using Gemini 2.0 flash
which is one of our most uh performant

1730
00:34:06,470 --> 00:34:06,480
which is one of our most uh performant
 

1731
00:34:06,480 --> 00:34:09,310
which is one of our most uh performant
um performance and speed balance models

1732
00:34:09,310 --> 00:34:09,320
um performance and speed balance models
 

1733
00:34:09,320 --> 00:34:13,109
um performance and speed balance models
uh uh that uh we released and this uh we

1734
00:34:13,109 --> 00:34:13,119
uh uh that uh we released and this uh we
 

1735
00:34:13,119 --> 00:34:15,470
uh uh that uh we released and this uh we
use it to see how we can do simple

1736
00:34:15,470 --> 00:34:15,480
use it to see how we can do simple
 

1737
00:34:15,480 --> 00:34:19,030
use it to see how we can do simple
prompting here then uh we move on from

1738
00:34:19,030 --> 00:34:19,040
prompting here then uh we move on from
 

1739
00:34:19,040 --> 00:34:21,190
prompting here then uh we move on from
this single turn prompting where we just

1740
00:34:21,190 --> 00:34:21,200
this single turn prompting where we just
 

1741
00:34:21,200 --> 00:34:23,990
this single turn prompting where we just
give an input and get an output to a

1742
00:34:23,990 --> 00:34:24,000
give an input and get an output to a
 

1743
00:34:24,000 --> 00:34:26,030
give an input and get an output to a
multi-term structure where we can have a

1744
00:34:26,030 --> 00:34:26,040
multi-term structure where we can have a
 

1745
00:34:26,040 --> 00:34:29,310
multi-term structure where we can have a
conversation with the model and the the

1746
00:34:29,310 --> 00:34:29,320
conversation with the model and the the
 

1747
00:34:29,320 --> 00:34:31,470
conversation with the model and the the
the the API automatically retains

1748
00:34:31,470 --> 00:34:31,480
the the API automatically retains
 

1749
00:34:31,480 --> 00:34:33,710
the the API automatically retains
previous context to inform forther

1750
00:34:33,710 --> 00:34:33,720
previous context to inform forther
 

1751
00:34:33,720 --> 00:34:35,669
previous context to inform forther
responses by the model for

1752
00:34:35,669 --> 00:34:35,679
responses by the model for
 

1753
00:34:35,679 --> 00:34:38,190
responses by the model for
conversational applications and then we

1754
00:34:38,190 --> 00:34:38,200
conversational applications and then we
 

1755
00:34:38,200 --> 00:34:39,710
conversational applications and then we
looked at the various option that you

1756
00:34:39,710 --> 00:34:39,720
looked at the various option that you
 

1757
00:34:39,720 --> 00:34:41,909
looked at the various option that you
can uh of models that we can utilize

1758
00:34:41,909 --> 00:34:41,919
can uh of models that we can utilize
 

1759
00:34:41,919 --> 00:34:45,550
can uh of models that we can utilize
from the API Beyond just 2.0 flash this

1760
00:34:45,550 --> 00:34:45,560
from the API Beyond just 2.0 flash this
 

1761
00:34:45,560 --> 00:34:48,909
from the API Beyond just 2.0 flash this
uh we this this is um you can also use

1762
00:34:48,909 --> 00:34:48,919
uh we this this is um you can also use
 

1763
00:34:48,919 --> 00:34:52,069
uh we this this is um you can also use
your own tuned models and uh they

1764
00:34:52,069 --> 00:34:52,079
your own tuned models and uh they
 

1765
00:34:52,079 --> 00:34:54,829
your own tuned models and uh they
provide them as um like specify the

1766
00:34:54,829 --> 00:34:54,839
provide them as um like specify the
 

1767
00:34:54,839 --> 00:34:58,310
provide them as um like specify the
model for your um API call

1768
00:34:58,310 --> 00:34:58,320
model for your um API call
 

1769
00:34:58,320 --> 00:35:01,510
model for your um API call
and the the next part of the Cod lab

1770
00:35:01,510 --> 00:35:01,520
and the the next part of the Cod lab
 

1771
00:35:01,520 --> 00:35:03,710
and the the next part of the Cod lab
which you'll be learning from here is um

1772
00:35:03,710 --> 00:35:03,720
which you'll be learning from here is um
 

1773
00:35:03,720 --> 00:35:06,349
which you'll be learning from here is um
the how to like uh modify the various

1774
00:35:06,349 --> 00:35:06,359
the how to like uh modify the various
 

1775
00:35:06,359 --> 00:35:08,270
the how to like uh modify the various
generation parameters and you'll be

1776
00:35:08,270 --> 00:35:08,280
generation parameters and you'll be
 

1777
00:35:08,280 --> 00:35:10,270
generation parameters and you'll be
playing around with a lot some of them

1778
00:35:10,270 --> 00:35:10,280
playing around with a lot some of them
 

1779
00:35:10,280 --> 00:35:12,390
playing around with a lot some of them
for instance output length this is

1780
00:35:12,390 --> 00:35:12,400
for instance output length this is
 

1781
00:35:12,400 --> 00:35:14,990
for instance output length this is
basically um specifi telling the model

1782
00:35:14,990 --> 00:35:15,000
basically um specifi telling the model
 

1783
00:35:15,000 --> 00:35:17,550
basically um specifi telling the model
to um like limit the amount of tokens

1784
00:35:17,550 --> 00:35:17,560
to um like limit the amount of tokens
 

1785
00:35:17,560 --> 00:35:20,270
to um like limit the amount of tokens
it's generating it's a hard limit keep

1786
00:35:20,270 --> 00:35:20,280
it's generating it's a hard limit keep
 

1787
00:35:20,280 --> 00:35:22,550
it's generating it's a hard limit keep
in mind so it does not automatically

1788
00:35:22,550 --> 00:35:22,560
in mind so it does not automatically
 

1789
00:35:22,560 --> 00:35:25,310
in mind so it does not automatically
tell the model to automatically generate

1790
00:35:25,310 --> 00:35:25,320
tell the model to automatically generate
 

1791
00:35:25,320 --> 00:35:28,109
tell the model to automatically generate
a smaller like a concise response it's

1792
00:35:28,109 --> 00:35:28,119
a smaller like a concise response it's
 

1793
00:35:28,119 --> 00:35:30,310
a smaller like a concise response it's
essentially just truncates the response

1794
00:35:30,310 --> 00:35:30,320
essentially just truncates the response
 

1795
00:35:30,320 --> 00:35:33,310
essentially just truncates the response
so if you are going to ask the model to

1796
00:35:33,310 --> 00:35:33,320
so if you are going to ask the model to
 

1797
00:35:33,320 --> 00:35:35,310
so if you are going to ask the model to
make like a thousand-word essay and give

1798
00:35:35,310 --> 00:35:35,320
make like a thousand-word essay and give
 

1799
00:35:35,320 --> 00:35:38,829
make like a thousand-word essay and give
it a very short like response like uh

1800
00:35:38,829 --> 00:35:38,839
it a very short like response like uh
 

1801
00:35:38,839 --> 00:35:40,670
it a very short like response like uh
Max response length it's going to just

1802
00:35:40,670 --> 00:35:40,680
Max response length it's going to just
 

1803
00:35:40,680 --> 00:35:43,310
Max response length it's going to just
cut off the essay abruptly so keep that

1804
00:35:43,310 --> 00:35:43,320
cut off the essay abruptly so keep that
 

1805
00:35:43,320 --> 00:35:45,230
cut off the essay abruptly so keep that
in mind uh but it can help you control

1806
00:35:45,230 --> 00:35:45,240
in mind uh but it can help you control
 

1807
00:35:45,240 --> 00:35:47,349
in mind uh but it can help you control
costs then we looked at something we

1808
00:35:47,349 --> 00:35:47,359
costs then we looked at something we
 

1809
00:35:47,359 --> 00:35:48,910
costs then we looked at something we
have talked about earlier what Kiran

1810
00:35:48,910 --> 00:35:48,920
have talked about earlier what Kiran
 

1811
00:35:48,920 --> 00:35:50,910
have talked about earlier what Kiran
mentioned earlier temperature this

1812
00:35:50,910 --> 00:35:50,920
mentioned earlier temperature this
 

1813
00:35:50,920 --> 00:35:52,470
mentioned earlier temperature this
basically controls the degree of

1814
00:35:52,470 --> 00:35:52,480
basically controls the degree of
 

1815
00:35:52,480 --> 00:35:55,309
basically controls the degree of
Randomness in token selection and long

1816
00:35:55,309 --> 00:35:55,319
Randomness in token selection and long
 

1817
00:35:55,319 --> 00:35:57,349
Randomness in token selection and long
story short um a higher temperature

1818
00:35:57,349 --> 00:35:57,359
story short um a higher temperature
 

1819
00:35:57,359 --> 00:35:59,230
story short um a higher temperature
leads Le to more diverse outputs while a

1820
00:35:59,230 --> 00:35:59,240
leads Le to more diverse outputs while a
 

1821
00:35:59,240 --> 00:36:00,470
leads Le to more diverse outputs while a
lower temperature leads to more

1822
00:36:00,470 --> 00:36:00,480
lower temperature leads to more
 

1823
00:36:00,480 --> 00:36:02,790
lower temperature leads to more
predictable deterministic outputs we

1824
00:36:02,790 --> 00:36:02,800
predictable deterministic outputs we
 

1825
00:36:02,800 --> 00:36:05,030
predictable deterministic outputs we
looked at an example of how when asking

1826
00:36:05,030 --> 00:36:05,040
looked at an example of how when asking
 

1827
00:36:05,040 --> 00:36:07,750
looked at an example of how when asking
the model to pick a color a random color

1828
00:36:07,750 --> 00:36:07,760
the model to pick a color a random color
 

1829
00:36:07,760 --> 00:36:10,790
the model to pick a color a random color
it can pick different colors um across

1830
00:36:10,790 --> 00:36:10,800
it can pick different colors um across
 

1831
00:36:10,800 --> 00:36:12,589
it can pick different colors um across
different iterations uh when their

1832
00:36:12,589 --> 00:36:12,599
different iterations uh when their
 

1833
00:36:12,599 --> 00:36:14,510
different iterations uh when their
temperature is higher while for a lower

1834
00:36:14,510 --> 00:36:14,520
temperature is higher while for a lower
 

1835
00:36:14,520 --> 00:36:16,670
temperature is higher while for a lower
temperature it tends to center around

1836
00:36:16,670 --> 00:36:16,680
temperature it tends to center around
 

1837
00:36:16,680 --> 00:36:17,910
temperature it tends to center around
one single

1838
00:36:17,910 --> 00:36:17,920
one single
 

1839
00:36:17,920 --> 00:36:20,150
one single
color then we also looked at things like

1840
00:36:20,150 --> 00:36:20,160
color then we also looked at things like
 

1841
00:36:20,160 --> 00:36:21,910
color then we also looked at things like
top how we can play around with knobs

1842
00:36:21,910 --> 00:36:21,920
top how we can play around with knobs
 

1843
00:36:21,920 --> 00:36:24,309
top how we can play around with knobs
like top p and top K which also kind of

1844
00:36:24,309 --> 00:36:24,319
like top p and top K which also kind of
 

1845
00:36:24,319 --> 00:36:26,670
like top p and top K which also kind of
constrain the tokens that the model

1846
00:36:26,670 --> 00:36:26,680
constrain the tokens that the model
 

1847
00:36:26,680 --> 00:36:28,950
constrain the tokens that the model
selects at the coding time or generation

1848
00:36:28,950 --> 00:36:28,960
selects at the coding time or generation
 

1849
00:36:28,960 --> 00:36:32,470
selects at the coding time or generation
time this can help you uh with your um

1850
00:36:32,470 --> 00:36:32,480
time this can help you uh with your um
 

1851
00:36:32,480 --> 00:36:33,990
time this can help you uh with your um
uh with selecting the amount of

1852
00:36:33,990 --> 00:36:34,000
uh with selecting the amount of
 

1853
00:36:34,000 --> 00:36:36,710
uh with selecting the amount of
Randomness and determinism in your

1854
00:36:36,710 --> 00:36:36,720
Randomness and determinism in your
 

1855
00:36:36,720 --> 00:36:39,349
Randomness and determinism in your
applications and then towards the last

1856
00:36:39,349 --> 00:36:39,359
applications and then towards the last
 

1857
00:36:39,359 --> 00:36:41,270
applications and then towards the last
section of the first C laab we looked at

1858
00:36:41,270 --> 00:36:41,280
section of the first C laab we looked at
 

1859
00:36:41,280 --> 00:36:43,430
section of the first C laab we looked at
various uh prompting techniques that we

1860
00:36:43,430 --> 00:36:43,440
various uh prompting techniques that we
 

1861
00:36:43,440 --> 00:36:45,430
various uh prompting techniques that we
have discussed earlier zero shot where

1862
00:36:45,430 --> 00:36:45,440
have discussed earlier zero shot where
 

1863
00:36:45,440 --> 00:36:47,349
have discussed earlier zero shot where
we give it a prompt and give it a task

1864
00:36:47,349 --> 00:36:47,359
we give it a prompt and give it a task
 

1865
00:36:47,359 --> 00:36:50,950
we give it a prompt and give it a task
and ask a response all the way to um

1866
00:36:50,950 --> 00:36:50,960
and ask a response all the way to um
 

1867
00:36:50,960 --> 00:36:54,390
and ask a response all the way to um
using like constraining the response so

1868
00:36:54,390 --> 00:36:54,400
using like constraining the response so
 

1869
00:36:54,400 --> 00:36:56,630
using like constraining the response so
instead of a free text field where we

1870
00:36:56,630 --> 00:36:56,640
instead of a free text field where we
 

1871
00:36:56,640 --> 00:36:59,750
instead of a free text field where we
just ask the model to generate some uh

1872
00:36:59,750 --> 00:36:59,760
just ask the model to generate some uh
 

1873
00:36:59,760 --> 00:37:02,030
just ask the model to generate some uh
text uh without the constraints we can

1874
00:37:02,030 --> 00:37:02,040
text uh without the constraints we can
 

1875
00:37:02,040 --> 00:37:03,750
text uh without the constraints we can
also kind of limit the responses that

1876
00:37:03,750 --> 00:37:03,760
also kind of limit the responses that
 

1877
00:37:03,760 --> 00:37:06,349
also kind of limit the responses that
the model can provide us the enumeration

1878
00:37:06,349 --> 00:37:06,359
the model can provide us the enumeration
 

1879
00:37:06,359 --> 00:37:09,390
the model can provide us the enumeration
enumeration set and we looked at also

1880
00:37:09,390 --> 00:37:09,400
enumeration set and we looked at also
 

1881
00:37:09,400 --> 00:37:11,349
enumeration set and we looked at also
other techniques like one shot and few

1882
00:37:11,349 --> 00:37:11,359
other techniques like one shot and few
 

1883
00:37:11,359 --> 00:37:12,950
other techniques like one shot and few
shot where we provide some input and

1884
00:37:12,950 --> 00:37:12,960
shot where we provide some input and
 

1885
00:37:12,960 --> 00:37:15,069
shot where we provide some input and
output examples to help it perform

1886
00:37:15,069 --> 00:37:15,079
output examples to help it perform
 

1887
00:37:15,079 --> 00:37:17,990
output examples to help it perform
better on certain tasks and um things

1888
00:37:17,990 --> 00:37:18,000
better on certain tasks and um things
 

1889
00:37:18,000 --> 00:37:20,710
better on certain tasks and um things
like Json mode uh where we can structure

1890
00:37:20,710 --> 00:37:20,720
like Json mode uh where we can structure
 

1891
00:37:20,720 --> 00:37:23,430
like Json mode uh where we can structure
give it a certain the provide the model

1892
00:37:23,430 --> 00:37:23,440
give it a certain the provide the model
 

1893
00:37:23,440 --> 00:37:25,990
give it a certain the provide the model
to a certain structure for its output it

1894
00:37:25,990 --> 00:37:26,000
to a certain structure for its output it
 

1895
00:37:26,000 --> 00:37:28,710
to a certain structure for its output it
can help it um uh structure its output

1896
00:37:28,710 --> 00:37:28,720
can help it um uh structure its output
 

1897
00:37:28,720 --> 00:37:30,790
can help it um uh structure its output
in a way which can easily be parsed by

1898
00:37:30,790 --> 00:37:30,800
in a way which can easily be parsed by
 

1899
00:37:30,800 --> 00:37:33,270
in a way which can easily be parsed by
the downstream models and then Chain of

1900
00:37:33,270 --> 00:37:33,280
the downstream models and then Chain of
 

1901
00:37:33,280 --> 00:37:35,349
the downstream models and then Chain of
Thought reasoning um and other prompting

1902
00:37:35,349 --> 00:37:35,359
Thought reasoning um and other prompting
 

1903
00:37:35,359 --> 00:37:36,829
Thought reasoning um and other prompting
techniques as

1904
00:37:36,829 --> 00:37:36,839
techniques as
 

1905
00:37:36,839 --> 00:37:40,190
techniques as
well and react as well so that was the

1906
00:37:40,190 --> 00:37:40,200
well and react as well so that was the
 

1907
00:37:40,200 --> 00:37:42,750
well and react as well so that was the
first collab and moving on to the second

1908
00:37:42,750 --> 00:37:42,760
first collab and moving on to the second
 

1909
00:37:42,760 --> 00:37:45,589
first collab and moving on to the second
collab which is also um we doing it for

1910
00:37:45,589 --> 00:37:45,599
collab which is also um we doing it for
 

1911
00:37:45,599 --> 00:37:48,430
collab which is also um we doing it for
first time this year um is the one on

1912
00:37:48,430 --> 00:37:48,440
first time this year um is the one on
 

1913
00:37:48,440 --> 00:37:51,069
first time this year um is the one on
evaluation and structured output so it's

1914
00:37:51,069 --> 00:37:51,079
evaluation and structured output so it's
 

1915
00:37:51,079 --> 00:37:53,430
evaluation and structured output so it's
as we learned in the podcast in our

1916
00:37:53,430 --> 00:37:53,440
as we learned in the podcast in our
 

1917
00:37:53,440 --> 00:37:55,430
as we learned in the podcast in our
discussion earlier evaluating the respon

1918
00:37:55,430 --> 00:37:55,440
discussion earlier evaluating the respon
 

1919
00:37:55,440 --> 00:37:57,990
discussion earlier evaluating the respon
uh response is pretty crucial especially

1920
00:37:57,990 --> 00:37:58,000
uh response is pretty crucial especially
 

1921
00:37:58,000 --> 00:38:02,069
uh response is pretty crucial especially
for your tasks and um we look in uh in

1922
00:38:02,069 --> 00:38:02,079
for your tasks and um we look in uh in
 

1923
00:38:02,079 --> 00:38:05,230
for your tasks and um we look in uh in
this code lab we we look at how you can

1924
00:38:05,230 --> 00:38:05,240
this code lab we we look at how you can
 

1925
00:38:05,240 --> 00:38:08,950
this code lab we we look at how you can
um basically um set up um set up some

1926
00:38:08,950 --> 00:38:08,960
um basically um set up um set up some
 

1927
00:38:08,960 --> 00:38:12,030
um basically um set up um set up some
context as like a PDF um for one of our

1928
00:38:12,030 --> 00:38:12,040
context as like a PDF um for one of our
 

1929
00:38:12,040 --> 00:38:16,710
context as like a PDF um for one of our
technical reports and utilize um uh an

1930
00:38:16,710 --> 00:38:16,720
technical reports and utilize um uh an
 

1931
00:38:16,720 --> 00:38:20,510
technical reports and utilize um uh an
llm as an evaluator where um to evaluate

1932
00:38:20,510 --> 00:38:20,520
llm as an evaluator where um to evaluate
 

1933
00:38:20,520 --> 00:38:23,589
llm as an evaluator where um to evaluate
this for instance um we provide in the

1934
00:38:23,589 --> 00:38:23,599
this for instance um we provide in the
 

1935
00:38:23,599 --> 00:38:26,109
this for instance um we provide in the
first part we see how we can give it a

1936
00:38:26,109 --> 00:38:26,119
first part we see how we can give it a
 

1937
00:38:26,119 --> 00:38:28,430
first part we see how we can give it a
point-wise evaluation prompt very we

1938
00:38:28,430 --> 00:38:28,440
point-wise evaluation prompt very we
 

1939
00:38:28,440 --> 00:38:32,190
point-wise evaluation prompt very we
carefully craft The Prompt for an llm to

1940
00:38:32,190 --> 00:38:32,200
carefully craft The Prompt for an llm to
 

1941
00:38:32,200 --> 00:38:35,470
carefully craft The Prompt for an llm to
to evaluate the response of another lm's

1942
00:38:35,470 --> 00:38:35,480
to evaluate the response of another lm's
 

1943
00:38:35,480 --> 00:38:38,109
to evaluate the response of another lm's
response with respect to what the prompt

1944
00:38:38,109 --> 00:38:38,119
response with respect to what the prompt
 

1945
00:38:38,119 --> 00:38:40,910
response with respect to what the prompt
that was provided and it evaluates it in

1946
00:38:40,910 --> 00:38:40,920
that was provided and it evaluates it in
 

1947
00:38:40,920 --> 00:38:42,950
that was provided and it evaluates it in
a with a score of one to5 with respect

1948
00:38:42,950 --> 00:38:42,960
a with a score of one to5 with respect
 

1949
00:38:42,960 --> 00:38:45,230
a with a score of one to5 with respect
to a carefully crafted rubrics which we

1950
00:38:45,230 --> 00:38:45,240
to a carefully crafted rubrics which we
 

1951
00:38:45,240 --> 00:38:48,589
to a carefully crafted rubrics which we
have defined here and uh we see how this

1952
00:38:48,589 --> 00:38:48,599
have defined here and uh we see how this
 

1953
00:38:48,599 --> 00:38:51,230
have defined here and uh we see how this
can be used and it gives some reasoning

1954
00:38:51,230 --> 00:38:51,240
can be used and it gives some reasoning
 

1955
00:38:51,240 --> 00:38:53,870
can be used and it gives some reasoning
as well as a score and we see here that

1956
00:38:53,870 --> 00:38:53,880
as well as a score and we see here that
 

1957
00:38:53,880 --> 00:38:56,190
as well as a score and we see here that
uh we get a score as well as the

1958
00:38:56,190 --> 00:38:56,200
uh we get a score as well as the
 

1959
00:38:56,200 --> 00:38:59,309
uh we get a score as well as the
reasoning behind it as well as well then

1960
00:38:59,309 --> 00:38:59,319
reasoning behind it as well as well then
 

1961
00:38:59,319 --> 00:39:02,829
reasoning behind it as well as well then
we play around to see uh like how you

1962
00:39:02,829 --> 00:39:02,839
we play around to see uh like how you
 

1963
00:39:02,839 --> 00:39:05,390
we play around to see uh like how you
can improve or reduce the score and what

1964
00:39:05,390 --> 00:39:05,400
can improve or reduce the score and what
 

1965
00:39:05,400 --> 00:39:07,589
can improve or reduce the score and what
makes a good prompt so explaining for

1966
00:39:07,589 --> 00:39:07,599
makes a good prompt so explaining for
 

1967
00:39:07,599 --> 00:39:09,750
makes a good prompt so explaining for
example explaining our technical reports

1968
00:39:09,750 --> 00:39:09,760
example explaining our technical reports
 

1969
00:39:09,760 --> 00:39:12,670
example explaining our technical reports
to a 5-year-old is obviously going to

1970
00:39:12,670 --> 00:39:12,680
to a 5-year-old is obviously going to
 

1971
00:39:12,680 --> 00:39:16,109
to a 5-year-old is obviously going to
lead to a slightly lower uh response by

1972
00:39:16,109 --> 00:39:16,119
lead to a slightly lower uh response by
 

1973
00:39:16,119 --> 00:39:17,670
lead to a slightly lower uh response by
the evaluator because we often have the

1974
00:39:17,670 --> 00:39:17,680
the evaluator because we often have the
 

1975
00:39:17,680 --> 00:39:19,430
the evaluator because we often have the
word puppy which is not present in our

1976
00:39:19,430 --> 00:39:19,440
word puppy which is not present in our
 

1977
00:39:19,440 --> 00:39:22,270
word puppy which is not present in our
technical report so good way toig to see

1978
00:39:22,270 --> 00:39:22,280
technical report so good way toig to see
 

1979
00:39:22,280 --> 00:39:23,990
technical report so good way toig to see
the difference and we get a very

1980
00:39:23,990 --> 00:39:24,000
the difference and we get a very
 

1981
00:39:24,000 --> 00:39:26,750
the difference and we get a very
expected score of one and in the last

1982
00:39:26,750 --> 00:39:26,760
expected score of one and in the last
 

1983
00:39:26,760 --> 00:39:28,630
expected score of one and in the last
part of our col lab um we discuss

1984
00:39:28,630 --> 00:39:28,640
part of our col lab um we discuss
 

1985
00:39:28,640 --> 00:39:31,510
part of our col lab um we discuss
various techniques for using evaluation

1986
00:39:31,510 --> 00:39:31,520
various techniques for using evaluation
 

1987
00:39:31,520 --> 00:39:34,030
various techniques for using evaluation
in practice for instance we discuss

1988
00:39:34,030 --> 00:39:34,040
in practice for instance we discuss
 

1989
00:39:34,040 --> 00:39:36,910
in practice for instance we discuss
pointwise evaluation um where for

1990
00:39:36,910 --> 00:39:36,920
pointwise evaluation um where for
 

1991
00:39:36,920 --> 00:39:39,309
pointwise evaluation um where for
summarization where we kind of take a

1992
00:39:39,309 --> 00:39:39,319
summarization where we kind of take a
 

1993
00:39:39,319 --> 00:39:41,309
summarization where we kind of take a
response take a prompt which generated a

1994
00:39:41,309 --> 00:39:41,319
response take a prompt which generated a
 

1995
00:39:41,319 --> 00:39:44,390
response take a prompt which generated a
response and have an llm evaluation

1996
00:39:44,390 --> 00:39:44,400
response and have an llm evaluation
 

1997
00:39:44,400 --> 00:39:47,230
response and have an llm evaluation
evaluator determine on a score on a

1998
00:39:47,230 --> 00:39:47,240
evaluator determine on a score on a
 

1999
00:39:47,240 --> 00:39:49,829
evaluator determine on a score on a
scale of one to five what score does it

2000
00:39:49,829 --> 00:39:49,839
scale of one to five what score does it
 

2001
00:39:49,839 --> 00:39:52,470
scale of one to five what score does it
get however it could be because the the

2002
00:39:52,470 --> 00:39:52,480
get however it could be because the the
 

2003
00:39:52,480 --> 00:39:54,750
get however it could be because the the
scale of 1 to five is not very fine

2004
00:39:54,750 --> 00:39:54,760
scale of 1 to five is not very fine
 

2005
00:39:54,760 --> 00:39:56,990
scale of 1 to five is not very fine
grained that uh there's a tie between

2006
00:39:56,990 --> 00:39:57,000
grained that uh there's a tie between
 

2007
00:39:57,000 --> 00:39:59,309
grained that uh there's a tie between
some responses and this is where

2008
00:39:59,309 --> 00:39:59,319
some responses and this is where
 

2009
00:39:59,319 --> 00:40:02,109
some responses and this is where
point-wise evaluation is uh sometimes

2010
00:40:02,109 --> 00:40:02,119
point-wise evaluation is uh sometimes
 

2011
00:40:02,119 --> 00:40:04,309
point-wise evaluation is uh sometimes
does not suffice and it's often useful

2012
00:40:04,309 --> 00:40:04,319
does not suffice and it's often useful
 

2013
00:40:04,319 --> 00:40:05,950
does not suffice and it's often useful
to have

2014
00:40:05,950 --> 00:40:05,960
to have
 

2015
00:40:05,960 --> 00:40:09,630
to have
um um um pairwise evaluation uh as well

2016
00:40:09,630 --> 00:40:09,640
um um um pairwise evaluation uh as well
 

2017
00:40:09,640 --> 00:40:11,750
um um um pairwise evaluation uh as well
where we uh where we give two responses

2018
00:40:11,750 --> 00:40:11,760
where we uh where we give two responses
 

2019
00:40:11,760 --> 00:40:14,349
where we uh where we give two responses
to a particular prompt and have the llm

2020
00:40:14,349 --> 00:40:14,359
to a particular prompt and have the llm
 

2021
00:40:14,359 --> 00:40:16,950
to a particular prompt and have the llm
as a judge select that response this is

2022
00:40:16,950 --> 00:40:16,960
as a judge select that response this is
 

2023
00:40:16,960 --> 00:40:20,910
as a judge select that response this is
what we also see and then uh uh and then

2024
00:40:20,910 --> 00:40:20,920
what we also see and then uh uh and then
 

2025
00:40:20,920 --> 00:40:24,349
what we also see and then uh uh and then
uh we finally end the co uh the collab

2026
00:40:24,349 --> 00:40:24,359
uh we finally end the co uh the collab
 

2027
00:40:24,359 --> 00:40:28,030
uh we finally end the co uh the collab
with um seeing how we can uh it's often

2028
00:40:28,030 --> 00:40:28,040
with um seeing how we can uh it's often
 

2029
00:40:28,040 --> 00:40:30,390
with um seeing how we can uh it's often
um useful as enen mentioned earlier to

2030
00:40:30,390 --> 00:40:30,400
um useful as enen mentioned earlier to
 

2031
00:40:30,400 --> 00:40:33,430
um useful as enen mentioned earlier to
generate multiple responses by the llm

2032
00:40:33,430 --> 00:40:33,440
generate multiple responses by the llm
 

2033
00:40:33,440 --> 00:40:35,510
generate multiple responses by the llm
athor judge instead of just selecting

2034
00:40:35,510 --> 00:40:35,520
athor judge instead of just selecting
 

2035
00:40:35,520 --> 00:40:37,950
athor judge instead of just selecting
the first response to ensure that the

2036
00:40:37,950 --> 00:40:37,960
the first response to ensure that the
 

2037
00:40:37,960 --> 00:40:40,710
the first response to ensure that the
score that you get is not just due to

2038
00:40:40,710 --> 00:40:40,720
score that you get is not just due to
 

2039
00:40:40,720 --> 00:40:42,270
score that you get is not just due to
noise and uh

2040
00:40:42,270 --> 00:40:42,280
noise and uh
 

2041
00:40:42,280 --> 00:40:45,349
noise and uh
stochasticity so yeah so it's a lot to

2042
00:40:45,349 --> 00:40:45,359
stochasticity so yeah so it's a lot to
 

2043
00:40:45,359 --> 00:40:49,510
stochasticity so yeah so it's a lot to
cover but um off to you page for the pop

2044
00:40:49,510 --> 00:40:49,520
cover but um off to you page for the pop
 

2045
00:40:49,520 --> 00:40:52,230
cover but um off to you page for the pop
quiz excellent thank you so much Anand

2046
00:40:52,230 --> 00:40:52,240
quiz excellent thank you so much Anand
 

2047
00:40:52,240 --> 00:40:55,430
quiz excellent thank you so much Anand
and to Mark McDonald uh who uh helped

2048
00:40:55,430 --> 00:40:55,440
and to Mark McDonald uh who uh helped
 

2049
00:40:55,440 --> 00:40:57,910
and to Mark McDonald uh who uh helped
create many of these code labs

2050
00:40:57,910 --> 00:40:57,920
create many of these code labs
 

2051
00:40:57,920 --> 00:40:59,630
create many of these code labs
um youall have done a great job of

2052
00:40:59,630 --> 00:40:59,640
um youall have done a great job of
 

2053
00:40:59,640 --> 00:41:01,589
um youall have done a great job of
making all of the concepts that are

2054
00:41:01,589 --> 00:41:01,599
making all of the concepts that are
 

2055
00:41:01,599 --> 00:41:03,109
making all of the concepts that are
being discussed in the white papers and

2056
00:41:03,109 --> 00:41:03,119
being discussed in the white papers and
 

2057
00:41:03,119 --> 00:41:06,190
being discussed in the white papers and
in the Q&A sections real uh and I'm

2058
00:41:06,190 --> 00:41:06,200
in the Q&A sections real uh and I'm
 

2059
00:41:06,200 --> 00:41:07,910
in the Q&A sections real uh and I'm
really looking forward to seeing what

2060
00:41:07,910 --> 00:41:07,920
really looking forward to seeing what
 

2061
00:41:07,920 --> 00:41:09,990
really looking forward to seeing what
people build and uh and their initial

2062
00:41:09,990 --> 00:41:10,000
people build and uh and their initial
 

2063
00:41:10,000 --> 00:41:12,309
people build and uh and their initial
impressions of the code Labs so this is

2064
00:41:12,309 --> 00:41:12,319
impressions of the code Labs so this is
 

2065
00:41:12,319 --> 00:41:15,510
impressions of the code Labs so this is
very very cool thank you um with that

2066
00:41:15,510 --> 00:41:15,520
very very cool thank you um with that
 

2067
00:41:15,520 --> 00:41:18,829
very very cool thank you um with that
I'm going to go ahead and move into the

2068
00:41:18,829 --> 00:41:18,839
I'm going to go ahead and move into the
 

2069
00:41:18,839 --> 00:41:21,630
I'm going to go ahead and move into the
uh the pop quiz um so I'm going to bring

2070
00:41:21,630 --> 00:41:21,640
uh the pop quiz um so I'm going to bring
 

2071
00:41:21,640 --> 00:41:23,790
uh the pop quiz um so I'm going to bring
up onto the screen some questions that

2072
00:41:23,790 --> 00:41:23,800
up onto the screen some questions that
 

2073
00:41:23,800 --> 00:41:25,589
up onto the screen some questions that
hopefully folks will be able to answer

2074
00:41:25,589 --> 00:41:25,599
hopefully folks will be able to answer
 

2075
00:41:25,599 --> 00:41:27,589
hopefully folks will be able to answer
now that you've had this uh now that

2076
00:41:27,589 --> 00:41:27,599
now that you've had this uh now that
 

2077
00:41:27,599 --> 00:41:29,790
now that you've had this uh now that
you've had this 24-hour crash course in

2078
00:41:29,790 --> 00:41:29,800
you've had this 24-hour crash course in
 

2079
00:41:29,800 --> 00:41:31,270
you've had this 24-hour crash course in
prompt engineering and foundational

2080
00:41:31,270 --> 00:41:31,280
prompt engineering and foundational
 

2081
00:41:31,280 --> 00:41:33,910
prompt engineering and foundational
models um to start with our first

2082
00:41:33,910 --> 00:41:33,920
models um to start with our first
 

2083
00:41:33,920 --> 00:41:36,349
models um to start with our first
question which Gemini configuration

2084
00:41:36,349 --> 00:41:36,359
question which Gemini configuration
 

2085
00:41:36,359 --> 00:41:38,069
question which Gemini configuration
setting controls the degree of

2086
00:41:38,069 --> 00:41:38,079
setting controls the degree of
 

2087
00:41:38,079 --> 00:41:40,349
setting controls the degree of
Randomness in the selection of the next

2088
00:41:40,349 --> 00:41:40,359
Randomness in the selection of the next
 

2089
00:41:40,359 --> 00:41:44,190
Randomness in the selection of the next
predicted token is it temperature is it

2090
00:41:44,190 --> 00:41:44,200
predicted token is it temperature is it
 

2091
00:41:44,200 --> 00:41:48,030
predicted token is it temperature is it
top K is it top P or is it the output

2092
00:41:48,030 --> 00:41:48,040
top K is it top P or is it the output
 

2093
00:41:48,040 --> 00:41:51,630
top K is it top P or is it the output
token count um remember back to uh to

2094
00:41:51,630 --> 00:41:51,640
token count um remember back to uh to
 

2095
00:41:51,640 --> 00:41:52,990
token count um remember back to uh to
the white paper as well as some of the

2096
00:41:52,990 --> 00:41:53,000
the white paper as well as some of the
 

2097
00:41:53,000 --> 00:41:54,990
the white paper as well as some of the
questions that Kieran was answering

2098
00:41:54,990 --> 00:41:55,000
questions that Kieran was answering
 

2099
00:41:55,000 --> 00:41:58,030
questions that Kieran was answering
today which Gemini configuration setting

2100
00:41:58,030 --> 00:41:58,040
today which Gemini configuration setting
 

2101
00:41:58,040 --> 00:42:00,270
today which Gemini configuration setting
controls the degree of Randomness and

2102
00:42:00,270 --> 00:42:00,280
controls the degree of Randomness and
 

2103
00:42:00,280 --> 00:42:03,630
controls the degree of Randomness and
the selection of the next predicted

2104
00:42:03,630 --> 00:42:03,640
the selection of the next predicted
 

2105
00:42:03,640 --> 00:42:05,990
the selection of the next predicted
token so hopefully everybody's had a

2106
00:42:05,990 --> 00:42:06,000
token so hopefully everybody's had a
 

2107
00:42:06,000 --> 00:42:08,589
token so hopefully everybody's had a
chance to jot this down on either a a

2108
00:42:08,589 --> 00:42:08,599
chance to jot this down on either a a
 

2109
00:42:08,599 --> 00:42:10,829
chance to jot this down on either a a
pen uh or a piece of paper with a pen or

2110
00:42:10,829 --> 00:42:10,839
pen uh or a piece of paper with a pen or
 

2111
00:42:10,839 --> 00:42:13,309
pen uh or a piece of paper with a pen or
a pencil um or if you're just kind of

2112
00:42:13,309 --> 00:42:13,319
a pencil um or if you're just kind of
 

2113
00:42:13,319 --> 00:42:16,190
a pencil um or if you're just kind of
taking notes in a dock um but the answer

2114
00:42:16,190 --> 00:42:16,200
taking notes in a dock um but the answer
 

2115
00:42:16,200 --> 00:42:18,990
taking notes in a dock um but the answer
is a temperature um so hopefully

2116
00:42:18,990 --> 00:42:19,000
is a temperature um so hopefully
 

2117
00:42:19,000 --> 00:42:21,990
is a temperature um so hopefully
hopefully everyone got this right um

2118
00:42:21,990 --> 00:42:22,000
hopefully everyone got this right um
 

2119
00:42:22,000 --> 00:42:24,710
hopefully everyone got this right um
question two which of the following is

2120
00:42:24,710 --> 00:42:24,720
question two which of the following is
 

2121
00:42:24,720 --> 00:42:27,670
question two which of the following is
not a technique used to accelerate

2122
00:42:27,670 --> 00:42:27,680
not a technique used to accelerate
 

2123
00:42:27,680 --> 00:42:31,150
not a technique used to accelerate
in large language models um is it

2124
00:42:31,150 --> 00:42:31,160
in large language models um is it
 

2125
00:42:31,160 --> 00:42:34,270
in large language models um is it
quantization is it distillation is it

2126
00:42:34,270 --> 00:42:34,280
quantization is it distillation is it
 

2127
00:42:34,280 --> 00:42:37,349
quantization is it distillation is it
flash attention or is it fine-tuning um

2128
00:42:37,349 --> 00:42:37,359
flash attention or is it fine-tuning um
 

2129
00:42:37,359 --> 00:42:39,990
flash attention or is it fine-tuning um
again and you're looking for uh what is

2130
00:42:39,990 --> 00:42:40,000
again and you're looking for uh what is
 

2131
00:42:40,000 --> 00:42:42,390
again and you're looking for uh what is
not a technique to accelerate inference

2132
00:42:42,390 --> 00:42:42,400
not a technique to accelerate inference
 

2133
00:42:42,400 --> 00:42:44,950
not a technique to accelerate inference
in large language

2134
00:42:44,950 --> 00:42:44,960
in large language
 

2135
00:42:44,960 --> 00:42:52,390
in large language
models and I countdown five 4 3 2 one

2136
00:42:52,390 --> 00:42:52,400
models and I countdown five 4 3 2 one
 

2137
00:42:52,400 --> 00:42:55,069
models and I countdown five 4 3 2 one
and the correct answer is D fine-tuning

2138
00:42:55,069 --> 00:42:55,079
and the correct answer is D fine-tuning
 

2139
00:42:55,079 --> 00:42:57,470
and the correct answer is D fine-tuning
fine tuning is not a technique used to

2140
00:42:57,470 --> 00:42:57,480
fine tuning is not a technique used to
 

2141
00:42:57,480 --> 00:42:59,150
fine tuning is not a technique used to
accelerate

2142
00:42:59,150 --> 00:42:59,160
accelerate
 

2143
00:42:59,160 --> 00:43:01,589
accelerate
inference question number three which of

2144
00:43:01,589 --> 00:43:01,599
inference question number three which of
 

2145
00:43:01,599 --> 00:43:03,990
inference question number three which of
the following is a unique characteristic

2146
00:43:03,990 --> 00:43:04,000
the following is a unique characteristic
 

2147
00:43:04,000 --> 00:43:06,349
the following is a unique characteristic
of the Gemini family of large language

2148
00:43:06,349 --> 00:43:06,359
of the Gemini family of large language
 

2149
00:43:06,359 --> 00:43:09,470
of the Gemini family of large language
models is it a Gemini models were the

2150
00:43:09,470 --> 00:43:09,480
models is it a Gemini models were the
 

2151
00:43:09,480 --> 00:43:11,069
models is it a Gemini models were the
first to introduce the concept of

2152
00:43:11,069 --> 00:43:11,079
first to introduce the concept of
 

2153
00:43:11,079 --> 00:43:13,990
first to introduce the concept of
unsupervised pre-training B Gemini

2154
00:43:13,990 --> 00:43:14,000
unsupervised pre-training B Gemini
 

2155
00:43:14,000 --> 00:43:17,349
unsupervised pre-training B Gemini
models can support multimodal inputs C

2156
00:43:17,349 --> 00:43:17,359
models can support multimodal inputs C
 

2157
00:43:17,359 --> 00:43:20,390
models can support multimodal inputs C
Gemini models are decoder only or D

2158
00:43:20,390 --> 00:43:20,400
Gemini models are decoder only or D
 

2159
00:43:20,400 --> 00:43:22,589
Gemini models are decoder only or D
Gemini models can support a context

2160
00:43:22,589 --> 00:43:22,599
Gemini models can support a context
 

2161
00:43:22,599 --> 00:43:24,750
Gemini models can support a context
window of up to 2 million tokens what is

2162
00:43:24,750 --> 00:43:24,760
window of up to 2 million tokens what is
 

2163
00:43:24,760 --> 00:43:27,109
window of up to 2 million tokens what is
a unique characteristic of the Gemini

2164
00:43:27,109 --> 00:43:27,119
a unique characteristic of the Gemini
 

2165
00:43:27,119 --> 00:43:29,950
a unique characteristic of the Gemini
family of large language models and I

2166
00:43:29,950 --> 00:43:29,960
family of large language models and I
 

2167
00:43:29,960 --> 00:43:32,510
family of large language models and I
countdown five

2168
00:43:32,510 --> 00:43:32,520
countdown five
 

2169
00:43:32,520 --> 00:43:39,790
countdown five
4 3 2 one the correct answer is D Gemini

2170
00:43:39,790 --> 00:43:39,800
4 3 2 one the correct answer is D Gemini
 

2171
00:43:39,800 --> 00:43:41,670
4 3 2 one the correct answer is D Gemini
models can support a context window of

2172
00:43:41,670 --> 00:43:41,680
models can support a context window of
 

2173
00:43:41,680 --> 00:43:43,829
models can support a context window of
up to 2 million tokens and hopefully if

2174
00:43:43,829 --> 00:43:43,839
up to 2 million tokens and hopefully if
 

2175
00:43:43,839 --> 00:43:45,190
up to 2 million tokens and hopefully if
you've been experimenting with some of

2176
00:43:45,190 --> 00:43:45,200
you've been experimenting with some of
 

2177
00:43:45,200 --> 00:43:48,069
you've been experimenting with some of
our Pro Models in AI dodev um which is

2178
00:43:48,069 --> 00:43:48,079
our Pro Models in AI dodev um which is
 

2179
00:43:48,079 --> 00:43:50,349
our Pro Models in AI dodev um which is
Google AI Studio you should have been

2180
00:43:50,349 --> 00:43:50,359
Google AI Studio you should have been
 

2181
00:43:50,359 --> 00:43:52,870
Google AI Studio you should have been
experimenting this uh with this

2182
00:43:52,870 --> 00:43:52,880
experimenting this uh with this
 

2183
00:43:52,880 --> 00:43:55,349
experimenting this uh with this
firsthand question number four how does

2184
00:43:55,349 --> 00:43:55,359
firsthand question number four how does
 

2185
00:43:55,359 --> 00:43:57,109
firsthand question number four how does
reinforcement learning from Human

2186
00:43:57,109 --> 00:43:57,119
reinforcement learning from Human
 

2187
00:43:57,119 --> 00:44:00,230
reinforcement learning from Human
feedback rhf improve large language

2188
00:44:00,230 --> 00:44:00,240
feedback rhf improve large language
 

2189
00:44:00,240 --> 00:44:02,950
feedback rhf improve large language
models is it a by training the model on

2190
00:44:02,950 --> 00:44:02,960
models is it a by training the model on
 

2191
00:44:02,960 --> 00:44:06,589
models is it a by training the model on
a massive data set of unlabeled text is

2192
00:44:06,589 --> 00:44:06,599
a massive data set of unlabeled text is
 

2193
00:44:06,599 --> 00:44:08,790
a massive data set of unlabeled text is
it B by using a reward model to

2194
00:44:08,790 --> 00:44:08,800
it B by using a reward model to
 

2195
00:44:08,800 --> 00:44:10,470
it B by using a reward model to
incentivize the generation of human

2196
00:44:10,470 --> 00:44:10,480
incentivize the generation of human
 

2197
00:44:10,480 --> 00:44:13,829
incentivize the generation of human
preferred responses is it C by reducing

2198
00:44:13,829 --> 00:44:13,839
preferred responses is it C by reducing
 

2199
00:44:13,839 --> 00:44:15,510
preferred responses is it C by reducing
the number of parameters in the model

2200
00:44:15,510 --> 00:44:15,520
the number of parameters in the model
 

2201
00:44:15,520 --> 00:44:18,750
the number of parameters in the model
for faster inference um or is a d by

2202
00:44:18,750 --> 00:44:18,760
for faster inference um or is a d by
 

2203
00:44:18,760 --> 00:44:20,390
for faster inference um or is a d by
converting the model into a recurrent

2204
00:44:20,390 --> 00:44:20,400
converting the model into a recurrent
 

2205
00:44:20,400 --> 00:44:22,390
converting the model into a recurrent
neural network for improved performance

2206
00:44:22,390 --> 00:44:22,400
neural network for improved performance
 

2207
00:44:22,400 --> 00:44:25,190
neural network for improved performance
how does rhf improve large language

2208
00:44:25,190 --> 00:44:25,200
how does rhf improve large language
 

2209
00:44:25,200 --> 00:44:27,910
how does rhf improve large language
models

2210
00:44:27,910 --> 00:44:27,920
models
 

2211
00:44:27,920 --> 00:44:29,670
models
and hopefully everybody remembers this a

2212
00:44:29,670 --> 00:44:29,680
and hopefully everybody remembers this a
 

2213
00:44:29,680 --> 00:44:31,150
and hopefully everybody remembers this a
little bit from the from the white

2214
00:44:31,150 --> 00:44:31,160
little bit from the from the white
 

2215
00:44:31,160 --> 00:44:33,230
little bit from the from the white
papers as well as through some of the

2216
00:44:33,230 --> 00:44:33,240
papers as well as through some of the
 

2217
00:44:33,240 --> 00:44:35,430
papers as well as through some of the
questions that we were answering today

2218
00:44:35,430 --> 00:44:35,440
questions that we were answering today
 

2219
00:44:35,440 --> 00:44:42,510
questions that we were answering today
I'm going to count down five 4 3 2 1 and

2220
00:44:42,510 --> 00:44:42,520
I'm going to count down five 4 3 2 1 and
 

2221
00:44:42,520 --> 00:44:45,470
I'm going to count down five 4 3 2 1 and
the correct answer is B um rhf uses a

2222
00:44:45,470 --> 00:44:45,480
the correct answer is B um rhf uses a
 

2223
00:44:45,480 --> 00:44:47,349
the correct answer is B um rhf uses a
reward bottle to incentivize the

2224
00:44:47,349 --> 00:44:47,359
reward bottle to incentivize the
 

2225
00:44:47,359 --> 00:44:49,670
reward bottle to incentivize the
generation of human preferred

2226
00:44:49,670 --> 00:44:49,680
generation of human preferred
 

2227
00:44:49,680 --> 00:44:52,230
generation of human preferred
responses question number five which

2228
00:44:52,230 --> 00:44:52,240
responses question number five which
 

2229
00:44:52,240 --> 00:44:54,470
responses question number five which
technique enhances an lm's reasoning

2230
00:44:54,470 --> 00:44:54,480
technique enhances an lm's reasoning
 

2231
00:44:54,480 --> 00:44:56,630
technique enhances an lm's reasoning
abilities by Pro by prompting it to

2232
00:44:56,630 --> 00:44:56,640
abilities by Pro by prompting it to
 

2233
00:44:56,640 --> 00:44:58,390
abilities by Pro by prompting it to
produce intermediate reasoning steps

2234
00:44:58,390 --> 00:44:58,400
produce intermediate reasoning steps
 

2235
00:44:58,400 --> 00:45:01,990
produce intermediate reasoning steps
leading to more accurate answers is it a

2236
00:45:01,990 --> 00:45:02,000
leading to more accurate answers is it a
 

2237
00:45:02,000 --> 00:45:05,230
leading to more accurate answers is it a
zero shot prompting B step back

2238
00:45:05,230 --> 00:45:05,240
zero shot prompting B step back
 

2239
00:45:05,240 --> 00:45:08,510
zero shot prompting B step back
prompting C self-consistent prompting or

2240
00:45:08,510 --> 00:45:08,520
prompting C self-consistent prompting or
 

2241
00:45:08,520 --> 00:45:12,030
prompting C self-consistent prompting or
D Chain of Thought

2242
00:45:12,030 --> 00:45:12,040

 

2243
00:45:12,040 --> 00:45:14,150

prompting hopefully everyone is thinking

2244
00:45:14,150 --> 00:45:14,160
prompting hopefully everyone is thinking
 

2245
00:45:14,160 --> 00:45:17,030
prompting hopefully everyone is thinking
back to the white papers um and

2246
00:45:17,030 --> 00:45:17,040
back to the white papers um and
 

2247
00:45:17,040 --> 00:45:18,790
back to the white papers um and
hopefully you've also been experimenting

2248
00:45:18,790 --> 00:45:18,800
hopefully you've also been experimenting
 

2249
00:45:18,800 --> 00:45:20,349
hopefully you've also been experimenting
with this prompting technique as you

2250
00:45:20,349 --> 00:45:20,359
with this prompting technique as you
 

2251
00:45:20,359 --> 00:45:22,870
with this prompting technique as you
were uh as you were kind of playing with

2252
00:45:22,870 --> 00:45:22,880
were uh as you were kind of playing with
 

2253
00:45:22,880 --> 00:45:25,030
were uh as you were kind of playing with
the Gemini models in AI Studio as well

2254
00:45:25,030 --> 00:45:25,040
the Gemini models in AI Studio as well
 

2255
00:45:25,040 --> 00:45:29,150
the Gemini models in AI Studio as well
as in your kagle code labs

2256
00:45:29,150 --> 00:45:29,160

 

2257
00:45:29,160 --> 00:45:30,630

but the answer

2258
00:45:30,630 --> 00:45:30,640
but the answer
 

2259
00:45:30,640 --> 00:45:33,710
but the answer
is D Chain of Thought prompting Chain of

2260
00:45:33,710 --> 00:45:33,720
is D Chain of Thought prompting Chain of
 

2261
00:45:33,720 --> 00:45:35,670
is D Chain of Thought prompting Chain of
Thought prompting helps the model show

2262
00:45:35,670 --> 00:45:35,680
Thought prompting helps the model show
 

2263
00:45:35,680 --> 00:45:37,950
Thought prompting helps the model show
intermediate reasoning steps which often

2264
00:45:37,950 --> 00:45:37,960
intermediate reasoning steps which often
 

2265
00:45:37,960 --> 00:45:41,549
intermediate reasoning steps which often
leads to more accurate answers um so

2266
00:45:41,549 --> 00:45:41,559
leads to more accurate answers um so
 

2267
00:45:41,559 --> 00:45:43,349
leads to more accurate answers um so
thank you to everyone for for

2268
00:45:43,349 --> 00:45:43,359
thank you to everyone for for
 

2269
00:45:43,359 --> 00:45:45,349
thank you to everyone for for
participating today hopefully you all

2270
00:45:45,349 --> 00:45:45,359
participating today hopefully you all
 

2271
00:45:45,359 --> 00:45:48,790
participating today hopefully you all
scored 100% correct on your pop quiz uh

2272
00:45:48,790 --> 00:45:48,800
scored 100% correct on your pop quiz uh
 

2273
00:45:48,800 --> 00:45:50,470
scored 100% correct on your pop quiz uh
and if you didn't you learned something

2274
00:45:50,470 --> 00:45:50,480
and if you didn't you learned something
 

2275
00:45:50,480 --> 00:45:52,950
and if you didn't you learned something
along the way so thank you for uh thank

2276
00:45:52,950 --> 00:45:52,960
along the way so thank you for uh thank
 

2277
00:45:52,960 --> 00:45:55,990
along the way so thank you for uh thank
you for kind of testing your your new

2278
00:45:55,990 --> 00:45:56,000
you for kind of testing your your new
 

2279
00:45:56,000 --> 00:45:58,030
you for kind of testing your your new
Knowledge and Skills and

2280
00:45:58,030 --> 00:45:58,040
Knowledge and Skills and
 

2281
00:45:58,040 --> 00:46:00,790
Knowledge and Skills and
capabilities um and the the last

2282
00:46:00,790 --> 00:46:00,800
capabilities um and the the last
 

2283
00:46:00,800 --> 00:46:03,470
capabilities um and the the last
question what is a minimum GPU memory

2284
00:46:03,470 --> 00:46:03,480
question what is a minimum GPU memory
 

2285
00:46:03,480 --> 00:46:05,549
question what is a minimum GPU memory
needed for inference on a three billion

2286
00:46:05,549 --> 00:46:05,559
needed for inference on a three billion
 

2287
00:46:05,559 --> 00:46:08,270
needed for inference on a three billion
parameter model using standard float

2288
00:46:08,270 --> 00:46:08,280
parameter model using standard float
 

2289
00:46:08,280 --> 00:46:11,390
parameter model using standard float
Precision um this is kind of a an extra

2290
00:46:11,390 --> 00:46:11,400
Precision um this is kind of a an extra
 

2291
00:46:11,400 --> 00:46:13,270
Precision um this is kind of a an extra
bonus question which you should have

2292
00:46:13,270 --> 00:46:13,280
bonus question which you should have
 

2293
00:46:13,280 --> 00:46:14,829
bonus question which you should have
been reading the white papers in order

2294
00:46:14,829 --> 00:46:14,839
been reading the white papers in order
 

2295
00:46:14,839 --> 00:46:19,790
been reading the white papers in order
to answer um is it three G uh three

2296
00:46:19,790 --> 00:46:19,800
to answer um is it three G uh three
 

2297
00:46:19,800 --> 00:46:22,790
to answer um is it three G uh three
gigabyte six 12 or

2298
00:46:22,790 --> 00:46:22,800
gigabyte six 12 or
 

2299
00:46:22,800 --> 00:46:26,710
gigabyte six 12 or
24 um how much GPU memory would you need

2300
00:46:26,710 --> 00:46:26,720
24 um how much GPU memory would you need
 

2301
00:46:26,720 --> 00:46:30,309
24 um how much GPU memory would you need
to to run a 3 billion parameter

2302
00:46:30,309 --> 00:46:30,319
to to run a 3 billion parameter
 

2303
00:46:30,319 --> 00:46:34,950
to to run a 3 billion parameter
model and the correct answer is 12 um so

2304
00:46:34,950 --> 00:46:34,960
model and the correct answer is 12 um so
 

2305
00:46:34,960 --> 00:46:36,910
model and the correct answer is 12 um so
hopefully hopefully everyone got this

2306
00:46:36,910 --> 00:46:36,920
hopefully hopefully everyone got this
 

2307
00:46:36,920 --> 00:46:40,030
hopefully hopefully everyone got this
right um and if not um refer back to the

2308
00:46:40,030 --> 00:46:40,040
right um and if not um refer back to the
 

2309
00:46:40,040 --> 00:46:43,470
right um and if not um refer back to the
white paper to to learn more so thank

2310
00:46:43,470 --> 00:46:43,480
white paper to to learn more so thank
 

2311
00:46:43,480 --> 00:46:46,829
white paper to to learn more so thank
you so much uh again for dialing In from

2312
00:46:46,829 --> 00:46:46,839
you so much uh again for dialing In from
 

2313
00:46:46,839 --> 00:46:48,950
you so much uh again for dialing In from
from all of the the folks that are

2314
00:46:48,950 --> 00:46:48,960
from all of the the folks that are
 

2315
00:46:48,960 --> 00:46:50,150
from all of the the folks that are
working on our

2316
00:46:50,150 --> 00:46:50,160
working on our
 

2317
00:46:50,160 --> 00:46:53,549
working on our
q&as um the kaggle team the Google deep

2318
00:46:53,549 --> 00:46:53,559
q&as um the kaggle team the Google deep
 

2319
00:46:53,559 --> 00:46:56,270
q&as um the kaggle team the Google deep
mine team the organizing team a huge

2320
00:46:56,270 --> 00:46:56,280
mine team the organizing team a huge
 

2321
00:46:56,280 --> 00:46:58,870
mine team the organizing team a huge
round of applause for everybody involved

2322
00:46:58,870 --> 00:46:58,880
round of applause for everybody involved
 

2323
00:46:58,880 --> 00:47:01,270
round of applause for everybody involved
um thank you so much and look forward to

2324
00:47:01,270 --> 00:47:01,280
um thank you so much and look forward to
 

2325
00:47:01,280 --> 00:47:04,680
um thank you so much and look forward to
seeing you tomorrow

